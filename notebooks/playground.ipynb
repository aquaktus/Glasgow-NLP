{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code playground\n",
    "This notebook is a small sandbox where half baked ideas can live. Code snippets that were developed but might be useful saterr can go here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c11c7d636f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "# import streamtologger\n",
    "# streamtologger.redirect(target=\"./logs-playground.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"str\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6c28d2cca682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"ff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"str\") to tuple"
     ]
    }
   ],
   "source": [
    "a = ()\n",
    "b = a + (\"ff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def super_print(filename):\n",
    "    '''filename is the file where output will be written'''\n",
    "    def wrap(func):\n",
    "        '''func is the function you are \"overriding\", i.e. wrapping'''\n",
    "        def wrapped_func(*args,**kwargs):\n",
    "            '''*args and **kwargs are the arguments supplied \n",
    "            to the overridden function'''\n",
    "            #use with statement to open, write to, and close the file safely\n",
    "            with open(filename,'a') as outputfile:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                outputfile.write(\"[{}] \".format(dt_string))\n",
    "                outputfile.write(\" \".join(str(x) for x in args))\n",
    "                outputfile.write(\"\\n\")\n",
    "            #now original function executed with its arguments as normal\n",
    "            return func(*args,**kwargs)\n",
    "        return wrapped_func\n",
    "    return wrap\n",
    "\n",
    "print = super_print('logs-playground.txt')(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 2, 1, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,4,3,6,2]).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_transformer_model = nn.Transformer().to(\"cuda\") # uses default hyperparameters\n",
    "src = torch.rand((10, 32, 512)).to(\"cuda\") # [src_seq_length, batch_size, embedding_size]\n",
    "tgt = torch.rand((20, 32, 512)).to(\"cuda\") # [tgt_seq_length, batch_size, embedding_size]\n",
    "rand_transformer_model(src, tgt).shape # [tgt_seq_length, batch_size, embedding_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (multihead_attn): MultiheadAttention(\n",
       "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_transformer_model.decoder.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand((20, 32, 512))\n",
    "k = torch.rand((10, 32, 512))\n",
    "v = torch.rand((10, 32, 512))\n",
    "att = nn.MultiheadAttention(512, 2)\n",
    "attn_output = att(q, k, v)\n",
    "# attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-made beam search code\n",
    "This code runs on paralelised batches and beams, it's fast, but doesn't have the appropriate stopping conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode_batch_ids(encoder_input, beam_size=3, max_seq_length=50):\n",
    "    batch_len = encoder_input.shape[1]\n",
    "    sos_id = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "    decoder_input = torch.zeros((1, beam_size * batch_len), dtype=torch.long, device=device).fill_(sos_id)\n",
    "    log_probs = torch.zeros((beam_size * batch_len,1))\n",
    "    print(\"log_probs:\", log_probs)\n",
    "    vocab_size = len(TGT_TEXT.vocab.itos)\n",
    "    \n",
    "    encoder_input = encoder_input.view(-1,1).repeat(1,beam_size).view(-1,beam_size * batch_len) # this tiles the input to the beam size * batch size\n",
    "    print(\"tiled input:\", encoder_input)\n",
    "    \n",
    "    for i in range(max_seq_length):\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        print(output.shape)\n",
    "        last_pred = output[-1].softmax(1)\n",
    "        print(\"last_pred shape:\", last_pred.shape)\n",
    "        log_predictions = last_pred.log()\n",
    "        seq_log_probs = log_predictions + log_probs.repeat(1,vocab_size)\n",
    "        print(\"seq_log_probs:\", seq_log_probs)\n",
    "        \n",
    "        seq_log_probs_positions = seq_log_probs.view(batch_len,-1).argsort(1)[:,-beam_size:]\n",
    "        print(\"seq_log_probs_positions:\", seq_log_probs_positions)\n",
    "        next_ids = seq_log_probs_positions.reshape(1, batch_len*beam_size) % vocab_size\n",
    "        print(\"next_ids:\", next_ids)\n",
    "        log_probs = seq_log_probs.view(batch_len,-1).gather(1,seq_log_probs_positions).view(beam_size * batch_len,1)\n",
    "        print(\"log_probs:\",log_probs)\n",
    "        \n",
    "        for batch_idx in range(seq_log_probs_positions.shape[0]):\n",
    "            for seq_choice_idx in range(seq_log_probs_positions.shape[1]):\n",
    "                seq_choice = seq_log_probs_positions[batch_idx,seq_choice_idx] // vocab_size\n",
    "                decoder_input[:,batch_idx*batch_len + seq_choice_idx] = decoder_input[:,batch_idx*batch_len + seq_choice]\n",
    "        \n",
    "        decoder_input = torch.cat((decoder_input, next_ids))\n",
    "    return decoder_input\n",
    "        \n",
    "#         last_pred = output[-1].argsort(dim=1)[:,:beam_size]\n",
    "\n",
    "sent1 = [\"<sos>\"] + SRC_TEXT.preprocess(\"create array\") + [\"<eos>\"] + [\"<pad>\"]\n",
    "sent2 = [\"<sos>\"] + SRC_TEXT.preprocess(\"if exists then\") + [\"<eos>\"]\n",
    "src_ids = SRC_TEXT.numericalize([sent1, sent2], device=device)\n",
    "print(\"input ids:\", src_ids)\n",
    "beam_search_decode_batch_ids(src_ids, max_seq_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non batch single node beam search\n",
    "This is the original beam search from online simply adapted to suit the transformerr architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return True\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        beta = 4.0\n",
    "        # Add here a function for shaping a reward\n",
    "\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decode(model, encoder_states):\n",
    "    '''\n",
    "    :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
    "    :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
    "    :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
    "    :return: decoded_batch\n",
    "    '''\n",
    "\n",
    "    beam_width = 10\n",
    "    topk = 3  # how many sentence do you want to generate\n",
    "    decoded_batch = []\n",
    "    \n",
    "    SOS_token = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "    EOS_token = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "    MAX_LENGTH = 7\n",
    "    \n",
    "    batch_size = encoder_states.shape[1]\n",
    "\n",
    "    # decoding goes sentence by sentence\n",
    "    for idx in range(batch_size):\n",
    "        encoder_input = encoder_states[:, idx].view(-1,1)\n",
    "        \n",
    "        # Start with the start of the sentence token\n",
    "        decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
    "\n",
    "        # Number of sentence to generate\n",
    "        endnodes = []\n",
    "        number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "        # starting node -  hidden vector, previous node, word id, logp, length\n",
    "        node = BeamSearchNode(decoder_input, None, SOS_token, 0, 1)\n",
    "        nodes = PriorityQueue()\n",
    "\n",
    "        # start the queue\n",
    "        nodes.put((-node.eval(), node))\n",
    "        qsize = 1\n",
    "\n",
    "        # start beam search\n",
    "        while True:\n",
    "            # give up when decoding takes too long\n",
    "            if qsize > 400: break\n",
    "\n",
    "            # fetch the best node\n",
    "            score, n = nodes.get()\n",
    "#             decoder_input = n.wordid\n",
    "            decoder_input = n.h\n",
    "\n",
    "            if n.wordid == EOS_token and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # decode for one step using decoder\n",
    "#             decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "#             print(encoder_input)\n",
    "#             print(decoder_input)\n",
    "            decoder_output = model(encoder_input, decoder_input)\n",
    "            last_token_logits = decoder_output[-1]\n",
    "            last_token_logs = last_token_logits.log_softmax(1)\n",
    "            # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "            log_prob, indexes = torch.topk(last_token_logs, beam_width)\n",
    "            nextnodes = []\n",
    "\n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[0][new_k]\n",
    "                log_p = log_prob[0][new_k].item()\n",
    "                decoder_input = torch.cat((decoder_input,decoded_t.view(1,-1)))\n",
    "                node = BeamSearchNode(decoder_input, n, decoded_t.cpu().item(), n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "\n",
    "            # put them into queue\n",
    "            for i in range(len(nextnodes)):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "                # increase qsize\n",
    "            qsize += len(nextnodes) - 1\n",
    "\n",
    "        # choose nbest paths, back trace them\n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "        utterances = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "            utterance = []\n",
    "            utterance.append(n.wordid)\n",
    "            # back trace\n",
    "            while n.prevNode != None:\n",
    "                n = n.prevNode\n",
    "                utterance.append(n.wordid)\n",
    "\n",
    "            utterance = utterance[::-1]\n",
    "            utterances.append(utterance)\n",
    "\n",
    "        decoded_batch.append(utterances)\n",
    "\n",
    "    return decoded_batch\n",
    "\n",
    "sent1 = [\"<sos>\"] + SRC_TEXT.preprocess(\"call the options.get method with string 'CULL_FREQUENCY' and integer 3 as arguments, use the string 'cull_frequency' and previous result as the arguments for the call to the params.get method, substitute the result for cull_frequency.\") + [\"<eos>\"] + [\"<pad>\"]\n",
    "# sent2 = [\"<sos>\"] + SRC_TEXT.preprocess(\"if not,\") + [\"<eos>\"]\n",
    "src_ids = SRC_TEXT.numericalize([sent1], device=device)\n",
    "# print(\"input ids:\", src_ids)\n",
    "outs = beam_decode(model, encoder_states=src_ids)\n",
    "\n",
    "for b in outs:\n",
    "    for sent in b:\n",
    "        print([TGT_TEXT.vocab.itos[id] for id in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation of node based beam search to parallelise batches\n",
    "This version comes from an already working version for a single batch. But there is a problem since when decodng different batches and passing them to the transformer, they all need to be the same length which doesn't work since there could be a node made in the past with a better probability score.\n",
    "It doesn't look too obvious from what I see to speed it up while rretaining the theoretical benefits of being able to explore the full tree of options.\n",
    "\n",
    "### The solution\n",
    "You just need to add padding to the decoded sequences that are shorter and keep track of the position that you need to take the output token from. easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SRC_TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-86d3751bbcca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0msent1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<sos>\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSRC_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'try,'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<eos>\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0msent2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<sos>\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSRC_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"if not,\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<eos>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0msrc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSRC_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SRC_TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "def beam_decode(model, encoder_states):\n",
    "    '''\n",
    "    :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
    "    :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
    "    :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
    "    :return: decoded_batch\n",
    "    '''\n",
    "\n",
    "    beam_width = 10\n",
    "    topk = 3  # how many sentence do you want to generate\n",
    "    decoded_batch = []\n",
    "    \n",
    "    batch_size = encoder_states.shape[1]\n",
    "    \n",
    "    SOS_token = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "    EOS_token = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "    MAX_LENGTH = 7\n",
    "\n",
    "    # decoding goes all batches at the same time\n",
    "    encoder_input = encoder_states\n",
    "\n",
    "    # Start with the start of the sentence token\n",
    "    decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
    "\n",
    "    # Number of sentence to generate\n",
    "    batch_endnodes = [[] for i in range(batch_size)]\n",
    "    number_required = topk\n",
    "\n",
    "    # starting node -  hidden vector, previous node, word id, logp, length\n",
    "    batch_node = [BeamSearchNode(decoder_input, None, SOS_token, 0, 1) for i in range(batch_size)]\n",
    "    batch_nodes = [PriorityQueue() for i in range(batch_size)]\n",
    "\n",
    "    # start the queue\n",
    "    for nodes, node in zip(batch_nodes, batch_node):\n",
    "        nodes.put((-node.eval(), node))\n",
    "        \n",
    "    batch_qsize = [1 for i in range(batch_size)]\n",
    "\n",
    "    # start beam search\n",
    "    while True:\n",
    "        # give up when decoding takes too long for the first batch, placeholder for now\n",
    "        print(batch_qsize)\n",
    "        if batch_qsize[0] > 200: break\n",
    "\n",
    "        # fetch the best node\n",
    "        best_nodes = [nodes.get() for nodes in batch_nodes]\n",
    "#         score, n = nodes.get()\n",
    "#             decoder_input = n.wordid\n",
    "#         decoder_input = n.h\n",
    "        \n",
    "        finished_nodes = [True if n.wordid == EOS_token and n.prevNode != None else False for (score, n) in best_nodes]\n",
    "        print(finished_nodes)\n",
    "        \n",
    "        working_nodes = []\n",
    "        working_node_id = 0\n",
    "        working_nodes_idx = []\n",
    "        for endnodes, (score, n) in zip(batch_endnodes, best_nodes):\n",
    "            if n.wordid == EOS_token and n.prevNode != None and len(endnodes) < number_required:\n",
    "                endnodes.append((score, n))\n",
    "            else:\n",
    "                working_nodes.append((score, n))\n",
    "                working_nodes_idx.append(working_node_id)\n",
    "            working_node_id += 1\n",
    "        \n",
    "        if all([len(endnodes) >= number_required for endnodes in batch_endnodes]):\n",
    "            break\n",
    "        \n",
    "        num_working_nodes = len(working_nodes)\n",
    "        print(working_nodes)\n",
    "        \n",
    "        step_encoder_input = encoder_input[:,:num_working_nodes].view(-1,num_working_nodes)\n",
    "        print(\"[n.h for (score, n) in working_nodes] shape:\", [n.h.shape for (score, n) in working_nodes])\n",
    "        step_decoder_input = torch.cat([n.h for (score, n) in working_nodes],dim=1)\n",
    "#         print(\"step_encoder_input shape:\", step_encoder_input.shape)\n",
    "        step_decoder_output = model(step_encoder_input, step_decoder_input)\n",
    "        step_token_logits = step_decoder_output[-1]\n",
    "        \n",
    "        print(step_token_logits)\n",
    "\n",
    "        # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "        log_prob, indexes = torch.topk(step_token_logits, beam_width)\n",
    "        print(log_prob.shape)\n",
    "        \n",
    "    \n",
    "        nextnodes = []\n",
    "        \n",
    "        for batch_id in working_nodes_idx:\n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[batch_id][new_k]\n",
    "                log_p = log_prob[batch_id][new_k].item()\n",
    "#                 print(\"decoder_input shape\", step_decoder_input.shape)\n",
    "#                 print(\"step_decoder_input shape:\", step_decoder_input[:,batch_id].view(-1,1).shape)\n",
    "#                 print(\"decoded_t shape:\", decoded_t.view(1,-1).shape)\n",
    "                decoder_input = torch.cat((step_decoder_input[:,batch_id].view(-1,1),decoded_t.view(1,-1)), dim=0)\n",
    "#                 print(\"decoder_input shape:\", decoder_input.shape)\n",
    "                node = BeamSearchNode(decoder_input, working_nodes[batch_id], decoded_t.cpu().item(), n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                batch_nodes[batch_id].put((score, node))\n",
    "                batch_qsize[batch_id] += 1\n",
    "        \n",
    "        # put them into queue\n",
    "#         for i in range(len(nextnodes)):\n",
    "#             score, nn = nextnodes[i]\n",
    "#             nodes.put((score, nn))\n",
    "#             # increase qsize\n",
    "#         qsize += len(nextnodes) - 1\n",
    "\n",
    "    # choose nbest paths, back trace them\n",
    "    if len(endnodes) == 0:\n",
    "        endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "    utterances = []\n",
    "    for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "        utterance = []\n",
    "        utterance.append(n.wordid)\n",
    "        # back trace\n",
    "        while n.prevNode != None:\n",
    "            n = n.prevNode\n",
    "            utterance.append(n.wordid)\n",
    "\n",
    "        utterance = utterance[::-1]\n",
    "        utterances.append(utterance)\n",
    "\n",
    "    decoded_batch.append(utterances)\n",
    "\n",
    "    return decoded_batch\n",
    "\n",
    "sent1 = [\"<sos>\"] + SRC_TEXT.preprocess('try,') + [\"<eos>\"] + [\"<pad>\"]\n",
    "sent2 = [\"<sos>\"] + SRC_TEXT.preprocess(\"if not,\") + [\"<eos>\"]\n",
    "src_ids = SRC_TEXT.numericalize([sent1, sent2], device=device)\n",
    "# print(\"input ids:\", src_ids)\n",
    "[len(x) for x in beam_decode(model, encoder_states=src_ids)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_data = [\n",
    "#     (\"my favourite foods are banana and toast\",\"would you like banana and toast ?\"),\n",
    "#     (\"my favourite foods are eggs and bacon and beans\",\"would you like eggs and bacon and beans ?\"),\n",
    "#     (\"my favourite food is chocolate\",\"would you like chocolate ?\"),\n",
    "#     (\"my favourite food is avocado\",\"would you like avocado ?\")\n",
    "# ]\n",
    "\n",
    "# other_data = [\n",
    "#     (\"what age is she ?\", \"she is 8 years old\"),\n",
    "#     (\"what age is he ?\", \"he is 4 years old\"),\n",
    "#     (\"how old are you ?\", \"i am 22 years old\"),\n",
    "#     (\"how old am i ?\", \"you are 28 years old\")\n",
    "# ]\n",
    "\n",
    "# SRC_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "# TGT_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "\n",
    "# train_dataset = val_dataset = samples_to_dataset(other_data, SRC_TEXT, TGT_TEXT)\n",
    "\n",
    "# # train_dataset, val_dataset = dataset.split([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Noise to the decoding process during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [-inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [-inf, 0., 0., -inf, -inf, 0., -inf, -inf, -inf, -inf],\n",
       "        [0., -inf, 0., 0., -inf, -inf, 0., -inf, -inf, -inf],\n",
       "        [-inf, 0., 0., -inf, 0., 0., 0., 0., -inf, -inf],\n",
       "        [-inf, 0., 0., 0., 0., -inf, 0., -inf, 0., -inf],\n",
       "        [0., -inf, 0., 0., -inf, 0., -inf, 0., -inf, -inf]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_e = 0.4\n",
    "noise_mask = (torch.rand(10,10) > noise_e).float()\n",
    "\n",
    "mask = (torch.triu(torch.ones(10,10))).transpose(0, 1)\n",
    "mask = torch.mul(mask, noise_mask)\n",
    "v = (torch.sum(mask, dim=-1) == 0).float()\n",
    "\n",
    "fix_mask = torch.zeros(10,10)\n",
    "fix_mask[:,0] = 1.0\n",
    "v = v.repeat(10, 1).transpose(0,1)\n",
    "fix_mask = torch.mul(fix_mask,v)\n",
    "\n",
    "print(fix_mask)\n",
    "mask += fix_mask\n",
    "\n",
    "\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch target masking transformer bug\n",
    "There is a bug in the implementation of the transformer attention mask during decoding. It being produced the other way round, paying attention to parts of the sentence that appear at the end. It is a simple to fix at removing ==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Transformer()\n",
    "model.generate_square_subsequent_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = 4\n",
    "mask = (torch.triu(torch.ones(sz, sz))==1).transpose(0, 1).float()\n",
    "print(mask)\n",
    "mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (torch.ones(10)==1)\n",
    "a.masked_fill(a == 1, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from queue import PriorityQueue\n",
    "a = PriorityQueue()\n",
    "a.put(torch.tensor(1))\n",
    "a.put(torch.tensor(-2))\n",
    "a.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.],\n",
      "        [3., 7.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1],[2],[3]])\n",
    "b = torch.tensor([[3],[4]])\n",
    "rough_input = [a,b]\n",
    "m = max([t.shape[0] for t in rough_input])\n",
    "print(m)\n",
    "z = torch.zeros((m,2)).fill_(7)\n",
    "for i in range(2):\n",
    "    length = rough_input[i].shape[0]\n",
    "    z[:length,i] = rough_input[i].view(-1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8]])\n"
     ]
    }
   ],
   "source": [
    "outputs = torch.tensor([[[1],[5]],[[2],[6]],[[3],[7]],[[4],[8]]]).transpose(0,1)\n",
    "p_mask = [2,3]\n",
    "\n",
    "for l in outputs[]:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0259, 0.0705, 0.1917, 0.5210, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095,\n",
       "        0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095,\n",
       "        0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.,2,3,4, 0, 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).softmax(0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atts: tensor([[[0.0248, 0.1017, 0.7406],\n",
      "         [0.8730, 0.7230, 0.6886]],\n",
      "\n",
      "        [[0.0148, 0.6740, 0.0523],\n",
      "         [0.4018, 0.8585, 0.9774]],\n",
      "\n",
      "        [[0.9876, 0.9629, 0.4758],\n",
      "         [0.0139, 0.4103, 0.1767]],\n",
      "\n",
      "        [[0.9024, 0.3109, 0.5361],\n",
      "         [0.5023, 0.6643, 0.6590]]])\n",
      "atts.shape: torch.Size([4, 2, 3])\n",
      "atts: tensor([[[0.0248, 0.1017, 0.7406],\n",
      "         [0.8730, 0.7230, 0.6886]],\n",
      "\n",
      "        [[0.0148, 0.6740, 0.0523],\n",
      "         [0.4018, 0.8585, 0.9774]],\n",
      "\n",
      "        [[0.9876, 0.9629, 0.4758],\n",
      "         [0.0139, 0.4103, 0.1767]],\n",
      "\n",
      "        [[0.9024, 0.3109, 0.5361],\n",
      "         [0.5023, 0.6643, 0.6590]]])\n",
      "src: tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "src.T: tensor([[1, 3, 5],\n",
      "        [2, 4, 6]])\n",
      "src.unsqueeze: tensor([[[1, 3, 5],\n",
      "         [2, 4, 6]]])\n",
      "src.inter: tensor([[[1, 3, 5],\n",
      "         [2, 4, 6]],\n",
      "\n",
      "        [[1, 3, 5],\n",
      "         [2, 4, 6]],\n",
      "\n",
      "        [[1, 3, 5],\n",
      "         [2, 4, 6]],\n",
      "\n",
      "        [[1, 3, 5],\n",
      "         [2, 4, 6]]])\n",
      "src.inter.shape: torch.Size([4, 2, 3])\n",
      "tensor([[[0.0000, 0.0248, 0.0000, 0.1017, 0.0000, 0.7406, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.8730, 0.0000, 0.7230, 0.0000, 0.6886, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0148, 0.0000, 0.6740, 0.0000, 0.0523, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4018, 0.0000, 0.8585, 0.0000, 0.9774, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.9876, 0.0000, 0.9629, 0.0000, 0.4758, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0139, 0.0000, 0.4103, 0.0000, 0.1767, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.9024, 0.0000, 0.3109, 0.0000, 0.5361, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.5023, 0.0000, 0.6643, 0.0000, 0.6590, 0.0000,\n",
      "          0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "atts = torch.rand(2,4,3).transpose(0,1) # [output_seq_length, batch_size, input_seq_length]\n",
    "print(\"atts:\", atts)\n",
    "print(\"atts.shape:\", atts.shape)\n",
    "\n",
    "# atts = atts * torch.tensor([[[0.1],[1]],[[10],[100]],[[1000],[10000]],[[100000],[1000000]]])\n",
    "print(\"atts:\", atts)\n",
    "\n",
    "# src = torch.randint(0, 10, (3,2))\n",
    "src = torch.tensor([[1,2],\n",
    "                    [3,4],\n",
    "                    [5,6]])\n",
    "print(\"src:\", src)\n",
    "src = src.transpose(0,1)\n",
    "print(\"src.T:\", src)\n",
    "src = src.unsqueeze(0)\n",
    "print(\"src.unsqueeze:\", src)\n",
    "src = torch.repeat_interleave(src, 4, dim=0)\n",
    "print(\"src.inter:\", src)\n",
    "print(\"src.inter.shape:\", src.shape)\n",
    "\n",
    "out_dist = torch.zeros(4, 2, 10).scatter_add_(2,src,atts)\n",
    "print(out_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python AST doodles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import astor\n",
    "\n",
    "import execnet\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_python_version(Version, Module, Function, ArgumentList):\n",
    "    gw      = execnet.makegateway(\"popen//python=python%s\" % Version)\n",
    "    channel = gw.remote_exec(\"\"\"\n",
    "        from %s import %s as the_function\n",
    "        channel.send(the_function(*channel.receive()))\n",
    "    \"\"\" % (Module, Function))\n",
    "    channel.send(ArgumentList)\n",
    "    return channel.receive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(*bar):\n",
    "    print(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def foo(*bar):\n",
      "    print(bar)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "func_source = inspect.getsource(foo)\n",
    "print(func_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is')\n"
     ]
    }
   ],
   "source": [
    "foo(\"this\", \"is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_as_python2(code_string):\n",
    "    gw = execnet.makegateway(\"popen//python=python2\")\n",
    "    channel = gw.remote_exec(f\"\"\"\n",
    "        channel.send(\"the_function(*channel.receive())\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr Bear!\n"
     ]
    },
    {
     "ename": "RemoteError",
     "evalue": "Traceback (most recent call last):\n  File \"<string>\", line 1084, in executetask\n  File \"<string>\", line 1, in do_exec\n  File \"<remote exec>\", line 3, in <module>\n  File \"<string>\", line 729, in send\n  File \"<string>\", line 1371, in dumps_internal\n  File \"<string>\", line 1389, in save\n  File \"<string>\", line 1405, in _save\nDumpError: can't serialize <class '_ast.Module'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-aa1903dbbfd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m result = call_python_version(\"2\", \"python2_code\", \"string_py2_ast\",  \n\u001b[0;32m----> 5\u001b[0;31m                              [\"print 'foo' \"]) \n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7d10cd00bb25>\u001b[0m in \u001b[0;36mcall_python_version\u001b[0;34m(Version, Module, Function, ArgumentList)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\" % (Module, Function))\n\u001b[1;32m      7\u001b[0m     \u001b[0mchannel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArgumentList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/execnet/gateway_base.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mENDMARKER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mitemqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for other receivers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getremoteerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRemoteError\u001b[0m: Traceback (most recent call last):\n  File \"<string>\", line 1084, in executetask\n  File \"<string>\", line 1, in do_exec\n  File \"<remote exec>\", line 3, in <module>\n  File \"<string>\", line 729, in send\n  File \"<string>\", line 1371, in dumps_internal\n  File \"<string>\", line 1389, in save\n  File \"<string>\", line 1405, in _save\nDumpError: can't serialize <class '_ast.Module'>\n"
     ]
    }
   ],
   "source": [
    "result = call_python_version(\"3\", \"python2_code\", \"my_function\",  \n",
    "                             [\"Mr\", \"Bear\"]) \n",
    "print(result) \n",
    "result = call_python_version(\"2\", \"python2_code\", \"string_py2_ast\",  \n",
    "                             [\"print 'foo' \"]) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_ast.Store"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(\"Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "def is_primitive(obj):\n",
    "    return any([isinstance(obj, str),isinstance(obj, int),isinstance(obj, float)])\n",
    "\n",
    "def ast_to_production_rules(tree):\n",
    "    \n",
    "    fields = tree._fields\n",
    "    name = type(tree).__name__\n",
    "    print(name, end =\" \")\n",
    "    \n",
    "    for field in fields:\n",
    "        field_obj = getattr(tree, field)\n",
    "        \n",
    "        if field_obj==None:\n",
    "            continue\n",
    "            \n",
    "        elif is_primitive(field_obj):\n",
    "            print(field, end=\" \")\n",
    "            print(field_obj, end=\" \")\n",
    "            \n",
    "        elif isinstance(field_obj, Iterable):\n",
    "            print(field, end=\" \")\n",
    "            for elem in field_obj:\n",
    "                ast_to_production_rules(elem)\n",
    "            print(\"<end_list>\", end=\" \")\n",
    "        else:\n",
    "            print(field, end=\" \")\n",
    "            ast_to_production_rules(field_obj)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module body Assign targets Name id a ctx Store <end_list> value Num n 0.4 <end_list> "
     ]
    }
   ],
   "source": [
    "ast_to_production_rules(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this ' is\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='this \\' is'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(body=[Expr(value=Call(func=Attribute(value=Name(id='parser', ctx=Load()), attr='add_argument', ctx=Load()), args=[Str(s='app_label')], keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))]))])\n",
      "\n",
      "parser.add_argument('app_label', help=\n",
      "    'App label of the application containing the migration.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_string = \"parser . add_argument ( 'app_label' ,  help = 'App label of the application containing the migration.' )\"\n",
    "tree = ast.parse(code_string)\n",
    "# tree = ast.parse(\"a='foo'\")\n",
    "print(ast.dump(tree))\n",
    "print()\n",
    "print(astor.to_source(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_num(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return float(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(body=[Expr(value=Call(func=Attribute(value=Name(id='parser', ctx=Load()), attr='add_argument', ctx=Load()), args=[Str(s='app_label')], keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))]))])\n",
      "\n",
      "Reduced tree: Module(body=[Expr(value=Call(func=Attribute(value=Name(id='parser', ctx=Load()), attr='add_argument', ctx=Load()), args=[Str(s='app_label')], keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))]))])\n",
      "2 Attr string so far: body=[Expr(value=Call(func=Attribute(value=Name(id='parser', ctx=Load()), attr='add_argument', ctx=Load()), args=[Str(s='app_label')], keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))]))]\n",
      "3 attribute name: body\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: value=Call(func=Attribute(value=Name(id='parser', ctx=Load()), attr='add_argument', ctx=Load()), args=[Str(s='app_label')], keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))])\n",
      "3 attribute name: value\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: func=Attribute(value=Name(id='parser', ctx=Load()), attr='add_argument', ctx=Load()), args=[Str(s='app_label')], keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))]\n",
      "3 attribute name: func\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: value=Name(id='parser', ctx=Load()), attr='add_argument', ctx=Load()\n",
      "3 attribute name: value\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: id='parser', ctx=Load()\n",
      "3 attribute name: id\n",
      "6 checking Quote or DQuote: True\n",
      "2 Attr string so far: ctx=Load()\n",
      "3 attribute name: ctx\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: attr='add_argument', ctx=Load()\n",
      "3 attribute name: attr\n",
      "6 checking Quote or DQuote: True\n",
      "2 Attr string so far: ctx=Load()\n",
      "3 attribute name: ctx\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: args=[Str(s='app_label')], keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))]\n",
      "3 attribute name: args\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: s='app_label'\n",
      "3 attribute name: s\n",
      "6 checking Quote or DQuote: True\n",
      "2 Attr string so far: keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))]\n",
      "3 attribute name: keywords\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: arg='help', value=Str(s='App label of the application containing the migration.')\n",
      "3 attribute name: arg\n",
      "6 checking Quote or DQuote: True\n",
      "2 Attr string so far: value=Str(s='App label of the application containing the migration.')\n",
      "3 attribute name: value\n",
      "6 checking Quote or DQuote: False\n",
      "2 Attr string so far: s='App label of the application containing the migration.'\n",
      "3 attribute name: s\n",
      "6 checking Quote or DQuote: True\n",
      "\n",
      "Module(body=[Expr(value=Call(func=Attribute(value=Name(id='parser', ctx=Load()), attr='add_argument', ctx=Load()), args=[Str(s='app_label')], keywords=[keyword(arg='help', value=Str(s='App label of the application containing the migration.'))]))])\n",
      "\n",
      "OG String    : parser . add_argument ( 'app_label' ,  help = 'App label of the application containing the migration.' )\n",
      "Code from AST: parser.add_argument('app_label', help=\n",
      "    'App label of the application containing the migration.')\n",
      "\n",
      "Rebuilt AST  : parser.add_argument('app_label', help=\n",
      "    'App label of the application containing the migration.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "def new_dump_to_ast(dump_string):\n",
    "    first_paren = dump_string.index(\"(\")\n",
    "    node_string = dump_string[:first_paren]\n",
    "#     print(1, \"creating node type:\", node_string)\n",
    "    module = importlib.import_module(\"ast\")\n",
    "    class_ = getattr(module, node_string)\n",
    "    node = class_()\n",
    "    inner_paren = dump_string[first_paren+1:-1]\n",
    "    \n",
    "    # handle variable length attributes\n",
    "    while len(inner_paren) > 0:\n",
    "        print(2, \"Attr string so far:\", inner_paren)\n",
    "        equal_idx = inner_paren.index(\"=\")\n",
    "        attr_string = inner_paren[:equal_idx]\n",
    "        print(3, \"attribute name:\", attr_string)\n",
    "        \n",
    "        # 5 cases: None, num, string, list, node\n",
    "        # first check if None\n",
    "#         print(4, \"checking None:\", inner_paren[equal_idx+1:equal_idx+5])\n",
    "#         print(5, \"checking number:\", inner_paren[equal_idx+1], inner_paren[equal_idx+1].isnumeric())\n",
    "        print(6, \"checking Quote or DQuote:\", inner_paren[equal_idx+1] == \"'\" or inner_paren[equal_idx+1] == '\"')\n",
    "        \n",
    "        if inner_paren[equal_idx+1:equal_idx+5] == \"None\":\n",
    "            setattr(node, attr_string, None)\n",
    "            inner_paren = inner_paren[equal_idx + 7:]\n",
    "        \n",
    "        elif inner_paren[equal_idx+1:equal_idx+5] == \"True\":\n",
    "            setattr(node, attr_string, True)\n",
    "            inner_paren = inner_paren[equal_idx + 7:]\n",
    "            \n",
    "        elif inner_paren[equal_idx+1:equal_idx+6] == \"False\":\n",
    "            setattr(node, attr_string, False)\n",
    "            inner_paren = inner_paren[equal_idx + 8:]\n",
    "            \n",
    "        # check if first character is a number\n",
    "        elif inner_paren[equal_idx+1].isnumeric():\n",
    "            number_finder = re.compile(\"([0-9]+(.?[0-9]*))( *([a-zA-Z]+))*\")\n",
    "            groups = number_finder.match(inner_paren[equal_idx+1:]).groups()\n",
    "#             print(6, \"Number groups identified:\", groups)\n",
    "            num = to_num(groups[0])\n",
    "#             print(7, \"Number:\", num)\n",
    "            setattr(node, attr_string, num)\n",
    "            inner_paren = inner_paren[equal_idx + len(groups[0])+2:]\n",
    "        \n",
    "        # check if first Character is quote or double quote to see if it's a string\n",
    "        elif inner_paren[equal_idx+1] == \"'\" or inner_paren[equal_idx+1] == '\"':\n",
    "            string_matcher = re.compile(\"['\\\"](.*?)['\\\"]\")\n",
    "            string = string_matcher.findall(inner_paren[equal_idx:])[0]\n",
    "            setattr(node, attr_string, string)\n",
    "            inner_paren = inner_paren[equal_idx + len(string)+5:]\n",
    "        \n",
    "        # deal with list\n",
    "        elif inner_paren[equal_idx+1] == \"[\":\n",
    "            list_last_square = get_square_index(inner_paren, equal_idx+1)\n",
    "            list_string = inner_paren[equal_idx+2:list_last_square]\n",
    "            \n",
    "            elem_list = []\n",
    "            while len(list_string) > 0:\n",
    "#                 print(8, \"List string so far:\", list_string)\n",
    "                value_first_paren = list_string.index(\"(\")\n",
    "                value_last_paren = get_paren_index(list_string, value_first_paren)\n",
    "                list_value_ast = new_dump_to_ast(list_string[:value_last_paren+1])\n",
    "                elem_list.append(list_value_ast)\n",
    "                list_string = list_string[value_last_paren+3:]\n",
    "            setattr(node, attr_string, elem_list)\n",
    "            inner_paren = inner_paren[list_last_square+3:]\n",
    "        \n",
    "        # deal with node\n",
    "        else:\n",
    "            value_first_paren = inner_paren.index(\"(\")        \n",
    "            value_last_paren = get_paren_index(inner_paren, value_first_paren)\n",
    "#             print(9,\"handling Node for: \",inner_paren[equal_idx+1:value_last_paren+1])\n",
    "            value_ast = new_dump_to_ast(inner_paren[equal_idx+1:value_last_paren+1])\n",
    "#             print(10, f\"obtained AST for{attr_string}:\", value_ast)\n",
    "            setattr(node, attr_string, value_ast)\n",
    "            inner_paren = inner_paren[value_last_paren+3:]\n",
    "    \n",
    "    \n",
    "    return node\n",
    "\n",
    "print(ast.dump(tree))\n",
    "print()\n",
    "t = tree\n",
    "print(\"Reduced tree:\", ast.dump(t))\n",
    "new_ast = new_dump_to_ast(ast.dump(t))\n",
    "\n",
    "print()\n",
    "print(ast.dump(new_ast))\n",
    "\n",
    "print()\n",
    "print(f\"OG String    : {code_string}\")\n",
    "print(f\"Code from AST: {astor.to_source(t)}\")\n",
    "print(f\"Rebuilt AST  : {astor.to_source(new_ast)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_ast.Module"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "module = importlib.import_module(\"ast\")\n",
    "class_ = getattr(module, \"Module\")\n",
    "instance = class_()\n",
    "type(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/all.code\", \"r\") as f:\n",
    "    count = 0\n",
    "    all_lines = 0\n",
    "    for line in f.readlines():\n",
    "        all_lines+=1\n",
    "        try:\n",
    "            can_code = canonicalize_code(line[:-1])\n",
    "            code_ast = ast.parse(can_code)\n",
    "            new_ast = new_dump_to_ast(ast.dump(code_ast))\n",
    "            can_code_ast = astor.to_source(new_ast)\n",
    "\n",
    "            de_can_code_ast = de_canonicalize_code(can_code_ast, code)\n",
    "            de_can_code_ast = re.sub(r'\\n', '', de_can_code_ast)\n",
    "            count += 1\n",
    "        except:\n",
    "            print(line[:-1])\n",
    "            pass\n",
    "print(f\"{count}/{all_lines} = {count*100/all_lines:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  else :\n",
      "if True: pass\n",
      "else :pass\n",
      "2 Attr string so far: body=[If(test=NameConstant(value=True), body=[Pass()], orelse=[Pass()])]\n",
      "3 attribute name: body\n",
      "6 checking Quote or DQuote: False\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_square_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f25fc97a4842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mcode_ast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mnew_ast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dump_to_ast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_ast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mcan_code_ast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_ast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_code_ast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a195fd12c322>\u001b[0m in \u001b[0;36mnew_dump_to_ast\u001b[0;34m(dump_string)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# deal with list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minner_paren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mequal_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"[\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mlist_last_square\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_square_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_paren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mlist_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_paren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mequal_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlist_last_square\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_square_index' is not defined"
     ]
    }
   ],
   "source": [
    "p_elif = re.compile(r'^elif\\s?')\n",
    "p_else = re.compile(r'^else\\s?')\n",
    "p_try = re.compile(r'^try\\s?')\n",
    "p_except = re.compile(r'^except\\s?')\n",
    "p_finally = re.compile(r'^finally\\s?')\n",
    "p_decorator = re.compile(r'^@.*')\n",
    "\n",
    "\n",
    "def canonicalize_code(code):\n",
    "    code = code.strip()\n",
    "    if p_elif.match(code):\n",
    "        code = 'if True: pass\\n' + code\n",
    "\n",
    "    if p_else.match(code):\n",
    "        code = 'if True: pass\\n' + code\n",
    "\n",
    "    if p_try.match(code):\n",
    "        code = code + 'pass\\nexcept: pass'\n",
    "    elif p_except.match(code):\n",
    "        code = 'try: pass\\n' + code\n",
    "    elif p_finally.match(code):\n",
    "        code = 'try: pass\\n' + code\n",
    "\n",
    "    if p_decorator.match(code):\n",
    "        code = code + '\\ndef dummy(): pass'\n",
    "\n",
    "    if code[-1] == ':':\n",
    "        code = code + 'pass'\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def de_canonicalize_code(code, ref_raw_code):\n",
    "    ref_raw_code = ref_raw_code.strip()\n",
    "    if code.endswith('def dummy():\\n    pass'):\n",
    "        code = code.replace('def dummy():\\n    pass', '').strip()\n",
    "\n",
    "    if p_elif.match(ref_raw_code):\n",
    "        # remove leading if true\n",
    "        code = code.replace('if True:\\n    pass', '').strip()\n",
    "    elif p_else.match(ref_raw_code):\n",
    "        # remove leading if true\n",
    "        code = code.replace('if True:\\n    pass', '').strip()\n",
    "\n",
    "    # try/catch/except stuff\n",
    "    if p_try.match(ref_raw_code):\n",
    "        code = code.replace('except:\\n    pass', '').strip()\n",
    "    elif p_except.match(ref_raw_code):\n",
    "        code = code.replace('try:\\n    pass', '').strip()\n",
    "    elif p_finally.match(ref_raw_code):\n",
    "        code = code.replace('try:\\n    pass', '').strip()\n",
    "\n",
    "    # remove ending pass\n",
    "    if code.endswith(':\\n    pass\\n') or code.endswith(':\\n    pass'):\n",
    "        code = code.replace('\\n    pass', '').strip()\n",
    "\n",
    "    return code\n",
    "\n",
    "code = linecache.getline(\"datasets/all.code\", 18345).rstrip()\n",
    "print(code)\n",
    "# code = \"eol_message = message . replace ( str ( '\\r\\n' ) , str ( '\\n' ) ) . replace ( str ( '\\r' ) , str ( '\\n' ) )\"\n",
    "can_code = canonicalize_code(code)\n",
    "print(can_code)\n",
    "code_ast = ast.parse(can_code)\n",
    "new_ast = new_dump_to_ast(ast.dump(code_ast))\n",
    "can_code_ast = astor.to_source(new_ast)\n",
    "print(can_code_ast)\n",
    "de_can_code_ast = de_canonicalize_code(can_code_ast, code)\n",
    "print(de_can_code_ast)\n",
    "de_can_code_ast = re.sub(r'\\n', '', de_can_code_ast)\n",
    "print(de_can_code_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_square_index(\"[()th[]is]\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eol_message = message.replace(str('\\r\\n'), str('\\n')).replace(str('\\r'),    str('\\n'))\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'\\n', '', can_code_ast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.3', '.3', None, None)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = re.compile(\"([0-9]+(.?[0-9]*))( *([a-zA-Z]+))*\") \n",
    "temp.match(\"3.3.dfd0\").groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a \" test\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-92bfb45f39d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"(?:\\\\\"|.)*?\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'this is a \\\" test\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"='parser', ctx=Load()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "temp = re.compile('\"(?:\\\\\"|.)*?\"')\n",
    "print('this is a \\\" test\"')\n",
    "print(temp.findall(\"='parser', ctx=Load()\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firstof statement requires at least one argument'"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_matcher = re.compile(\"['\\\"](.*?)['\\\"]\")\n",
    "string = string_matcher.findall('\"firstof statement requires at least one argument\" foo bar')[0]\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'foo'"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"foo\"\n",
    "a.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ABC[23]][89], 0: 8\n",
      "[ABC[23]][89], 4: 7\n",
      "[ABC[23]][89], 9: 12\n",
      "[ABC[23]][89], 1: -1\n"
     ]
    }
   ],
   "source": [
    "from collections import deque \n",
    "  \n",
    "def get_square_index(s, i): \n",
    "    if s[i] != '[': \n",
    "        return -1\n",
    "  \n",
    "    d = deque() \n",
    "    for k in range(i, len(s)): \n",
    "        if s[k] == ']': \n",
    "            d.popleft() \n",
    "\n",
    "        elif s[k] == '[': \n",
    "            d.append(s[i]) \n",
    "\n",
    "        if not d: \n",
    "            return k \n",
    "    return -1\n",
    "\n",
    "def get_paren_index(s, i): \n",
    "    if s[i] != '(': \n",
    "        return -1\n",
    "  \n",
    "    d = deque() \n",
    "    for k in range(i, len(s)): \n",
    "        if s[k] == ')': \n",
    "            d.popleft() \n",
    "\n",
    "        elif s[k] == '(': \n",
    "            d.append(s[i]) \n",
    "\n",
    "        if not d: \n",
    "            return k \n",
    "    return -1\n",
    "  \n",
    "# Driver code to test above method. \n",
    "def test(s, i): \n",
    "    matching_index = get_square_index(s, i) \n",
    "    print(s + \", \" + str(i) + \": \" + str(matching_index)) \n",
    "  \n",
    "def main(): \n",
    "    test(\"[ABC[23]][89]\", 0) # should be 8 \n",
    "    test(\"[ABC[23]][89]\", 4) # should be 7 \n",
    "    test(\"[ABC[23]][89]\", 9) # should be 12 \n",
    "    test(\"[ABC[23]][89]\", 1) # No matching bracket \n",
    "  \n",
    "if __name__ == \"__main__\": \n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run_as_python2'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.body[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: astor.codetoast is deprecated.  Please use astor.code_to_ast.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '__code__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-77ae3589b921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mastor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodetoast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"print('foo')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/astor/__init__.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwarg)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mnewfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mclass\u001b[0m \u001b[0mModProxy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/astor/file_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, codeobj)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodeobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_file_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodeobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/astor/file_util.py\u001b[0m in \u001b[0;36mget_file_info\u001b[0;34m(codeobj)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mlinenum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mfunc_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodeobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlinenum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_firstlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '__code__'"
     ]
    }
   ],
   "source": [
    "astor.codetoast(\"print('foo')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy_gen_transformer import Transformer\n",
    "model = Transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Assign (targets (list (Name (id (str val)) (ctx (Store -Store-))))) (value (Call (func (Attribute (value (Call (func (Name (id (str Header)) (ctx (Load -Load-)))) (args (list (Name (id (str val)) (ctx (Load -Load-))) (Name (id (str encoding)) (ctx (Load -Load-))))))) (attr (str encode)) (ctx (Load -Load-)))))))\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import sys\n",
    "import re\n",
    "import inspect\n",
    "\n",
    "def typename(x):\n",
    "    return type(x).__name__\n",
    "\n",
    "def escape(text):\n",
    "    text = text \\\n",
    "        .replace('\"', '`') \\\n",
    "        .replace('\\'', '`') \\\n",
    "        .replace(' ', '-SP-') \\\n",
    "        .replace('\\t', '-TAB-') \\\n",
    "        .replace('\\n', '-NL-') \\\n",
    "        .replace('(', '-LRB-') \\\n",
    "        .replace(')', '-RRB-') \\\n",
    "        .replace('|', '-BAR-')\n",
    "    return repr(text)[1:-1] if text else '-NONE-'\n",
    "\n",
    "def makestr(node):\n",
    "\n",
    "    #if node is None or isinstance(node, ast.Pass):\n",
    "    #    return ''\n",
    "\n",
    "    if isinstance(node, ast.AST):\n",
    "        n = 0\n",
    "        nodename = typename(node)\n",
    "        s = '(' + nodename\n",
    "        for chname, chval in ast.iter_fields(node):\n",
    "            chstr = makestr(chval)\n",
    "            if chstr:\n",
    "                s += ' (' + chname + ' ' + chstr + ')'\n",
    "                n += 1\n",
    "        if not n:\n",
    "            s += ' -' + nodename + '-' # (Foo) -> (Foo -Foo-)\n",
    "        s += ')'\n",
    "        return s\n",
    "\n",
    "    elif isinstance(node, list):\n",
    "        n = 0\n",
    "        s = '(list'\n",
    "        for ch in node:\n",
    "            chstr = makestr(ch)\n",
    "            if chstr:\n",
    "                s += ' ' + chstr\n",
    "                n += 1\n",
    "        s += ')'\n",
    "        return s if n else ''\n",
    "\n",
    "    elif isinstance(node, str):\n",
    "        return '(str ' + escape(node) + ')'\n",
    "\n",
    "    elif isinstance(node, bytes):\n",
    "        return '(bytes ' + escape(str(node)) + ')'\n",
    "\n",
    "    else:\n",
    "        return '(' + typename(node) + ' ' + str(node) + ')'\n",
    "\n",
    "\n",
    "def main():\n",
    "    p_elif = re.compile(r'^elif\\s?')\n",
    "    p_else = re.compile(r'^else\\s?')\n",
    "    p_try = re.compile(r'^try\\s?')\n",
    "    p_except = re.compile(r'^except\\s?')\n",
    "    p_finally = re.compile(r'^finally\\s?')\n",
    "    p_decorator = re.compile(r'^@.*')\n",
    "\n",
    "    for l in [\"\"\"val = Header ( val , encoding ) . encode ( )\"\"\"]:  # val = ', ' . join ( sanitize_address ( addr , encoding )  for addr in getaddresses ( ( val , ) ) )\n",
    "        l = l.strip()\n",
    "        if not l:\n",
    "            print()\n",
    "            sys.stdout.flush()\n",
    "            continue\n",
    "\n",
    "        if p_elif.match(l): l = 'if True: pass\\n' + l\n",
    "        if p_else.match(l): l = 'if True: pass\\n' + l\n",
    "\n",
    "        if p_try.match(l): l = l + 'pass\\nexcept: pass'\n",
    "        elif p_except.match(l): l = 'try: pass\\n' + l\n",
    "        elif p_finally.match(l): l = 'try: pass\\n' + l\n",
    "\n",
    "        if p_decorator.match(l): l = l + '\\ndef dummy(): pass'\n",
    "        if l[-1] == ':': l = l + 'pass'\n",
    "\n",
    "        parse = ast.parse(l)\n",
    "        parse = parse.body[0]\n",
    "        dump = makestr(parse)\n",
    "        print(dump)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch saving and restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.container import ModuleList\n",
    "import copy\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers=3, embedding_size=4):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers_foo = _get_clones(nn.Linear(embedding_size,embedding_size), num_layers)\n",
    "        self.activation_fn = nn.ReLU()\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        output = inp\n",
    "        for i in range(self.num_layers):\n",
    "            output = self.layers_foo[i](output)\n",
    "            print(output.shape)\n",
    "            output = self.activation_fn(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([5, 4])\n",
      "torch.Size([5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2231, 0.0000, 0.4172, 0.0000],\n",
       "        [0.2352, 0.0000, 0.4164, 0.0000],\n",
       "        [0.2311, 0.0000, 0.4159, 0.0000],\n",
       "        [0.2271, 0.0000, 0.4177, 0.0000],\n",
       "        [0.2278, 0.0000, 0.4178, 0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_inp = torch.rand((5,4))\n",
    "\n",
    "simple_model = SimpleModel()\n",
    "simple_model(ex_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layers_foo.0.weight',\n",
       "              tensor([[-0.2663,  0.1339, -0.2985, -0.3879],\n",
       "                      [-0.0576, -0.4047,  0.1100,  0.3523],\n",
       "                      [ 0.0260, -0.1466,  0.1175,  0.0271],\n",
       "                      [-0.1310, -0.1201,  0.2210, -0.4280]])),\n",
       "             ('layers_foo.0.bias',\n",
       "              tensor([ 0.4291, -0.2156,  0.3614, -0.4289])),\n",
       "             ('layers_foo.1.weight',\n",
       "              tensor([[-0.2663,  0.1339, -0.2985, -0.3879],\n",
       "                      [-0.0576, -0.4047,  0.1100,  0.3523],\n",
       "                      [ 0.0260, -0.1466,  0.1175,  0.0271],\n",
       "                      [-0.1310, -0.1201,  0.2210, -0.4280]])),\n",
       "             ('layers_foo.1.bias',\n",
       "              tensor([ 0.4291, -0.2156,  0.3614, -0.4289])),\n",
       "             ('layers_foo.2.weight',\n",
       "              tensor([[-0.2663,  0.1339, -0.2985, -0.3879],\n",
       "                      [-0.0576, -0.4047,  0.1100,  0.3523],\n",
       "                      [ 0.0260, -0.1466,  0.1175,  0.0271],\n",
       "                      [-0.1310, -0.1201,  0.2210, -0.4280]])),\n",
       "             ('layers_foo.2.bias',\n",
       "              tensor([ 0.4291, -0.2156,  0.3614, -0.4289]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model_saved_state = simple_model.state_dict()\n",
    "simple_model_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers=3, embedding_size=4):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers_foo = _get_clones(nn.Linear(embedding_size,embedding_size), num_layers)\n",
    "        self.activation_fn = nn.ReLU()\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        output = inp\n",
    "        for i in range(self.num_layers):\n",
    "            output = self.layers_foo[i](output)\n",
    "            print(output.shape)\n",
    "            output = self.activation_fn(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with GitHub tokenization and ASTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
