{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad: A symbolic playground for Seq2Seq models\n",
    "\n",
    "```\n",
    "what does Henry do? <scratch_pad> search('Henry') -> ['Henry is a School Psychologist'] </scratch_pad> School Psychologist\n",
    "\n",
    "what is 556 - 301? <scratch_pad> 556 - 301 -> 255 </scratch_pad> 255\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from src.models_and_transforms.BERT_models import AlphaBERT\n",
    "from src.models_and_transforms.text_transforms import Numericalise_Transform, Scratch_Pad_Sequence_Policy_Creator_Transform\n",
    "from src.pipe_datasets import Scratch_Pad_Policy_Dataset\n",
    "import re\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer, GPT2TokenizerFast\n",
    "from transformers import BertConfig, BertTokenizerFast, BertForMaskedLM\n",
    "from tokenizers import processors, Tokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import Trainer, Callback, seed_everything\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "from src.useful_utils import chunks\n",
    "from src.models_and_transforms.GPT2_models import GPT2_Scratch_Pad_Model\n",
    "sys.path.insert(0,\"src/external_repos/pyfuzz/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "tokenized_corpus = [re.findall(r\"[\\w']+|[.,!?;]\", doc) for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(q, n=1):\n",
    "    return bm25.get_top_n(q.split(), corpus, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_exec(code):\n",
    "    if not code:\n",
    "        return ''\n",
    "    try:\n",
    "        prior_code, _, last_line = code.rpartition('\\n')\n",
    "        exec(f'{prior_code}\\nglobal __i__; __i__ = {last_line}')\n",
    "        global __i__\n",
    "        return str(__i__)\n",
    "    except Exception as e:\n",
    "        if hasattr(e,'msg'):\n",
    "            return \"ERROR: \" + e.msg\n",
    "        return \"ERROR: \" + str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Hello there good man!']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_exec('''search('man')''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a function to process code in the scratch pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ERROR: name 'd' is not defined\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_exec('r=\"rrr\"\\nd+\"4\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6a26d5ff3bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test=5>>>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'this is a test <ScratchPad>r=4>>></ScratchPad>Jhon<ScratchPad>search(\"Oliver\")>>>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscratch_pad_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexecution_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'>>>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(['test=5>>>', 'this is a test <ScratchPad>r=4>>></ScratchPad>Jhon<ScratchPad>search(\"Oliver\")>>>'], padding=True, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "def scratch_pad_complete(batch_input_ids):\n",
    "    execution_token_id = tokenizer.get_vocab()['>>>']\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    start_scratch_pad_id = tokenizer.get_vocab()['<ScratchPad>']\n",
    "    end_scratch_pad_id = tokenizer.get_vocab()['</ScratchPad>']\n",
    "    if not (batch_input_ids[:,-1] == execution_token_id).any():\n",
    "        # there are no execution tokens, so return\n",
    "        return batch_input_ids\n",
    "    full_sents = []\n",
    "    for i in range(len(batch_input_ids)):\n",
    "        input_ids = list(batch_input_ids[i])\n",
    "        input_ids = [tok_id for tok_id in input_ids if tok_id != pad_token_id]\n",
    "        sequence = tokenizer.decode(input_ids, spaces_between_special_tokens=False)\n",
    "        \n",
    "        in_scratch_pad = False\n",
    "        if start_scratch_pad_id in input_ids:\n",
    "            in_scratch_pad = True\n",
    "            last_start_idx = len(input_ids) - 1 - input_ids[::-1].index(start_scratch_pad_id)\n",
    "            if end_scratch_pad_id in input_ids[last_start_idx:]:\n",
    "                in_scratch_pad = False\n",
    "        \n",
    "        if (batch_input_ids[i][-1] != execution_token_id) or not in_scratch_pad:\n",
    "            full_sents.append(sequence)\n",
    "            continue\n",
    "        \n",
    "        prior_scratch_pad_sequence, _, last_scratch_pad_sequence = sequence.rpartition('<ScratchPad>')\n",
    "        prior_scratch_pad_sequences = re.findall(r'\\<ScratchPad\\>([^]]*)\\</ScratchPad\\>', prior_scratch_pad_sequence)\n",
    "#         print(prior_scratch_pad_sequences)\n",
    "        all_statements = ''.join(prior_scratch_pad_sequences + [last_scratch_pad_sequence])\n",
    "        individual_statements = re.split(r'>>>.*\\n|>>>', all_statements)[:-1]\n",
    "        stmnt_out = my_exec('\\n'.join(individual_statements))\n",
    "        full_sents.append(sequence + stmnt_out + '\\n')\n",
    "    \n",
    "    return tokenizer(full_sents, padding=True, return_tensors='pt').input_ids\n",
    "\n",
    "input_ids = scratch_pad_complete(input_ids)\n",
    "[tokenizer.decode(input_ids[i], spaces_between_special_tokens=False) for i in range(input_ids.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = GPT2Config()\n",
    "configuration.n_head = 1\n",
    "configuration.n_embd = 10\n",
    "configuration.n_layer = 2\n",
    "configuration.n_ctx = 512\n",
    "configuration.n_positions = 512\n",
    "configuration.vocab_size = 50265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side='left', pad_token='<pad>', eos_token='<eos>',\n",
    "                                                       additional_special_tokens=['\\n', '<ScratchPad>', '</ScratchPad>'])\n",
    "model = GPT2_Scratch_Pad_Model(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: tensor([[ 4598, 50260,    19,    10,    20, 33409]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scratch_pad_complete' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d33861b44f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m gen_sequences = model.generate(tokens, max_length=15, do_sample=True, num_beams=3, use_cache=False, pad_token_id=tokenizer.pad_token_id, \n\u001b[0;32m----> 5\u001b[0;31m                                num_return_sequences=3, postfix_additional_tokens_fn=scratch_pad_complete)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces_between_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scratch_pad_complete' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode('do<ScratchPad>4+5>>>', return_tensors='pt')\n",
    "print(\"Input tokens: \"+ str(tokens))\n",
    "model.eval()\n",
    "gen_sequences = model.generate(tokens, max_length=15, do_sample=True, num_beams=3, use_cache=False, pad_token_id=tokenizer.pad_token_id, \n",
    "                               num_return_sequences=3, postfix_additional_tokens_fn=scratch_pad_complete)\n",
    "[tokenizer.decode(gen_sequences[i], spaces_between_special_tokens=False) for i in range(gen_sequences.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Num params: \" + str(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   40,  2883,  6155, 50260,    19, 33409]]),\n",
       " 'past_key_values': None,\n",
       " 'use_cache': None,\n",
       " 'position_ids': None,\n",
       " 'attention_mask': None,\n",
       " 'token_type_ids': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prepare_inputs_for_generation(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT for AlphaZero style learning\n",
    "The model will be both responsible for state $s$ evaluation into a value $v$ and polilcy $\\textbf{p}$\n",
    "\n",
    "\\begin{equation*}\n",
    "(v,\\textbf{p})=f(s|\\theta)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig()\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side='left', pad_token='[PAD]', eos_token='[EOS]', \n",
    "                                                       additional_special_tokens=['\\n', '<ScratchPad>', '</ScratchPad>', '[VALUE]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_attention_heads = 1\n",
    "config.hidden_size = 10\n",
    "config.num_hidden_layers = 2\n",
    "config.intermediate_size = 64\n",
    "config.n_ctx = 512\n",
    "config.n_positions = 512\n",
    "config.vocab_size = len(tokenizer.get_vocab())\n",
    "config.position_embedding_type = 'relative_key_query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlphaBERT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: tensor([[ 4598, 50260,    19,    10,    20, 33409, 50263, 50262]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9300]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.0425, -0.1846,  0.0738,  ..., -0.5901, -0.1247, -1.0043]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode('do<ScratchPad>4+5>>>[MASK][VALUE]', return_tensors='pt')\n",
    "print(\"Input tokens: \"+ str(tokens))\n",
    "model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {'input_ids':tokens, \n",
    "         'target_policy':torch.tensor([[33409]]), \n",
    "#          'target_value':torch.tensor([[1.0]]),\n",
    "         'attention_mask':torch.ones_like(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6052)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = torch.tensor([[0,1.0,0.0]])\n",
    "policy = torch.tensor([[0.01,0.01,0.9]])\n",
    "temp = 1.0\n",
    "torch.dot(pi.view(-1)**temp,torch.log(policy.view(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_policy = torch.tensor([[2]])\n",
    "policy_dist = torch.tensor([[0.01,0.01,3.0]])\n",
    "policy_loss = nn.NLLLoss()(policy_dist, target_policy.view(-1))\n",
    "policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(12.3789, grad_fn=<AddBackward0>),\n",
       " 'log': {'train_loss': tensor(12.3789, grad_fn=<AddBackward0>)}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_step(batch,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [{'input_text':'\"$%'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericaliser. Ex: 'This is a test' -> [1212, 318, 257, 1332]\n"
     ]
    }
   ],
   "source": [
    "numericalise_transform = Numericalise_Transform(numericaliser=lambda inp: tokenizer.encode(inp, add_special_tokens=False), \n",
    "                                                fields=[('input_text', 'input_seq')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_text': '\"$%', 'input_seq': [1, 3, 4]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = numericalise_transform(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 3, 50263, 50262], 'target_policy': [4]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_creator_transform = Scratch_Pad_Sequence_Policy_Creator_Transform(execution_token_id=tokenizer.get_vocab()['>>>'],\n",
    "                                                                         newline_token_id=tokenizer.get_vocab()['\\n'],\n",
    "                                                                         mask_token_id=tokenizer.get_vocab()['[MASK]'],\n",
    "                                                                         value_token_id=tokenizer.get_vocab()['[VALUE]'])\n",
    "policy_train_data = policy_creator_transform(train_data)\n",
    "policy_train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [50263, 50262], 'target_policy': [1]},\n",
       " {'input_ids': [1, 50263, 50262], 'target_policy': [3]},\n",
       " {'input_ids': [1, 3, 50263, 50262], 'target_policy': [4]}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d23ef280f3645c1b46f06c6891e64e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scratch_pad_policy_dataset = Scratch_Pad_Policy_Dataset(train_data, slow_pipe=[numericalise_transform, policy_creator_transform], \n",
    "                                                        real_time_pipe=[], PAD=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50258, 50258, 50263, 50262],\n",
       "         [50258,     1, 50263, 50262],\n",
       "         [    1,     3, 50263, 50262]]),\n",
       " 'attention_mask': tensor([[0., 0., 1., 1.],\n",
       "         [0., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " 'target_policy': tensor([[1],\n",
       "         [3],\n",
       "         [4]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = scratch_pad_policy_dataset.to_dataloader(3)\n",
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.5069, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_step(batch,0)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['target_policy'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5614,  0.4802, -0.9755,  0.6096, -0.5561],\n",
      "        [ 0.2958, -0.8632, -0.6333, -0.3695,  0.4399],\n",
      "        [-0.8264, -0.4387,  0.2190,  0.3989,  0.2264]], requires_grad=True)\n",
      "tensor([1, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.6296, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "inp = torch.randn(3, 5, requires_grad=True)\n",
    "print(inp)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(inp, target)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0949, -0.2459,  0.0204,  0.0996,  0.0310],\n",
       "        [ 0.0987,  0.0310, -0.2944,  0.0507,  0.1140],\n",
       "        [ 0.0288,  0.0424, -0.2515,  0.0979,  0.0824]])"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.000998951611109078:  10%|▉         | 974/10000 [00:06<01:04, 140.56it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-6d2ddcf16e63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm.tqdm(range(10000))\n",
    "opt = model.configure_optimizers()\n",
    "it = iter(train_dataloader)\n",
    "for i in pbar:\n",
    "    try:\n",
    "        opt.zero_grad()\n",
    "        loss = model.training_step(next(it),0)['loss']\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        pbar.set_description(str(float(loss)))\n",
    "    except StopIteration:\n",
    "        it = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name        | Type      | Params\n",
      "------------------------------------------\n",
      "0 | BERT        | BertModel | 532 K \n",
      "1 | dropout     | Dropout   | 0     \n",
      "2 | value_layer | Linear    | 11    \n",
      "3 | LM_layer    | Linear    | 552 K \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(gradient_clip_val=0.5, amp_level='O1')\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'[MASK][VALUE]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.3421,  3.2267, -9.8514,  1.2210, 11.6785, -9.6219, -9.7639, -8.8435,\n",
       "        -9.0634, -9.5598], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(torch.tensor([[1,3,50263, 50262]]))[1][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2_Scratch_Pad_Batch_Numericaliser_Transform():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side='left', pad_token='<pad>', eos_token='<eos>',\n",
    "                                                       additional_special_tokens=['\\n', '<ScratchPad>', '</ScratchPad>'])\n",
    "        \n",
    "    def __call__(self, samples, **kwargs):\n",
    "        '''\n",
    "        samples: [dict]: [{'sequences':['this is a test', 'do <ScratchPad>4+4>>>8\\n']}]\n",
    "        returns: [dict]: [{'input_ids'&'attention_mask'&'target_ids':tensor[batch_size, seq_len], 'pad_id':666, 'sequences':['this is a test',...]}]\n",
    "        '''\n",
    "        execution_token_id = self.tokenizer.get_vocab()['>>>']\n",
    "        newline_token_id = self.tokenizer.get_vocab()['\\n']\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        for sample_obj in samples:\n",
    "            model_inputs = tokens = self.tokenizer(sample_obj['sequences'], return_tensors='pt', padding=True)\n",
    "            input_ids = model_inputs['input_ids']\n",
    "            attention_mask = model_inputs['attention_mask']\n",
    "            target_ids = model_inputs['input_ids'].clone()\n",
    "            batch_size = input_ids.shape[0]\n",
    "            seq_len = input_ids.shape[1]\n",
    "            for i in range(batch_size):\n",
    "                auto_gen_token_mode = False\n",
    "                for j in range(seq_len):\n",
    "                    target_ids[i][j] = pad_token_id if auto_gen_token_mode else target_ids[i][j]\n",
    "                    if input_ids[i][j] == execution_token_id:\n",
    "                        auto_gen_token_mode = True\n",
    "                    elif input_ids[i][j] == newline_token_id:\n",
    "                        auto_gen_token_mode = False\n",
    "            sample_obj['input_ids'] = input_ids[:,:-1]\n",
    "            sample_obj['attention_mask'] = attention_mask[:,:-1]\n",
    "            sample_obj['target_ids'] = target_ids[:,1:]\n",
    "            sample_obj['pad_id'] = pad_token_id\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequences': ['do <ScratchPad>4+4>>>8\\n</ScratchPad>8<eos>'],\n",
       "  'input_ids': tensor([[ 4598, 50260,    19,    10,    19, 33409,    23, 50259, 50261,    23]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       "  'target_ids': tensor([[50260,    19,    10,    19, 33409, 50258, 50258, 50261,    23, 50257]]),\n",
       "  'pad_id': 50258}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP_numericaliser_transform = GPT2_Scratch_Pad_Batch_Numericaliser_Transform()\n",
    "test_samples = [{'sequences':['do <ScratchPad>4+4>>>8\\n</ScratchPad>8<eos>']}]\n",
    "SP_numericaliser_transform(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequences': ['do <ScratchPad>4+4>>>8\\n</ScratchPad>8<eos>'],\n",
       " 'input_ids': tensor([[ 4598, 50260,    19,    10,    19, 33409,    23, 50259, 50261,    23]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'target_ids': tensor([[50260,    19,    10,    19, 33409, 50258, 50258, 50261,    23, 50257]]),\n",
       " 'pad_id': 50258}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(10.8125, grad_fn=<NllLossBackward>),\n",
       " 'logits': tensor([[[-0.0474, -0.0402,  0.1575,  ..., -0.0349,  0.0304,  0.0153],\n",
       "          [-0.0620, -0.0391, -0.1390,  ...,  0.0104, -0.0307, -0.0093],\n",
       "          [ 0.0239, -0.0122, -0.1372,  ...,  0.0544,  0.0018, -0.0160],\n",
       "          ...,\n",
       "          [-0.0334,  0.0984,  0.0767,  ..., -0.0619, -0.0424, -0.0090],\n",
       "          [ 0.0285,  0.0038, -0.1458,  ...,  0.1122, -0.0479,  0.0007],\n",
       "          [ 0.0172, -0.0707,  0.0732,  ..., -0.0518,  0.0464,  0.0136]]],\n",
       "        grad_fn=<UnsafeViewBackward>)}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_step(test_samples[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d1ef63342931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# del test_samples[0]['target_ids']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# del test_samples[0]['pad_id']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtest_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'sequences'"
     ]
    }
   ],
   "source": [
    "# del test_samples[0]['sequences']\n",
    "# del test_samples[0]['target_ids']\n",
    "# del test_samples[0]['pad_id']\n",
    "torch.argmax(model(**test_samples[0])[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name        | Type      | Params\n",
      "------------------------------------------\n",
      "0 | transformer | GPT2Model | 510 K \n",
      "1 | lm_head     | Linear    | 502 K \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809ffe6ed3ac4b2a990301d3df0cc828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "trainer = Trainer(gradient_clip_val=0.5, amp_level='O1')\n",
    "trainer.fit(model, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = json.load(open('people.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Oliver', 'Dentist'], ['Noah', 'Registered Nurse'], ['George', 'Pharmacist']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_sentences = [f\"{name} is a {job}\" for name, job in people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_people_corpus = [re.findall(r\"[\\w']+|[.,!?;]\", doc) for doc in people_sentences]\n",
    "bm25 = BM25Okapi(tokenized_people_corpus)\n",
    "def search(q, n=1):\n",
    "    return bm25.get_top_n(q.split(), people_sentences, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oliver is a Dentist']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('Oliver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Oliver is a Dentist']\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_exec('''search('Oliver')''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding the generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_generate_utils import GenerationMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers.generation_beam_search import BeamScorer, BeamSearchScorer\n",
    "from transformers.generation_logits_process import (\n",
    "    HammingDiversityLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    NoBadWordsLogitsProcessor,\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    PrefixConstrainedLogitsProcessor,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GenerationMixin.generate of <src.custom_generate_utils.GenerationMixin object at 0x7f729bffa860>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerationMixin().generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side='left', pad_token='<pad>', additional_special_tokens=['\\n', '<ScratchPad>', '</ScratchPad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decode() missing 1 required positional argument: 'token_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-376-bc8599e47df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: decode() missing 1 required positional argument: 'token_ids'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(['foo', 'bar baz'], padding=True, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens:  tensor([[ 5661,   318,   257,  1332, 50259,    19, 33409]])\n",
      "baz\n",
      "i'm in beam search mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this is a test<ScratchPad>4>>>4\\n decidedly decidedlymineilage force force',\n",
       " 'this is a test<ScratchPad>4>>>4\\n decidedly decidedlyionsions 84 84']"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.custom_generate_utils import GenerationMixin\n",
    "model.generate = GenerationMixin.generate\n",
    "model.beam_sample = GenerationMixin.beam_sample\n",
    "model.beam_search = GenerationMixin.beam_search\n",
    "\n",
    "input_ids = tokenizer(['this is a test <ScratchPad>4>>>'], padding=True, return_tensors='pt').input_ids\n",
    "print(\"Input tokens: \",  input_ids)\n",
    "gen_sequences = model.generate(model, input_ids, max_length=15, num_beams=2, num_beam_groups=1, diversity_penalty=0, use_cache=False, pad_token_id=50257, \n",
    "               do_sample=False, num_return_sequences=2, postfix_additional_tokens_fn=scratch_pad_complete)\n",
    "[tokenizer.decode(gen_sequences[i], spaces_between_special_tokens=False) for i in range(gen_sequences.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5661,   318,   257,  1332, 50259,    64,    28,    18, 33409]])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(['this is a test<ScratchPad>a=3>>>'], padding=True, return_tensors='pt')\n",
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test<ScratchPad>a=3>>>'"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 5661,   318,   257,  1332, 50259,    64,    28,    18, 33409], spaces_between_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_ids!=50257).to(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Service Current, Category', '9991']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Service Current', 'Service', '9991', '1.22']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'mysrv events Generating Event Name <ScratchPad>Service Current, Category</ScratchPad> [Service] Test <ScratchPad>9991</ScratchPad> Value [1.22]'\n",
    "print(re.findall(r'\\<ScratchPad\\>([^]]*)\\</ScratchPad\\>', s))\n",
    "['Service Current', 'Service', '9991', '1.22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9991</ScratchPad> Value [1.22]'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.rpartition('<ScratchPad>')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
