{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-graph N-beddings\n",
    "![Drag Racing](images/HyperGraph_Nbedding.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "import torch.nn as nn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.rand((3,8))\n",
    "edges = torch.tensor([[0,1,0],\n",
    "                      [0,0,0],\n",
    "                      [2,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge_Attention(Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.Wq = Linear(embed_dim,embed_dim)\n",
    "        self.Wk = Linear(embed_dim,embed_dim)\n",
    "        self.Wv = Linear(embed_dim,embed_dim)\n",
    "        \n",
    "        self.edge_embedding = nn.Embedding(3, embed_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values, edge_matrix):\n",
    "        # edge_matrix [L,L]\n",
    "        seq_len = queries.shape[0]\n",
    "        Q = self.Wq(queries) # [L,E]\n",
    "        K = self.Wk(keys) # [L,E]\n",
    "        V = self.Wv(values) # [L,E]\n",
    "        \n",
    "        attention_matrix = torch.zeros((seq_len,seq_len)) # [L,L]\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                edge_ij_emb = self.edge_embedding(edge_matrix[i,j])\n",
    "                attention_matrix[i,j] = torch.dot(Q[i]+edge_ij_emb, K[j]+edge_ij_emb)\n",
    "                \n",
    "        attention_matrix = torch.softmax(attention_matrix, dim=-1)\n",
    "        output = torch.matmul(attention_matrix, V)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0125,  0.1365,  0.0868, -0.0329,  0.0552, -0.2522,  0.0808,  0.0194],\n",
      "        [-0.1271,  0.0905,  0.0298, -0.0840,  0.1488, -0.2664,  0.2652, -0.0683],\n",
      "        [-0.0094,  0.1227,  0.0752, -0.0388,  0.0496, -0.2534,  0.0653,  0.0209]],\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "ea = Edge_Attention(8)\n",
    "ea(src,src,src,edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Edge_Attention(Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        '''\n",
    "        Edge attation computation, provides structured information to the attention computation. \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.Wq = Linear(embed_dim,embed_dim)\n",
    "        self.Wk = Linear(embed_dim,embed_dim)\n",
    "        self.Wv = Linear(embed_dim,embed_dim)\n",
    "        \n",
    "        self.edge_embedding = nn.Embedding(3, embed_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values, edge_matrix):\n",
    "        '''\n",
    "        L sequence length, N Batch size, E embeding dim\n",
    "        queries: [L,N,E]\n",
    "        keys: [L,N,E]\n",
    "        values: [L,N,E]\n",
    "        edge_matrix: [N,L,L]\n",
    "        \n",
    "        returns: [L,N,E]\n",
    "        >>> src = torch.rand((2,3,8))\n",
    "        >>> edges = torch.tensor([[[0,1,0],\n",
    "                                   [0,0,0],\n",
    "                                   [2,2,0]],\n",
    "                                  [[0,0,0],\n",
    "                                   [1,0,0],\n",
    "                                   [2,2,0]]])\n",
    "        >>> ea = Batch_Edge_Attention(8)\n",
    "        >>> ea(src,src,src,edges).shape\n",
    "        '''\n",
    "        queries = queries.permute(1,0,2)\n",
    "        keys = keys.permute(1,0,2)\n",
    "        values = values.permute(1,0,2)\n",
    "        batch_size = edge_matrix.shape[0]\n",
    "        seq_len = queries.shape[1]\n",
    "        Q = self.Wq(queries) # [N,L,E]\n",
    "        K = self.Wk(keys) # [N,L,E]\n",
    "        V = self.Wv(values) # [N,L,E]\n",
    "        \n",
    "        attention_matrix = torch.zeros((batch_size,seq_len,seq_len)) # [N,L,L]\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                edge_ij_emb = self.edge_embedding(edge_matrix[:,i,j])\n",
    "                A = Q[:,i]+edge_ij_emb\n",
    "                B = K[:,j]+edge_ij_emb\n",
    "                attention_matrix[:,i,j] = torch.bmm(A.unsqueeze(dim=1), B.unsqueeze(dim=2)).squeeze()\n",
    "                \n",
    "        attention_matrix = torch.softmax(attention_matrix, dim=-1)\n",
    "        output = torch.bmm(attention_matrix, V)\n",
    "        output = output.permute(1,0,2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 3\n",
    "embed_dim = 128\n",
    "device = \"cpu\"\n",
    "src = torch.rand((seq_len,2,embed_dim), device=device)\n",
    "edges = torch.tensor([[[0,1,0],\n",
    "                       [0,0,0],\n",
    "                       [2,2,0]],\n",
    "                      \n",
    "                      [[0,0,0],\n",
    "                       [1,0,0],\n",
    "                       [2,2,0]]], device=device)\n",
    "# edges = torch.randint(0,3,(2,embed_dim,embed_dim), device=device)\n",
    "ea = Batch_Edge_Attention(embed_dim).to(device)\n",
    "ea(src,src,src,edges).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 389 ms, sys: 4.92 ms, total: 394 ms\n",
      "Wall time: 25.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 128])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time ea(src,src,src,edges).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = MultiheadAttention(embed_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 740 ms, sys: 4.06 ms, total: 744 ms\n",
      "Wall time: 49.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 2, 128])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time self_attn(src, src, src)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving speed with scatter and sparce matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fast_Edge_Attention(Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        '''\n",
    "        Edge attation computation, provides structured information to the attention computation. \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.Wq = Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.Wk = Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.Wv = Linear(embed_dim,embed_dim, bias=False)\n",
    "        \n",
    "        self.edge_embedding = nn.Embedding(3, embed_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values, edge_matrix, global_attention=False, mode='reading'):\n",
    "        '''\n",
    "        L sequence length, N Batch size, E embeding dim\n",
    "        queries: [L,N,E]\n",
    "        keys: [L,N,E]\n",
    "        values: [L,N,E]\n",
    "        edge_matrix: [N,L,L]\n",
    "        \n",
    "        returns: [L,N,E]\n",
    "        >>> src = torch.rand((2,3,8))\n",
    "        >>> edges = torch.tensor([[[0,1,0],\n",
    "                                   [0,0,0],\n",
    "                                   [2,2,0]],\n",
    "                                  [[0,0,0],\n",
    "                                   [1,0,0],\n",
    "                                   [2,2,0]]])\n",
    "        >>> ea = Batch_Edge_Attention(8)\n",
    "        >>> ea(src,src,src,edges).shape\n",
    "        '''\n",
    "        \n",
    "        if mode=='reading':\n",
    "            edge_matrix = edge_matrix.permute(0,2,1)\n",
    "        \n",
    "        queries = queries.permute(1,0,2)\n",
    "        keys = keys.permute(1,0,2)\n",
    "        values = values.permute(1,0,2)\n",
    "        batch_size = edge_matrix.shape[0]\n",
    "        seq_len = queries.shape[1]\n",
    "        Q = self.Wq(queries) # [N,L,E]\n",
    "        K = self.Wk(keys) # [N,L,E]\n",
    "        V = self.Wv(values) # [N,L,E]\n",
    "        \n",
    "        if global_attention:\n",
    "            attention_matrix = torch.bmm(Q,K.permute(0,2,1))\n",
    "        else:\n",
    "            attention_matrix = torch.full((batch_size,seq_len,seq_len), -float('inf'))\n",
    "        \n",
    "        sparse_edges = edge_matrix.to_sparse()\n",
    "        sparse_edges_indices = sparse_edges.indices()\n",
    "        query_indices = sparse_edges_indices[[True, True, False]]\n",
    "        key_indices = sparse_edges_indices[[True, False, True]]\n",
    "        \n",
    "        query_edge_vectors = Q[query_indices[0],query_indices[1]]\n",
    "        key_edge_vectors = Q[key_indices[0],key_indices[1]]\n",
    "        \n",
    "        indexed_edge_embeddings = self.edge_embedding(sparse_edges.values())\n",
    "#         query_edge_vectors += indexed_edge_embeddings\n",
    "        key_edge_vectors += indexed_edge_embeddings\n",
    "        \n",
    "        edge_attention_values = torch.bmm(query_edge_vectors.unsqueeze(1),key_edge_vectors.unsqueeze(2)).squeeze()\n",
    "        attention_matrix[sparse_edges_indices[0],sparse_edges_indices[1],sparse_edges_indices[2]] = edge_attention_values\n",
    "                \n",
    "        attention_scores = torch.softmax(attention_matrix, dim=-1)\n",
    "        attention_scores[attention_scores != attention_scores] = 0\n",
    "        print(attention_scores, attention_scores.shape, V, V.shape)\n",
    "        output = torch.bmm(attention_scores, V)\n",
    "        output = output.permute(1,0,2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea.Wv.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.5000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 1.0000, 0.0000],\n",
       "         [0.5000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(attention_scores, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 3\n",
    "embed_dim = 4\n",
    "device = \"cpu\"\n",
    "src = torch.tensor([\n",
    "    [[1,0,0,0],\n",
    "     [1,0,0,0]],\n",
    "    \n",
    "    [[0,1,0,0],\n",
    "     [0,1,0,0]],\n",
    "    \n",
    "    [[0,0,1,0],\n",
    "     [0,0,1,0]]\n",
    "], dtype=torch.float32, device=device)\n",
    "edges = torch.tensor([[[0,1,2],\n",
    "                       [0,0,2],\n",
    "                       [0,0,0]],\n",
    "                      \n",
    "                      [[0,0,2],\n",
    "                       [1,0,2],\n",
    "                       [0,0,0]]], device=device)\n",
    "fea = Fast_Edge_Attention(embed_dim).to(device)\n",
    "fea.edge_embedding.weight.data = torch.zeros(3,4)\n",
    "fea.Wk.weight.data = torch.eye(4,4)\n",
    "fea.Wq.weight.data = torch.eye(4,4)\n",
    "fea.Wv.weight.data = torch.eye(4,4)\n",
    "# fea.edge_embedding.weight.data[[1],[0]] = 1\n",
    "# fea.edge_embedding.weight.data[[2],[0]] = 2\n",
    "# edges = torch.randint(0,3,(2,embed_dim,embed_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 1.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000]]], grad_fn=<IndexPutBackward>) torch.Size([2, 3, 3]) tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.]]], grad_fn=<UnsafeViewBackward>) torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000, 0.0000]]], grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, attention_scores, V = fea(src,src,src,edges, global_attention=False, mode='reading')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 21836])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.to_sparse().indices().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 660 ms, sys: 213 ms, total: 872 ms\n",
      "Wall time: 64.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 2, 128])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fea(src,src,src,edges).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoBoT (Autoencoding BOttom-up Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.core.lightning import LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the nodes are the embeddings for the most low level properties intrinsic to nodes themselves.\n",
    "# 0 -> pad\n",
    "# 1 -> concept node\n",
    "# 2 -> eos\n",
    "# 3-100 -> raw token nodes: \" the\", \"#of\", \" foo\"\n",
    "nodes = torch.tensor([3,4,1,5,6,7,1,8])\n",
    "\n",
    "# The adjacency matrix represents 3 types of relationships\n",
    "# 0 -> no rellationship\n",
    "# 1 -> directional peer connection\n",
    "# 2 -> directional parent connection\n",
    "adjacency_matix = torch.tensor([[0,0,2,0,0,0,0,0],\n",
    "                                [1,0,2,0,0,0,0,0],\n",
    "                                [0,0,0,0,0,0,0,0],\n",
    "                                [0,0,0,0,0,0,2,0],\n",
    "                                [0,0,0,1,0,0,2,0],\n",
    "                                [0,0,0,0,1,0,2,0],\n",
    "                                [0,0,1,0,0,0,0,0],\n",
    "                                [0,0,1,0,0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 1, 3, 4, 4, 5, 5, 6, 7, 7],\n",
       "                       [2, 0, 2, 6, 3, 6, 4, 6, 2, 2, 6]]),\n",
       "       values=tensor([2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1]),\n",
       "       size=(8, 8), nnz=11, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_matix.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find kernel to dispatch to for operator 'aten::_softmax'. Tried to look up kernel for dispatch key 'SparseCPUTensorId'. Registered dispatch keys are: [CUDATensorId, CPUTensorId, VariableTensorId, MkldnnCPUTensorId]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-f7a89dff95f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Didn't find kernel to dispatch to for operator 'aten::_softmax'. Tried to look up kernel for dispatch key 'SparseCPUTensorId'. Registered dispatch keys are: [CUDATensorId, CPUTensorId, VariableTensorId, MkldnnCPUTensorId]"
     ]
    }
   ],
   "source": [
    "torch.tensor([1,0,2,0]).to_sparse().softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 1, 3, 4, 4, 5, 5, 6, 7, 7],\n",
       "                       [2, 0, 2, 6, 3, 6, 4, 6, 2, 2, 6],\n",
       "                       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "       values=tensor([2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1]),\n",
       "       size=(8, 8, 1), nnz=11, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_matix.unsqueeze(-1).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n",
      "                       [0, 0, 1, 0, 1, 1],\n",
      "                       [1, 2, 2, 2, 0, 2]]),\n",
      "       values=tensor([1, 2, 2, 2, 1, 2]),\n",
      "       size=(2, 3, 3), nnz=6, layout=torch.sparse_coo)\n",
      "foo\n",
      "tensor(indices=tensor([[0, 0, 1],\n",
      "                       [1, 2, 2]]),\n",
      "       values=tensor([1, 2, 2]),\n",
      "       size=(3, 3), nnz=3, layout=torch.sparse_coo)\n",
      "foo\n",
      "tensor(indices=tensor([[0, 1, 1],\n",
      "                       [2, 0, 2]]),\n",
      "       values=tensor([2, 1, 2]),\n",
      "       size=(3, 3), nnz=3, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "class Graph_Edge_Attention(Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        '''\n",
    "        Edge attation computation, provides structured information to the attention computation. \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.Wq = Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.Wk = Linear(embed_dim,embed_dim, bias=False)\n",
    "        self.Wv = Linear(embed_dim,embed_dim, bias=False)\n",
    "        \n",
    "        self.edge_embedding = nn.Embedding(3, embed_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values, edge_matrix, global_attention=False, mode=\"reading\"):\n",
    "        '''\n",
    "        L sequence length, N Batch size, E embeding dim\n",
    "        queries: [L,N,E]\n",
    "        keys: [L,N,E]\n",
    "        values: [L,N,E]\n",
    "        edge_matrix: [N,L,L]\n",
    "        \n",
    "        returns: [L,N,E]\n",
    "        '''\n",
    "        queries = queries.permute(1,0,2)\n",
    "        keys = keys.permute(1,0,2)\n",
    "        values = values.permute(1,0,2)\n",
    "        batch_size = edge_matrix.shape[0]\n",
    "        seq_len = queries.shape[1]\n",
    "        Q = self.Wq(queries) # [N,L,E]\n",
    "        K = self.Wk(keys) # [N,L,E]\n",
    "        V = self.Wv(values) # [N,L,E]\n",
    "        \n",
    "        sparse_edges = edge_matrix.to_sparse()\n",
    "        sparse_edges_indices = sparse_edges.indices()\n",
    "        if mode == \"reading\":\n",
    "            query_indices = sparse_edges_indices[[True, True, False]]\n",
    "            key_indices = sparse_edges_indices[[True, False, True]]\n",
    "        if mode == \"writing\":\n",
    "            key_indices = sparse_edges_indices[[True, True, False]]\n",
    "            query_indices = sparse_edges_indices[[True, False, True]]\n",
    "        else:\n",
    "            print(\"MODE NOT RECOGNISED\")\n",
    "        \n",
    "        query_edge_vectors = Q[query_indices[0],query_indices[1]]\n",
    "        key_edge_vectors = Q[key_indices[0],key_indices[1]]\n",
    "        \n",
    "        indexed_edge_embeddings = self.edge_embedding(sparse_edges.values())\n",
    "        \n",
    "        sparse_edges = edges.to_sparse()\n",
    "        for batch_idx, query_node_idx, key_node_idx in sparse_edges.T:\n",
    "            print(\"foo\")\n",
    "            print(edge)\n",
    "            \n",
    "seq_len = 3\n",
    "embed_dim = 128\n",
    "device = \"cpu\"\n",
    "src = torch.zeros((seq_len,2,embed_dim), device=device)\n",
    "edges = torch.tensor([[[0,1,2],\n",
    "                       [0,0,2],\n",
    "                       [0,0,0]],\n",
    "                      \n",
    "                      [[0,0,2],\n",
    "                       [1,0,2],\n",
    "                       [0,0,0]]], device=device)\n",
    "# edges = torch.randint(0,3,(2,embed_dim,embed_dim), device=device)\n",
    "gea = Graph_Edge_Attention(embed_dim).to(device)\n",
    "gea.edge_embedding.weight.data = torch.zeros(3,128)\n",
    "gea.edge_embedding.weight.data[[1],[0]] = 1\n",
    "gea.edge_embedding.weight.data[[2],[0]] = 2\n",
    "gea(src,src,src,edges, global_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoBoT(LightningModule):\n",
    "    def __init__(self, d_model=512, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.edge_embedding = nn.Embedding(3, embed_dim)\n",
    "        self.node_embedding = nn.Embedding(100, embed_dim)\n",
    "        \n",
    "        self.reader = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'c':4,'b':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.pop('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 5}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
