{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy-Generator Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import operator\n",
    "import collections\n",
    "import json\n",
    "from dotmap import DotMap\n",
    "\n",
    "from models_and_trainers.copy_gen_transformer import TransformerModel, PositionalEncoding\n",
    "from models_and_trainers.exposed_transformer import Transformer, TransformerDecoderLayer, TransformerDecoder\n",
    "import utils.beam_search as beam_search\n",
    "from IPython.core.debugger import set_trace as tr\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CONFIG'] = \"config_copy_gen_vcb1600_maxlen200.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config_fp = os.environ['CONFIG']\n",
    "except:\n",
    "    config_fp = \"config.json\"\n",
    "    \n",
    "with open(config_fp) as config_file:\n",
    "    config = json.loads(config_file.read())\n",
    "    config = DotMap(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env CONFIG: config_copy_gen_vcb1600_maxlen200.json\n",
      "DotMap(max_seq_length=200, vocab_size=1600, log_file_name='logs_copy_gen_vcb1600_maxlen2000.txt', out_file_name='out_copy_gen_vcb1600_maxlen2000.txt', train_steps=1000000, train_learning_rate=0.005, src_file='./datasets/all-fixed.desc', tgt_file='./datasets/all.code', eval_interval=10000, log_interval=400, eval_beam_size=1, train_batch_size=32, eval_batch_size=32, model_layers=4, model_att_heads=8, model_embed_dim=512, model_dim_feedforward=1024, model_att_mask_noise=0.0)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def super_print(filename):\n",
    "    '''filename is the file where output will be written'''\n",
    "    def wrap(func):\n",
    "        '''func is the function you are \"overriding\", i.e. wrapping'''\n",
    "        def wrapped_func(*args,**kwargs):\n",
    "            '''*args and **kwargs are the arguments supplied \n",
    "            to the overridden function'''\n",
    "            #use with statement to open, write to, and close the file safely\n",
    "            with open(filename,'a', encoding=\"utf-8\") as outputfile:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                outputfile.write(\"[{}] \".format(dt_string))\n",
    "                outputfile.write(\" \".join(str(x) for x in args))\n",
    "                outputfile.write(\"\\n\")\n",
    "            #now original function executed with its arguments as normal\n",
    "            return func(*args,**kwargs)\n",
    "        return wrapped_func\n",
    "    return wrap\n",
    "\n",
    "print = super_print(config.log_file_name)(print)\n",
    "print(\"env CONFIG:\",config_fp)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.set_device(0) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['create', 'variable', 'student_names', 'with', 'string', \"'foo bar baz'\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"create variable student_names with string 'foo bar baz'\"\n",
    "\n",
    "def string_split(s):\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(_|\\W)', s))) # this will chunk all code properly by plits strings with quotes\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|_|\\W)', s))) # this keeps the strings intact\n",
    "    return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|\\W)', s)))\n",
    "\n",
    "print(string_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_array(src_fp, tgt_fp):\n",
    "    lines = []\n",
    "    with open(src_fp, \"r\", encoding=\"utf-8\") as src_file, open(tgt_fp, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        for src, tgt in zip(src_file, tgt_file):\n",
    "            lines.append((src, tgt))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(data, max_seq_length=200, tokenizer=string_split):\n",
    "    return [(src, tgt) for src, tgt in data if len(string_split(src)) <= max_seq_length and len(string_split(tgt)) <= max_seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_dataset(samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples: [(src_string),(tgt_string)]\n",
    "        src/tgt_tokenizer: a func that takes a string and returns an array of strings\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    TEXT_FIELD = Field(sequential=True, use_vocab=False, init_token='<sos>',eos_token='<eos>')\n",
    "    \n",
    "    for sample in samples:\n",
    "        src_string, tgt_string = sample\n",
    "        examples.append(torchtext.data.Example.fromdict({\"src\":src_string, \"tgt\":tgt_string}, \n",
    "                                        fields={\"src\":(\"src\",TEXT_FIELD), \"tgt\":(\"tgt\",TEXT_FIELD)}))\n",
    "        \n",
    "    dataset = torchtext.data.Dataset(examples,fields={\"src\":src_field, \"tgt\":tgt_field})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src length: 557\n",
      "Max tgt length: 527\n",
      "Full dataset size: 18805\n",
      "Limited dataset size: 18794\n"
     ]
    }
   ],
   "source": [
    "# data = corpus_to_array(config.src_file, config.tgt_file)\n",
    "data = corpus_to_array(\"datasets/all.desc\", \"datasets/all.code\")\n",
    "random.shuffle(data)\n",
    "print(\"Max src length:\", max([len(string_split(src)) for src, tgt in data]))\n",
    "print(\"Max tgt length:\", max([len(string_split(tgt)) for src, tgt in data]))\n",
    "\n",
    "print(\"Full dataset size:\", len(data))\n",
    "max_seq_length=config.max_seq_length\n",
    "data = filter_corpus(data, max_seq_length=max_seq_length, tokenizer=string_split)\n",
    "print(\"Limited dataset size:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  call the method __init__ with an argument lexer, from the base class of the class DebugParser.\\n',\n",
       "  '          super ( DebugParser , self ) . __init__ ( lexer )\\n'),\n",
       " ('  call the method self.extend_nodelist with 3 arguments: nodelist, var_node and token.\\n',\n",
       "  ' self . extend_nodelist ( nodelist , var_node , token )\\n'),\n",
       " ('  if instance is None,\\n', '          if instance is None :\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "          \"my favourite foods are banana and toast\",\n",
    "          \"my favourite foods are eggs and bacon and beans\",\n",
    "          \"my favourite food is chocolate\",\n",
    "          \"my favourite food is avocado\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "           \"would you like banana and toast ?\",\n",
    "           \"would you like eggs and bacon and beans ?\",\n",
    "           \"would you like chocolate ?\",\n",
    "           \"would you like avocado ?\"\n",
    "]\n",
    "# max_seq_length = 9\n",
    "# data = list(zip(inputs, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {\"<unk>\":0, \"<sos>\":1, \"<eos>\":2, \"<pad>\":3}\n",
    "max_vocab = config.vocab_size - len(stoi)\n",
    "\n",
    "all_toks = []\n",
    "for (src, tgt) in data:\n",
    "    all_toks += string_split(src)\n",
    "    all_toks += string_split(tgt)\n",
    "\n",
    "most_freq = collections.Counter(all_toks).most_common(max_vocab)\n",
    "\n",
    "for tok, count in most_freq:\n",
    "    stoi[tok] = len(stoi)\n",
    "    \n",
    "itos = [k for k,v in sorted(stoi.items(), key=lambda kv: kv[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(string):\n",
    "    OOVs = []\n",
    "    IDs = []\n",
    "    words = string_split(string)\n",
    "    for word in words:\n",
    "        try:\n",
    "            id = stoi[word]\n",
    "            IDs.append(id)\n",
    "        except KeyError as e:\n",
    "            # word is OOV\n",
    "            IDs.append(len(stoi) + len(OOVs))\n",
    "            OOVs.append(word)\n",
    "    return IDs, OOVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(string, OOVs):\n",
    "    IDs = []\n",
    "    words = string_split(string)\n",
    "    for word in words:\n",
    "        try:\n",
    "            id = stoi[word]\n",
    "            IDs.append(id)\n",
    "        except KeyError as e:\n",
    "            # word is OOV\n",
    "            try:\n",
    "                IDs.append(len(stoi) + OOVs.index(word))\n",
    "            except ValueError as e:\n",
    "                IDs.append(stoi[\"<unk>\"])\n",
    "    return IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, OOVs):\n",
    "    extended_itos = itos.copy()\n",
    "    extended_itos += [OOV+\"(COPY)\" for OOV in OOVs]\n",
    "    return \" \".join([extended_itos[id] for id in ids if id<len(extended_itos)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FIELD = Field(sequential=True, use_vocab=False, unk_token=0, init_token=1,eos_token=2, pad_token=3)\n",
    "OOV_TEXT_FIELD = Field(sequential=True, use_vocab=False, pad_token=3)\n",
    "\n",
    "OOV_stoi = {}\n",
    "OOV_itos = {}\n",
    "OOV_starter_count = 30000\n",
    "OOV_count = OOV_starter_count\n",
    "\n",
    "examples = []\n",
    "\n",
    "for (src, tgt) in data:\n",
    "    src_ids, OOVs = encode_input(src)\n",
    "    tgt_ids = encode_output(tgt, OOVs)\n",
    "    OOV_ids = []\n",
    "    \n",
    "    for OOV in OOVs:\n",
    "        try:\n",
    "            idx = OOV_stoi[OOV]\n",
    "            OOV_ids.append(idx)\n",
    "        except KeyError as e:\n",
    "            OOV_count += 1\n",
    "            OOV_stoi[OOV] = OOV_count\n",
    "            OOV_itos[OOV_count] = OOV\n",
    "            OOV_ids.append(OOV_count)\n",
    "            \n",
    "    examples.append(torchtext.data.Example.fromdict({\"src\":src_ids, \"tgt\":tgt_ids, \"OOVs\":OOV_ids}, \n",
    "                                                    fields={\"src\":(\"src\",TEXT_FIELD), \"tgt\":(\"tgt\",TEXT_FIELD), \"OOVs\":(\"OOVs\", OOV_TEXT_FIELD)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Hearthstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not DotMap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a54fbdd800aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_src_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_tgt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2a3df7abf2a1>\u001b[0m in \u001b[0;36mcorpus_to_array\u001b[0;34m(src_fp, tgt_fp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorpus_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtgt_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not DotMap"
     ]
    }
   ],
   "source": [
    "test_data = corpus_to_array(config.eval_src_file, config.eval_tgt_file)\n",
    "test_data = filter_corpus(test_data, max_seq_length=max_seq_length, tokenizer=string_split)\n",
    "\n",
    "test_examples = []\n",
    "\n",
    "for (src, tgt) in test_data:\n",
    "    src_ids, OOVs = encode_input(src)\n",
    "    tgt_ids = encode_output(tgt, OOVs)\n",
    "    OOV_ids = []\n",
    "    \n",
    "    for OOV in OOVs:\n",
    "        try:\n",
    "            idx = OOV_stoi[OOV]\n",
    "            OOV_ids.append(idx)\n",
    "        except KeyError as e:\n",
    "            OOV_count += 1\n",
    "            OOV_stoi[OOV] = OOV_count\n",
    "            OOV_itos[OOV_count] = OOV\n",
    "            OOV_ids.append(OOV_count)\n",
    "            \n",
    "    test_examples.append(torchtext.data.Example.fromdict({\"src\":src_ids, \"tgt\":tgt_ids, \"OOVs\":OOV_ids}, \n",
    "                                                    fields={\"src\":(\"src\",TEXT_FIELD), \"tgt\":(\"tgt\",TEXT_FIELD), \"OOVs\":(\"OOVs\", OOV_TEXT_FIELD)}))\n",
    "\n",
    "val_dataset = torchtext.data.Dataset(test_examples,fields={\"src\":TEXT_FIELD, \"tgt\":TEXT_FIELD, \"OOVs\":OOV_TEXT_FIELD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchtext.data.Dataset(examples,fields={\"src\":TEXT_FIELD, \"tgt\":TEXT_FIELD, \"OOVs\":OOV_TEXT_FIELD})\n",
    "train_dataset, val_dataset = dataset.split([0.9,0.1])\n",
    "# train_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 32])\n",
      "torch.Size([29, 32])\n",
      "SOURCE: <sos> if timeout is None , <eos>\n",
      "\n",
      "TARGET: <sos> if timeout is None : <eos>\n"
     ]
    }
   ],
   "source": [
    "batch_size = config.train_batch_size\n",
    "\n",
    "train_iterator = BucketIterator(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    repeat=True,\n",
    "    shuffle=True,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "# The iterator generates batches with padded length for sequences with similar sizes, a batch is [seq_length, batch_size]\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    idx = 3\n",
    "#     print([SRC_TEXT.vocab.itos[id] for id in batch.src.cpu().numpy()[:,idx]])\n",
    "    OOVs = [OOV_itos[OOV] for OOV in batch.OOVs.cpu()[:,idx].tolist() if OOV != 3] # 3 is the <pad> token\n",
    "    src_ids = batch.src.cpu()[:,idx].tolist()\n",
    "    src_ids = src_ids[:src_ids.index(2)+1]\n",
    "    tgt_ids = batch.tgt.cpu()[:,idx].tolist()\n",
    "    tgt_ids = tgt_ids[:tgt_ids.index(2)+1]\n",
    "    \n",
    "    print(batch.src.shape)\n",
    "    print(batch.tgt.shape)\n",
    "    \n",
    "    print(\"SOURCE:\",decode(src_ids, OOVs))\n",
    "    print()\n",
    "    print(\"TARGET:\",decode(tgt_ids, OOVs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foo bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, dropout=0.5):\n",
    "        super(CopyModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.embedding_size = config.model_embed_dim\n",
    "        self.pos_encoder = PositionalEncoding(self.embedding_size, dropout)\n",
    "        self.src_encoder = nn.Embedding(src_vocab_size, self.embedding_size)\n",
    "        self.tgt_encoder = nn.Embedding(tgt_vocab_size, self.embedding_size)\n",
    "        \n",
    "        self.transformer = Transformer(d_model=config.model_embed_dim, \n",
    "                                       nhead=config.model_att_heads, \n",
    "                                       num_encoder_layers=config.model_layers, \n",
    "                                       num_decoder_layers=config.model_layers, \n",
    "                                       dim_feedforward=config.model_dim_feedforward)\n",
    "        self.decoder = nn.Linear(self.embedding_size, tgt_vocab_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.p_generator = nn.Linear(config.model_embed_dim,1)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.tgt_mask = None\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.src_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "#         noise_e = 0.05 if self.training else 0.0 # this is code to add noise to the decoding process during training\n",
    "        noise_e = config.model_att_mask_noise if self.training else 0.0\n",
    "        noise_mask = (torch.rand(sz,sz) > noise_e).float()\n",
    "\n",
    "        mask = (torch.triu(torch.ones(sz,sz))).transpose(0, 1)\n",
    "        mask = torch.mul(mask, noise_mask)\n",
    "        v = (torch.sum(mask, dim=-1) == 0).float()\n",
    "\n",
    "        fix_mask = torch.zeros(sz,sz)\n",
    "        fix_mask[:,0] = 1.0\n",
    "        v = v.repeat(sz, 1).transpose(0,1)\n",
    "        fix_mask = torch.mul(fix_mask,v)\n",
    "\n",
    "        mask += fix_mask\n",
    "        \n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        self.tgt_mask = self._generate_square_subsequent_mask(len(tgt)).to(self.device)\n",
    "        \n",
    "\n",
    "        src_emb = self.src_encoder(src) * math.sqrt(self.embedding_size)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        \n",
    "        tgt_emb = self.tgt_encoder(tgt) * math.sqrt(self.embedding_size)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        \n",
    "        output, atts = self.transformer(src_emb, tgt_emb, tgt_mask=self.tgt_mask)\n",
    "        \n",
    "        \n",
    "        src_scat = src.transpose(0,1)\n",
    "        src_scat = src_scat.unsqueeze(0)\n",
    "        src_scat = torch.repeat_interleave(src_scat, tgt.shape[0], dim=0)\n",
    "#         print(\"src_scat.shqape\", src_scat.shape)\n",
    "        \n",
    "        p_gens = self.p_generator(output).sigmoid()\n",
    "        atts = atts.transpose(0,1)\n",
    "#         print(\"att.shqape\", atts.shape)\n",
    "        atts = atts * (1 - p_gens)\n",
    "                \n",
    "        output = self.decoder(output)\n",
    "#         output[:,:,12:] = -np.inf\n",
    "        output = output.softmax(-1)\n",
    "        output = output * p_gens\n",
    "        \n",
    "        output = output.scatter_add_(2,src_scat,atts)\n",
    "        output = output\n",
    "        \n",
    "        return output.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 1800])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos) + max_seq_length\n",
    "\n",
    "model = CopyModel(vocab_size,vocab_size).to(device) \n",
    "src = torch.randint(0, vocab_size, (3,2)).to(device)\n",
    "tgt = torch.randint(0, vocab_size, (5,2)).to(device)\n",
    "\n",
    "out = model(src, tgt)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_bleu(refrence, prediction):\n",
    "    \"\"\"\n",
    "    Implementation from ReCode\n",
    "    and moses multi belu script sets BLEU to 0.0 if len(toks) < 4\n",
    "    \"\"\"\n",
    "    ngram_weights = [0.25] * min(4, len(refrence))\n",
    "    return sentence_bleu([refrence], prediction, weights=ngram_weights, \n",
    "                          smoothing_function=SmoothingFunction().method3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iterator = BucketIterator(val_dataset,\n",
    "    batch_size = config.eval_batch_size,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "def batch_filter_ids(batch_list):\n",
    "    return [[id for id in l if id not in [1,2,3]] for l in batch_list]\n",
    "\n",
    "def evaluate(beam_size=1, log=False):\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    with torch.no_grad(), open(config.out_file_name, \"w\", encoding=\"utf-8\") as out_fp:\n",
    "        BLEU_scores = []\n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "            batch_size = batch.src.shape[1]\n",
    "            \n",
    "            encoder_inputs = batch.src\n",
    "            predictions = beam_search.beam_search_decode(model,\n",
    "                              batch_encoder_ids=encoder_inputs,\n",
    "                              SOS_token=stoi[\"<sos>\"],\n",
    "                              EOS_token=stoi[\"<eos>\"],\n",
    "                              PAD_token=stoi[\"<pad>\"],\n",
    "                              beam_size=beam_size,\n",
    "                              max_length=30,\n",
    "                              num_out=1)\n",
    "            \n",
    "            sources = encoder_inputs.transpose(0,1).cpu().tolist()\n",
    "            sources = batch_filter_ids(sources)\n",
    "            \n",
    "            predictions = [t[0].view(-1).cpu().tolist() for t in predictions]\n",
    "            predictions = batch_filter_ids(predictions)\n",
    "            \n",
    "            targets = batch.tgt.transpose(0,1).cpu().tolist()\n",
    "            targets = batch_filter_ids(targets)\n",
    "            \n",
    "#             print(batch.tgt)\n",
    "            \n",
    "            OOVss = [[OOV_itos[OOV] for OOV in batch.OOVs.cpu()[:,idx].tolist() if OOV != 3] for idx in range(batch_size)]\n",
    "            \n",
    "            if i % int(len(valid_iterator)/3) == 0:\n",
    "                print(\"| EVALUATION | {:5d}/{:5d} batches |\".format(i, len(valid_iterator)))\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                BLEU = nltk_bleu(targets[j], predictions[j])\n",
    "                BLEU_scores.append(BLEU)\n",
    "                \n",
    "                out_fp.write(\"SRC  :\" + decode(sources[j], OOVss[j]) + \"\\n\")\n",
    "                out_fp.write(\"TGT  :\" + decode(targets[j], OOVss[j]) + \"\\n\")\n",
    "                out_fp.write(\"PRED :\" + decode(predictions[j], OOVss[j]) + \"\\n\")\n",
    "                out_fp.write(\"BLEU :\" + str(BLEU) + \"\\n\")\n",
    "                out_fp.write(\"\\n\")\n",
    "                \n",
    "                if log:\n",
    "                    print(\"SRC  :\" + decode(sources[j], OOVss[j]))\n",
    "                    print(\"TGT  :\" + decode(targets[j], OOVss[j]))\n",
    "                    print(\"PRED :\" + decode(predictions[j], OOVss[j]))\n",
    "                    print(\"BLEU :\" + str(BLEU))\n",
    "                    print()\n",
    "        out_fp.write(\"\\n\\n| EVALUATION | BLEU: {:5.2f} |\\n\".format(np.average(BLEU_scores)))\n",
    "        print(\"| EVALUATION | BLEU: {:5.3f} |\".format(np.average(BLEU_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EVALUATION |     0/   59 batches |\n",
      "| EVALUATION |    19/   59 batches |\n",
      "| EVALUATION |    38/   59 batches |\n",
      "| EVALUATION |    57/   59 batches |\n",
      "| EVALUATION | BLEU: 0.001 |\n"
     ]
    }
   ],
   "source": [
    "evaluate(beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    tgt_vocab_size = len(itos) + max_seq_length\n",
    "    encoder_input = batch.src\n",
    "    decoder_input = batch.tgt[:-1]\n",
    "    targets = batch.tgt[1:]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(encoder_input, decoder_input)\n",
    "\n",
    "    loss = criterion(output.view(-1, tgt_vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    elapsed = time.time() - start_time\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=stoi['<pad>'])\n",
    "lr = config.train_learning_rate # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   400/1000000 steps | lr 0.0050 | ms/batch 124.59 | loss  4.41 | ppl    82.36\n",
      "|   800/1000000 steps | lr 0.0050 | ms/batch 133.65 | loss  3.68 | ppl    39.70\n",
      "|  1200/1000000 steps | lr 0.0050 | ms/batch 156.32 | loss  3.44 | ppl    31.21\n",
      "|  1600/1000000 steps | lr 0.0050 | ms/batch 125.02 | loss  3.32 | ppl    27.71\n",
      "|  2000/1000000 steps | lr 0.0050 | ms/batch 126.41 | loss  3.23 | ppl    25.36\n",
      "|  2400/1000000 steps | lr 0.0050 | ms/batch 120.44 | loss  3.16 | ppl    23.58\n",
      "|  2800/1000000 steps | lr 0.0050 | ms/batch 123.07 | loss  3.10 | ppl    22.09\n",
      "|  3200/1000000 steps | lr 0.0050 | ms/batch 124.91 | loss  3.06 | ppl    21.35\n",
      "|  3600/1000000 steps | lr 0.0050 | ms/batch 152.45 | loss  2.99 | ppl    19.98\n",
      "|  4000/1000000 steps | lr 0.0050 | ms/batch 133.89 | loss  2.94 | ppl    18.92\n",
      "|  4400/1000000 steps | lr 0.0050 | ms/batch 123.98 | loss  2.90 | ppl    18.20\n",
      "|  4800/1000000 steps | lr 0.0050 | ms/batch 120.26 | loss  2.86 | ppl    17.46\n",
      "|  5200/1000000 steps | lr 0.0050 | ms/batch 121.51 | loss  2.82 | ppl    16.84\n",
      "|  5600/1000000 steps | lr 0.0050 | ms/batch 122.28 | loss  2.77 | ppl    15.99\n",
      "|  6000/1000000 steps | lr 0.0050 | ms/batch 141.22 | loss  2.75 | ppl    15.65\n",
      "|  6400/1000000 steps | lr 0.0050 | ms/batch 144.09 | loss  2.71 | ppl    15.10\n",
      "|  6800/1000000 steps | lr 0.0050 | ms/batch 122.60 | loss  2.66 | ppl    14.34\n",
      "|  7200/1000000 steps | lr 0.0050 | ms/batch 121.38 | loss  2.62 | ppl    13.68\n",
      "|  7600/1000000 steps | lr 0.0050 | ms/batch 122.06 | loss  2.60 | ppl    13.51\n",
      "|  8000/1000000 steps | lr 0.0050 | ms/batch 125.77 | loss  2.57 | ppl    13.06\n",
      "|  8400/1000000 steps | lr 0.0049 | ms/batch 131.58 | loss  2.53 | ppl    12.56\n",
      "|  8800/1000000 steps | lr 0.0049 | ms/batch 151.25 | loss  2.49 | ppl    12.07\n",
      "|  9200/1000000 steps | lr 0.0049 | ms/batch 125.28 | loss  2.46 | ppl    11.66\n",
      "|  9600/1000000 steps | lr 0.0049 | ms/batch 123.10 | loss  2.44 | ppl    11.50\n",
      "| 10000/1000000 steps | lr 0.0049 | ms/batch 120.72 | loss  2.40 | ppl    11.01\n",
      "Evaluating model\n",
      "| EVALUATION |     0/   59 batches |\n",
      "| EVALUATION |    19/   59 batches |\n",
      "| EVALUATION |    38/   59 batches |\n",
      "| EVALUATION |    57/   59 batches |\n",
      "| EVALUATION | BLEU: 0.217 |\n",
      "| 10400/1000000 steps | lr 0.0049 | ms/batch 384.81 | loss  2.37 | ppl    10.73\n"
     ]
    }
   ],
   "source": [
    "def train(steps=10000, log_interval=200, learning_interval=4000, eval_interval=1000):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    step = 1\n",
    "    for batch in train_iterator:\n",
    "        loss = train_step(batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} steps | '\n",
    "                  'lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    step, steps, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        if step % eval_interval == 0:\n",
    "            print(\"Evaluating model\")\n",
    "            evaluate()\n",
    "            model.train()\n",
    "        \n",
    "        if step % learning_interval == 0:\n",
    "            scheduler.step()\n",
    "        \n",
    "        step += 1\n",
    "        if step >= steps:\n",
    "            print(\"Finished training\")\n",
    "\n",
    "\n",
    "\n",
    "            return\n",
    "\n",
    "train(steps=config.train_steps,eval_interval=config.eval_interval,log_interval=config.log_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
