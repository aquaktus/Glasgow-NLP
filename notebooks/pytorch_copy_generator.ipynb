{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy-Generator Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import operator\n",
    "import collections\n",
    "\n",
    "from base_transformer import TransformerModel, PositionalEncoding\n",
    "from copy_gen_transformer import Transformer, TransformerDecoderLayer, TransformerDecoder\n",
    "import beam_search\n",
    "from IPython.core.debugger import set_trace as tr\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def super_print(filename):\n",
    "    '''filename is the file where output will be written'''\n",
    "    def wrap(func):\n",
    "        '''func is the function you are \"overriding\", i.e. wrapping'''\n",
    "        def wrapped_func(*args,**kwargs):\n",
    "            '''*args and **kwargs are the arguments supplied \n",
    "            to the overridden function'''\n",
    "            #use with statement to open, write to, and close the file safely\n",
    "            with open(filename,'a') as outputfile:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                outputfile.write(\"[{}] \".format(dt_string))\n",
    "                outputfile.write(\" \".join(str(x) for x in args))\n",
    "                outputfile.write(\"\\n\")\n",
    "            #now original function executed with its arguments as normal\n",
    "            return func(*args,**kwargs)\n",
    "        return wrapped_func\n",
    "    return wrap\n",
    "\n",
    "print = super_print('logs-copy-gen.txt')(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.set_device(0) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['create', 'variable', 'student_names', 'with', 'string', \"'foo bar baz'\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"create variable student_names with string 'foo bar baz'\"\n",
    "\n",
    "def string_split(s):\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(_|\\W)', s))) # this will chunk all code properly by plits strings with quotes\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|_|\\W)', s))) # this keeps the strings intact\n",
    "    return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|\\W)', s)))\n",
    "\n",
    "print(string_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_array(src_fp, tgt_fp):\n",
    "    lines = []\n",
    "    with open(src_fp, \"r\") as src_file, open(tgt_fp, \"r\") as tgt_file:\n",
    "        for src, tgt in zip(src_file, tgt_file):\n",
    "            lines.append((src, tgt))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(data, max_seq_length=200, tokenizer=string_split):\n",
    "    return [(src, tgt) for src, tgt in data if len(string_split(src)) <= max_seq_length and len(string_split(tgt)) <= max_seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_dataset(samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples: [(src_string),(tgt_string)]\n",
    "        src/tgt_tokenizer: a func that takes a string and returns an array of strings\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    TEXT_FIELD = Field(sequential=True, use_vocab=False, init_token='<sos>',eos_token='<eos>')\n",
    "    \n",
    "    for sample in samples:\n",
    "        src_string, tgt_string = sample\n",
    "        examples.append(torchtext.data.Example.fromdict({\"src\":src_string, \"tgt\":tgt_string}, \n",
    "                                        fields={\"src\":(\"src\",TEXT_FIELD), \"tgt\":(\"tgt\",TEXT_FIELD)}))\n",
    "        \n",
    "    dataset = torchtext.data.Dataset(examples,fields={\"src\":src_field, \"tgt\":tgt_field})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src length: 557\n",
      "Max tgt length: 527\n",
      "Full dataset size: 18805\n",
      "Limited dataset size: 18797\n"
     ]
    }
   ],
   "source": [
    "data = corpus_to_array(\"datasets/all-fixed.desc\", \"datasets/all.code\")\n",
    "# data = corpus_to_array(\"datasets/all.desc\", \"datasets/all.code\")\n",
    "random.shuffle(data)\n",
    "print(\"Max src length:\", max([len(string_split(src)) for src, tgt in data]))\n",
    "print(\"Max tgt length:\", max([len(string_split(tgt)) for src, tgt in data]))\n",
    "\n",
    "print(\"Full dataset size:\", len(data))\n",
    "max_seq_length=200\n",
    "data = filter_corpus(data, max_seq_length=max_seq_length, tokenizer=string_split)\n",
    "print(\"Limited dataset size:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "          \"my favourite foods are banana and toast\",\n",
    "          \"my favourite foods are eggs and bacon and beans\",\n",
    "          \"my favourite food is chocolate\",\n",
    "          \"my favourite food is avocado\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "           \"would you like banana and toast ?\",\n",
    "           \"would you like eggs and bacon and beans ?\",\n",
    "           \"would you like chocolate ?\",\n",
    "           \"would you like avocado ?\"\n",
    "]\n",
    "# max_seq_length = 9\n",
    "# data = list(zip(inputs, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {\"<unk>\":0, \"<sos>\":1, \"<eos>\":2, \"<pad>\":3}\n",
    "max_vocab = 1000 - len(stoi)\n",
    "\n",
    "all_toks = []\n",
    "for (src, tgt) in data:\n",
    "    all_toks += string_split(src)\n",
    "    all_toks += string_split(tgt)\n",
    "\n",
    "most_freq = collections.Counter(all_toks).most_common(max_vocab)\n",
    "\n",
    "for tok, count in most_freq:\n",
    "    stoi[tok] = len(stoi)\n",
    "    \n",
    "itos = [k for k,v in sorted(stoi.items(), key=lambda kv: kv[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(string):\n",
    "    OOVs = []\n",
    "    IDs = []\n",
    "    words = string_split(string)\n",
    "    for word in words:\n",
    "        try:\n",
    "            id = stoi[word]\n",
    "            IDs.append(id)\n",
    "        except KeyError as e:\n",
    "            # word is OOV\n",
    "            IDs.append(len(stoi) + len(OOVs))\n",
    "            OOVs.append(word)\n",
    "    return IDs, OOVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(string, OOVs):\n",
    "    IDs = []\n",
    "    words = string_split(string)\n",
    "    for word in words:\n",
    "        try:\n",
    "            id = stoi[word]\n",
    "            IDs.append(id)\n",
    "        except KeyError as e:\n",
    "            # word is OOV\n",
    "            try:\n",
    "                IDs.append(len(stoi) + OOVs.index(word))\n",
    "            except ValueError as e:\n",
    "                IDs.append(stoi[\"<unk>\"])\n",
    "    return IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, OOVs):\n",
    "    extended_itos = itos.copy()\n",
    "    extended_itos += [OOV+\"(COPY)\" for OOV in OOVs]\n",
    "    return \" \".join([extended_itos[id] for id in ids if id<len(extended_itos)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FIELD = Field(sequential=True, use_vocab=False, unk_token=0, init_token=1,eos_token=2, pad_token=3)\n",
    "OOV_TEXT_FIELD = Field(sequential=True, use_vocab=False, pad_token=3)\n",
    "\n",
    "OOV_stoi = {}\n",
    "OOV_itos = {}\n",
    "OOV_starter_count = 30000\n",
    "OOV_count = OOV_starter_count\n",
    "\n",
    "examples = []\n",
    "\n",
    "for (src, tgt) in data:\n",
    "    src_ids, OOVs = encode_input(src)\n",
    "    tgt_ids = encode_output(tgt, OOVs)\n",
    "    OOV_ids = []\n",
    "    \n",
    "    for OOV in OOVs:\n",
    "        try:\n",
    "            idx = OOV_stoi[OOV]\n",
    "            OOV_ids.append(idx)\n",
    "        except KeyError as e:\n",
    "            OOV_count += 1\n",
    "            OOV_stoi[OOV] = OOV_count\n",
    "            OOV_itos[OOV_count] = OOV\n",
    "            OOV_ids.append(OOV_count)\n",
    "            \n",
    "    examples.append(torchtext.data.Example.fromdict({\"src\":src_ids, \"tgt\":tgt_ids, \"OOVs\":OOV_ids}, \n",
    "                                                    fields={\"src\":(\"src\",TEXT_FIELD), \"tgt\":(\"tgt\",TEXT_FIELD), \"OOVs\":(\"OOVs\", OOV_TEXT_FIELD)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchtext.data.Dataset(examples,fields={\"src\":TEXT_FIELD, \"tgt\":TEXT_FIELD, \"OOVs\":OOV_TEXT_FIELD})\n",
    "train_dataset, val_dataset = dataset.split([0.9,0.1])\n",
    "# train_dataset = val_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([53, 32])\n",
      "torch.Size([32, 32])\n",
      "SOURCE: <sos> if TypeError or ValueError exceptions are caught , <eos>\n",
      "\n",
      "TARGET: <sos> except ( ValueError , TypeError ) : <eos>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_iterator = BucketIterator(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    repeat=True,\n",
    "    shuffle=True,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "# The iterator generates batches with padded length for sequences with similar sizes, a batch is [seq_length, batch_size]\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    idx = 3\n",
    "#     print([SRC_TEXT.vocab.itos[id] for id in batch.src.cpu().numpy()[:,idx]])\n",
    "    OOVs = [OOV_itos[OOV] for OOV in batch.OOVs.cpu()[:,idx].tolist() if OOV != 3] # 3 is the <pad> token\n",
    "    src_ids = batch.src.cpu()[:,idx].tolist()\n",
    "    src_ids = src_ids[:src_ids.index(2)+1]\n",
    "    tgt_ids = batch.tgt.cpu()[:,idx].tolist()\n",
    "    tgt_ids = tgt_ids[:tgt_ids.index(2)+1]\n",
    "    \n",
    "    print(batch.src.shape)\n",
    "    print(batch.tgt.shape)\n",
    "    \n",
    "    print(\"SOURCE:\",decode(src_ids, OOVs))\n",
    "    print()\n",
    "    print(\"TARGET:\",decode(tgt_ids, OOVs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foo bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_size=512, dropout=0.5):\n",
    "        super(CopyModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, dropout)\n",
    "        self.src_encoder = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.tgt_encoder = nn.Embedding(tgt_vocab_size, embedding_size)\n",
    "        \n",
    "        self.transformer = Transformer(d_model=embedding_size, nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024)\n",
    "        self.decoder = nn.Linear(embedding_size, tgt_vocab_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.p_generator = nn.Linear(embedding_size,1)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.tgt_mask = None\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.src_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "#         noise_e = 0.05 if self.training else 0.0 # this is code to add noise to the decoding process during training\n",
    "        noise_e = 0.0 if self.training else 0.0\n",
    "        noise_mask = (torch.rand(sz,sz) > noise_e).float()\n",
    "\n",
    "        mask = (torch.triu(torch.ones(sz,sz))).transpose(0, 1)\n",
    "        mask = torch.mul(mask, noise_mask)\n",
    "        v = (torch.sum(mask, dim=-1) == 0).float()\n",
    "\n",
    "        fix_mask = torch.zeros(sz,sz)\n",
    "        fix_mask[:,0] = 1.0\n",
    "        v = v.repeat(sz, 1).transpose(0,1)\n",
    "        fix_mask = torch.mul(fix_mask,v)\n",
    "\n",
    "        mask += fix_mask\n",
    "        \n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        self.tgt_mask = self._generate_square_subsequent_mask(len(tgt)).to(self.device)\n",
    "        \n",
    "\n",
    "        src_emb = self.src_encoder(src) * math.sqrt(self.embedding_size)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        \n",
    "        tgt_emb = self.tgt_encoder(tgt) * math.sqrt(self.embedding_size)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        \n",
    "        output, atts = self.transformer(src_emb, tgt_emb, tgt_mask=self.tgt_mask)\n",
    "        \n",
    "        \n",
    "        src_scat = src.transpose(0,1)\n",
    "        src_scat = src_scat.unsqueeze(0)\n",
    "        src_scat = torch.repeat_interleave(src_scat, tgt.shape[0], dim=0)\n",
    "#         print(\"src_scat.shqape\", src_scat.shape)\n",
    "        \n",
    "        p_gens = self.p_generator(output).sigmoid()\n",
    "        atts = atts.transpose(0,1)\n",
    "#         print(\"att.shqape\", atts.shape)\n",
    "        atts = atts * (1 - p_gens)\n",
    "                \n",
    "        output = self.decoder(output)\n",
    "#         output[:,:,12:] = -np.inf\n",
    "        output = output.softmax(-1)\n",
    "        output = output * p_gens\n",
    "        \n",
    "        output = output.scatter_add_(2,src_scat,atts)\n",
    "        output = output\n",
    "        \n",
    "        return output.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 1200])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos) + max_seq_length\n",
    "\n",
    "model = CopyModel(vocab_size,vocab_size).to(device) \n",
    "src = torch.randint(0, vocab_size, (3,2)).to(device)\n",
    "tgt = torch.randint(0, vocab_size, (5,2)).to(device)\n",
    "\n",
    "out = model(src, tgt)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_bleu(refrence, prediction):\n",
    "    \"\"\"\n",
    "    Implementation from ReCode\n",
    "    and moses multi belu script sets BLEU to 0.0 if len(toks) < 4\n",
    "    \"\"\"\n",
    "    ngram_weights = [0.25] * min(4, len(refrence))\n",
    "    return sentence_bleu([refrence], prediction, weights=ngram_weights, \n",
    "                          smoothing_function=SmoothingFunction().method3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iterator = BucketIterator(val_dataset,\n",
    "    batch_size = 128,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "def batch_filter_ids(batch_list):\n",
    "    return [[id for id in l if id not in [0,1,2,3]] for l in batch_list]\n",
    "\n",
    "def evaluate(beam_size=1, log=False):\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    with torch.no_grad(), open(\"out.txt\", \"w\") as out_fp:\n",
    "        BLEU_scores = []\n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "            batch_size = batch.src.shape[1]\n",
    "            \n",
    "            encoder_inputs = batch.src\n",
    "            predictions = beam_search.beam_search_decode(model,\n",
    "                              batch_encoder_ids=encoder_inputs,\n",
    "                              SOS_token=stoi[\"<sos>\"],\n",
    "                              EOS_token=stoi[\"<eos>\"],\n",
    "                              PAD_token=stoi[\"<pad>\"],\n",
    "                              beam_size=beam_size,\n",
    "                              max_length=20,\n",
    "                              num_out=1)\n",
    "            \n",
    "            sources = encoder_inputs.transpose(0,1).cpu().tolist()\n",
    "            sources = batch_filter_ids(sources)\n",
    "            \n",
    "            predictions = [t[0].view(-1).cpu().tolist() for t in predictions]\n",
    "            predictions = batch_filter_ids(predictions)\n",
    "            \n",
    "            targets = batch.tgt.transpose(0,1).cpu().tolist()\n",
    "            targets = batch_filter_ids(targets)\n",
    "            \n",
    "#             print(batch.tgt)\n",
    "            \n",
    "            OOVss = [[OOV_itos[OOV] for OOV in batch.OOVs.cpu()[:,idx].tolist() if OOV != 3] for idx in range(batch_size)]\n",
    "            \n",
    "            if i % int(len(valid_iterator)/3) == 0:\n",
    "                print(\"| EVALUATION | {:5d}/{:5d} batches |\".format(i, len(valid_iterator)))\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                BLEU = nltk_bleu(targets[j], predictions[j])\n",
    "                BLEU_scores.append(BLEU)\n",
    "                \n",
    "                out_fp.write(\"SRC  :\" + decode(sources[j], OOVss[j]) + \"\\n\")\n",
    "                out_fp.write(\"TGT  :\" + decode(targets[j], OOVss[j]) + \"\\n\")\n",
    "                out_fp.write(\"PRED :\" + decode(predictions[j], OOVss[j]) + \"\\n\")\n",
    "                out_fp.write(\"BLEU :\" + str(BLEU) + \"\\n\")\n",
    "                out_fp.write(\"\\n\")\n",
    "                \n",
    "                if log:\n",
    "                    print(\"SRC  :\" + decode(sources[j], OOVss[j]))\n",
    "                    print(\"TGT  :\" + decode(targets[j], OOVss[j]))\n",
    "                    print(\"PRED :\" + decode(predictions[j], OOVss[j]))\n",
    "                    print(\"BLEU :\" + str(BLEU))\n",
    "                    print()\n",
    "        out_fp.write(\"\\n\\n| EVALUATION | BLEU: {:5.2f} |\\n\".format(np.average(BLEU_scores)))\n",
    "        print(\"| EVALUATION | BLEU: {:5.2f} |\".format(np.average(BLEU_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EVALUATION |     0/   15 batches |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ee948df27117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-1f7caadfc377>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(beam_size, log)\u001b[0m\n\u001b[1;32m     22\u001b[0m                               \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                               \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                               num_out=1)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/beam_search.py\u001b[0m in \u001b[0;36mbeam_search_decode\u001b[0;34m(model, batch_encoder_ids, beam_size, num_out, max_length, SOS_token, EOS_token, PAD_token)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrough_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mpadded_decoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrough_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mencoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_input\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworking_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    tgt_vocab_size = len(itos) + max_seq_length\n",
    "    encoder_input = batch.src\n",
    "    decoder_input = batch.tgt[:-1]\n",
    "    targets = batch.tgt[1:]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(encoder_input, decoder_input)\n",
    "\n",
    "    loss = criterion(output.view(-1, tgt_vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    elapsed = time.time() - start_time\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=stoi['<pad>'])\n",
    "lr = 0.005 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   200/1000000 steps | lr 0.0050 | ms/batch 35.10 | loss  4.54 | ppl    93.38\n",
      "|   400/1000000 steps | lr 0.0050 | ms/batch 34.41 | loss  3.89 | ppl    49.03\n",
      "|   600/1000000 steps | lr 0.0050 | ms/batch 34.92 | loss  3.61 | ppl    37.08\n",
      "|   800/1000000 steps | lr 0.0050 | ms/batch 35.08 | loss  3.41 | ppl    30.28\n",
      "|  1000/1000000 steps | lr 0.0050 | ms/batch 35.36 | loss  3.31 | ppl    27.52\n",
      "|  1200/1000000 steps | lr 0.0050 | ms/batch 34.28 | loss  3.25 | ppl    25.77\n",
      "|  1400/1000000 steps | lr 0.0050 | ms/batch 35.24 | loss  3.17 | ppl    23.90\n",
      "|  1600/1000000 steps | lr 0.0050 | ms/batch 35.15 | loss  3.15 | ppl    23.43\n",
      "|  1800/1000000 steps | lr 0.0050 | ms/batch 36.19 | loss  3.12 | ppl    22.59\n",
      "|  2000/1000000 steps | lr 0.0050 | ms/batch 34.31 | loss  3.06 | ppl    21.30\n",
      "|  2200/1000000 steps | lr 0.0050 | ms/batch 34.93 | loss  3.03 | ppl    20.75\n",
      "|  2400/1000000 steps | lr 0.0050 | ms/batch 34.97 | loss  2.99 | ppl    19.87\n",
      "|  2600/1000000 steps | lr 0.0050 | ms/batch 35.67 | loss  2.98 | ppl    19.76\n",
      "|  2800/1000000 steps | lr 0.0050 | ms/batch 34.77 | loss  2.93 | ppl    18.64\n",
      "|  3000/1000000 steps | lr 0.0050 | ms/batch 35.52 | loss  2.92 | ppl    18.63\n",
      "|  3200/1000000 steps | lr 0.0050 | ms/batch 34.84 | loss  2.87 | ppl    17.71\n",
      "|  3400/1000000 steps | lr 0.0050 | ms/batch 35.03 | loss  2.87 | ppl    17.58\n",
      "|  3600/1000000 steps | lr 0.0050 | ms/batch 35.55 | loss  2.83 | ppl    16.87\n",
      "|  3800/1000000 steps | lr 0.0050 | ms/batch 34.99 | loss  2.80 | ppl    16.39\n",
      "|  4000/1000000 steps | lr 0.0050 | ms/batch 35.07 | loss  2.78 | ppl    16.04\n",
      "|  4200/1000000 steps | lr 0.0050 | ms/batch 35.29 | loss  2.77 | ppl    15.96\n",
      "|  4400/1000000 steps | lr 0.0050 | ms/batch 36.35 | loss  2.75 | ppl    15.58\n",
      "|  4600/1000000 steps | lr 0.0050 | ms/batch 34.53 | loss  2.72 | ppl    15.21\n",
      "|  4800/1000000 steps | lr 0.0050 | ms/batch 34.61 | loss  2.70 | ppl    14.87\n",
      "|  5000/1000000 steps | lr 0.0050 | ms/batch 36.05 | loss  2.70 | ppl    14.87\n",
      "|  5200/1000000 steps | lr 0.0050 | ms/batch 35.04 | loss  2.65 | ppl    14.10\n",
      "|  5400/1000000 steps | lr 0.0050 | ms/batch 35.26 | loss  2.64 | ppl    13.98\n",
      "|  5600/1000000 steps | lr 0.0050 | ms/batch 34.41 | loss  2.61 | ppl    13.57\n",
      "|  5800/1000000 steps | lr 0.0050 | ms/batch 35.85 | loss  2.62 | ppl    13.79\n",
      "|  6000/1000000 steps | lr 0.0050 | ms/batch 35.46 | loss  2.60 | ppl    13.43\n",
      "|  6200/1000000 steps | lr 0.0050 | ms/batch 35.83 | loss  2.57 | ppl    13.03\n",
      "|  6400/1000000 steps | lr 0.0050 | ms/batch 34.73 | loss  2.51 | ppl    12.31\n",
      "|  6600/1000000 steps | lr 0.0050 | ms/batch 35.64 | loss  2.54 | ppl    12.74\n",
      "|  6800/1000000 steps | lr 0.0050 | ms/batch 35.46 | loss  2.48 | ppl    11.98\n",
      "|  7000/1000000 steps | lr 0.0050 | ms/batch 35.60 | loss  2.48 | ppl    11.98\n",
      "|  7200/1000000 steps | lr 0.0050 | ms/batch 35.52 | loss  2.46 | ppl    11.68\n",
      "|  7400/1000000 steps | lr 0.0050 | ms/batch 35.25 | loss  2.47 | ppl    11.77\n",
      "|  7600/1000000 steps | lr 0.0050 | ms/batch 35.20 | loss  2.44 | ppl    11.44\n",
      "|  7800/1000000 steps | lr 0.0050 | ms/batch 34.65 | loss  2.40 | ppl    11.03\n",
      "|  8000/1000000 steps | lr 0.0050 | ms/batch 35.71 | loss  2.39 | ppl    10.94\n",
      "|  8200/1000000 steps | lr 0.0049 | ms/batch 35.36 | loss  2.38 | ppl    10.76\n",
      "|  8400/1000000 steps | lr 0.0049 | ms/batch 35.65 | loss  2.38 | ppl    10.85\n",
      "|  8600/1000000 steps | lr 0.0049 | ms/batch 34.80 | loss  2.34 | ppl    10.38\n",
      "|  8800/1000000 steps | lr 0.0049 | ms/batch 34.71 | loss  2.31 | ppl    10.12\n",
      "|  9000/1000000 steps | lr 0.0049 | ms/batch 36.32 | loss  2.31 | ppl    10.10\n",
      "|  9200/1000000 steps | lr 0.0049 | ms/batch 35.64 | loss  2.27 | ppl     9.71\n",
      "|  9400/1000000 steps | lr 0.0049 | ms/batch 35.56 | loss  2.30 | ppl     9.97\n",
      "|  9600/1000000 steps | lr 0.0049 | ms/batch 35.13 | loss  2.26 | ppl     9.62\n",
      "|  9800/1000000 steps | lr 0.0049 | ms/batch 35.71 | loss  2.26 | ppl     9.59\n",
      "| 10000/1000000 steps | lr 0.0049 | ms/batch 35.23 | loss  2.23 | ppl     9.29\n",
      "Evaluating model\n",
      "| EVALUATION |     0/   15 batches |\n",
      "| EVALUATION |     5/   15 batches |\n",
      "| EVALUATION |    10/   15 batches |\n",
      "| EVALUATION | BLEU:  0.25 |\n",
      "| 10200/1000000 steps | lr 0.0049 | ms/batch 77.72 | loss  2.21 | ppl     9.14\n",
      "| 10400/1000000 steps | lr 0.0049 | ms/batch 35.51 | loss  2.20 | ppl     9.02\n",
      "| 10600/1000000 steps | lr 0.0049 | ms/batch 34.95 | loss  2.20 | ppl     8.99\n",
      "| 10800/1000000 steps | lr 0.0049 | ms/batch 35.32 | loss  2.18 | ppl     8.87\n",
      "| 11000/1000000 steps | lr 0.0049 | ms/batch 35.57 | loss  2.16 | ppl     8.64\n",
      "| 11200/1000000 steps | lr 0.0049 | ms/batch 36.03 | loss  2.16 | ppl     8.63\n",
      "| 11400/1000000 steps | lr 0.0049 | ms/batch 34.48 | loss  2.15 | ppl     8.54\n",
      "| 11600/1000000 steps | lr 0.0049 | ms/batch 35.08 | loss  2.12 | ppl     8.29\n",
      "| 11800/1000000 steps | lr 0.0049 | ms/batch 36.06 | loss  2.13 | ppl     8.38\n",
      "| 12000/1000000 steps | lr 0.0049 | ms/batch 35.22 | loss  2.09 | ppl     8.06\n",
      "| 12200/1000000 steps | lr 0.0049 | ms/batch 34.84 | loss  2.09 | ppl     8.10\n",
      "| 12400/1000000 steps | lr 0.0049 | ms/batch 36.33 | loss  2.09 | ppl     8.10\n",
      "| 12600/1000000 steps | lr 0.0049 | ms/batch 34.77 | loss  2.07 | ppl     7.92\n",
      "| 12800/1000000 steps | lr 0.0049 | ms/batch 34.85 | loss  2.03 | ppl     7.58\n",
      "| 13000/1000000 steps | lr 0.0049 | ms/batch 35.22 | loss  2.07 | ppl     7.91\n",
      "| 13200/1000000 steps | lr 0.0049 | ms/batch 35.05 | loss  2.03 | ppl     7.58\n",
      "| 13400/1000000 steps | lr 0.0049 | ms/batch 34.72 | loss  2.03 | ppl     7.65\n",
      "| 13600/1000000 steps | lr 0.0049 | ms/batch 35.78 | loss  2.02 | ppl     7.56\n",
      "| 13800/1000000 steps | lr 0.0049 | ms/batch 35.47 | loss  1.98 | ppl     7.25\n",
      "| 14000/1000000 steps | lr 0.0049 | ms/batch 36.90 | loss  2.00 | ppl     7.37\n",
      "| 14200/1000000 steps | lr 0.0049 | ms/batch 37.10 | loss  1.99 | ppl     7.34\n",
      "| 14400/1000000 steps | lr 0.0049 | ms/batch 36.45 | loss  1.96 | ppl     7.11\n",
      "| 14600/1000000 steps | lr 0.0049 | ms/batch 36.24 | loss  1.97 | ppl     7.14\n",
      "| 14800/1000000 steps | lr 0.0049 | ms/batch 35.53 | loss  1.96 | ppl     7.08\n",
      "| 15000/1000000 steps | lr 0.0049 | ms/batch 35.97 | loss  1.96 | ppl     7.09\n",
      "| 15200/1000000 steps | lr 0.0049 | ms/batch 35.63 | loss  1.93 | ppl     6.90\n",
      "| 15400/1000000 steps | lr 0.0049 | ms/batch 35.47 | loss  1.93 | ppl     6.86\n",
      "| 15600/1000000 steps | lr 0.0049 | ms/batch 34.91 | loss  1.92 | ppl     6.85\n",
      "| 15800/1000000 steps | lr 0.0049 | ms/batch 36.42 | loss  1.91 | ppl     6.74\n",
      "| 16000/1000000 steps | lr 0.0049 | ms/batch 34.99 | loss  1.91 | ppl     6.77\n",
      "| 16200/1000000 steps | lr 0.0048 | ms/batch 35.28 | loss  1.89 | ppl     6.63\n",
      "| 16400/1000000 steps | lr 0.0048 | ms/batch 36.18 | loss  1.91 | ppl     6.72\n",
      "| 16600/1000000 steps | lr 0.0048 | ms/batch 36.48 | loss  1.88 | ppl     6.56\n",
      "| 16800/1000000 steps | lr 0.0048 | ms/batch 35.45 | loss  1.88 | ppl     6.57\n",
      "| 17000/1000000 steps | lr 0.0048 | ms/batch 35.31 | loss  1.88 | ppl     6.52\n",
      "| 17200/1000000 steps | lr 0.0048 | ms/batch 35.70 | loss  1.87 | ppl     6.50\n",
      "| 17400/1000000 steps | lr 0.0048 | ms/batch 35.43 | loss  1.85 | ppl     6.33\n",
      "| 17600/1000000 steps | lr 0.0048 | ms/batch 35.30 | loss  1.86 | ppl     6.41\n",
      "| 17800/1000000 steps | lr 0.0048 | ms/batch 35.62 | loss  1.83 | ppl     6.25\n",
      "| 18000/1000000 steps | lr 0.0048 | ms/batch 36.73 | loss  1.86 | ppl     6.40\n",
      "| 18200/1000000 steps | lr 0.0048 | ms/batch 35.20 | loss  1.83 | ppl     6.22\n",
      "| 18400/1000000 steps | lr 0.0048 | ms/batch 35.63 | loss  1.83 | ppl     6.24\n",
      "| 18600/1000000 steps | lr 0.0048 | ms/batch 35.33 | loss  1.82 | ppl     6.18\n",
      "| 18800/1000000 steps | lr 0.0048 | ms/batch 35.56 | loss  1.81 | ppl     6.13\n",
      "| 19000/1000000 steps | lr 0.0048 | ms/batch 35.58 | loss  1.80 | ppl     6.07\n",
      "| 19200/1000000 steps | lr 0.0048 | ms/batch 35.68 | loss  1.81 | ppl     6.09\n",
      "| 19400/1000000 steps | lr 0.0048 | ms/batch 36.16 | loss  1.79 | ppl     5.99\n",
      "| 19600/1000000 steps | lr 0.0048 | ms/batch 34.59 | loss  1.80 | ppl     6.02\n",
      "| 19800/1000000 steps | lr 0.0048 | ms/batch 35.26 | loss  1.79 | ppl     5.98\n",
      "| 20000/1000000 steps | lr 0.0048 | ms/batch 35.38 | loss  1.76 | ppl     5.81\n",
      "Evaluating model\n",
      "| EVALUATION |     0/   15 batches |\n",
      "| EVALUATION |     5/   15 batches |\n",
      "| EVALUATION |    10/   15 batches |\n",
      "| EVALUATION | BLEU:  0.33 |\n",
      "| 20200/1000000 steps | lr 0.0048 | ms/batch 81.52 | loss  1.76 | ppl     5.84\n",
      "| 20400/1000000 steps | lr 0.0048 | ms/batch 35.70 | loss  1.78 | ppl     5.91\n",
      "| 20600/1000000 steps | lr 0.0048 | ms/batch 35.79 | loss  1.75 | ppl     5.77\n",
      "| 20800/1000000 steps | lr 0.0048 | ms/batch 35.88 | loss  1.75 | ppl     5.76\n",
      "| 21000/1000000 steps | lr 0.0048 | ms/batch 35.59 | loss  1.76 | ppl     5.83\n",
      "| 21200/1000000 steps | lr 0.0048 | ms/batch 35.30 | loss  1.73 | ppl     5.67\n",
      "| 21400/1000000 steps | lr 0.0048 | ms/batch 35.95 | loss  1.74 | ppl     5.72\n",
      "| 21600/1000000 steps | lr 0.0048 | ms/batch 34.68 | loss  1.73 | ppl     5.63\n",
      "| 21800/1000000 steps | lr 0.0048 | ms/batch 35.63 | loss  1.73 | ppl     5.63\n",
      "| 22000/1000000 steps | lr 0.0048 | ms/batch 35.10 | loss  1.74 | ppl     5.68\n",
      "| 22200/1000000 steps | lr 0.0048 | ms/batch 35.01 | loss  1.71 | ppl     5.50\n",
      "| 22400/1000000 steps | lr 0.0048 | ms/batch 35.70 | loss  1.71 | ppl     5.55\n",
      "| 22600/1000000 steps | lr 0.0048 | ms/batch 35.28 | loss  1.72 | ppl     5.58\n",
      "| 22800/1000000 steps | lr 0.0048 | ms/batch 35.63 | loss  1.70 | ppl     5.48\n",
      "| 23000/1000000 steps | lr 0.0048 | ms/batch 35.56 | loss  1.71 | ppl     5.53\n",
      "| 23200/1000000 steps | lr 0.0048 | ms/batch 35.04 | loss  1.69 | ppl     5.40\n",
      "| 23400/1000000 steps | lr 0.0048 | ms/batch 35.71 | loss  1.67 | ppl     5.33\n",
      "| 23600/1000000 steps | lr 0.0048 | ms/batch 35.43 | loss  1.68 | ppl     5.38\n",
      "| 23800/1000000 steps | lr 0.0048 | ms/batch 35.57 | loss  1.70 | ppl     5.45\n",
      "| 24000/1000000 steps | lr 0.0048 | ms/batch 36.62 | loss  1.70 | ppl     5.50\n",
      "| 24200/1000000 steps | lr 0.0047 | ms/batch 36.25 | loss  1.69 | ppl     5.40\n",
      "| 24400/1000000 steps | lr 0.0047 | ms/batch 35.04 | loss  1.64 | ppl     5.14\n",
      "| 24600/1000000 steps | lr 0.0047 | ms/batch 35.35 | loss  1.66 | ppl     5.24\n",
      "| 24800/1000000 steps | lr 0.0047 | ms/batch 35.72 | loss  1.66 | ppl     5.26\n",
      "| 25000/1000000 steps | lr 0.0047 | ms/batch 35.69 | loss  1.67 | ppl     5.32\n",
      "| 25200/1000000 steps | lr 0.0047 | ms/batch 35.45 | loss  1.65 | ppl     5.20\n",
      "| 25400/1000000 steps | lr 0.0047 | ms/batch 35.89 | loss  1.65 | ppl     5.20\n"
     ]
    }
   ],
   "source": [
    "def train(steps=10000, log_interval=200, learning_interval=4000, eval_interval=1000):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    step = 1\n",
    "    for batch in train_iterator:\n",
    "        loss = train_step(batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} steps | '\n",
    "                  'lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    step, steps, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        if step % eval_interval == 0:\n",
    "            print(\"Evaluating model\")\n",
    "            evaluate()\n",
    "            model.train()\n",
    "        \n",
    "        if step % learning_interval == 0:\n",
    "            scheduler.step()\n",
    "        \n",
    "        step += 1\n",
    "        if step >= steps:\n",
    "            print(\"Finished training\")\n",
    "\n",
    "\n",
    "\n",
    "            return\n",
    "\n",
    "train(steps=1000000,eval_interval=10000,log_interval=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
