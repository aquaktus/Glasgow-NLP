{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from models_and_trainers.BERT_style_encoder import BERTStyleEncoder, BertTokenizer\n",
    "from models_and_trainers.copy_gen_transformer import CopyGeneratorTransformer\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "from utils.useful_utils import string_split_v3, string_split_v2, string_split_v1, nltk_bleu\n",
    "from utils.dataset_loaders import SRC_TGT_pairs\n",
    "import utils.beam_search as beam_search\n",
    "import tqdm.notebook as tqdm \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.set_device(0) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_fp = \"datasets/django_folds/django.fold1-10.train.src\"\n",
    "tgt_train_fp = \"datasets/django_folds/django.fold1-10.train.tgt\"\n",
    "src_test_fp = \"datasets/django_folds/django.fold1-10.test.src\"\n",
    "tgt_test_fp = \"datasets/django_folds/django.fold1-10.test.tgt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SRC_TGT_pairs(src_train_fp, tgt_train_fp).samples\n",
    "test_data = SRC_TGT_pairs(src_test_fp, tgt_test_fp).samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raise', 'an', 'exception', 'of', 'class', 'value', '##er', '##ror', ',', 'with', 'string']\n",
      "['[unused0]']\n",
      "We have added 0 tokens\n",
      "[101, 5333, 2019, 6453, 1997, 2465, 3643, 2121, 29165, 1010, 2007, 5164, 102]\n",
      "['[UNK]', '[unused0]', '[PAD]', '[CLS]', '[MASK]', '[SEP]', '[unused1]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inp = tokenizer.encode(\"raise an exception of class ValueError, with string\")[1:-1]\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inp))\n",
    "print(tokenizer.convert_ids_to_tokens([1]))\n",
    "special_tokens_dict = {'bos_token': '[unused0]','eos_token': '[unused1]'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "print(tokenizer.build_inputs_with_special_tokens(inp))\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29ca05694404a6fbda10d757fe0599b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16797), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32da89bf1e64394b14254b84a481cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1874), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT_FIELD = Field(sequential=True, use_vocab=False, unk_token=100, pad_token=0)\n",
    "# OOV_TEXT_FIELD = Field(sequential=True, use_vocab=False, pad_token=3)\n",
    "\n",
    "def data2dataset(data):\n",
    "    TEXT_FIELD = Field(sequential=True, use_vocab=False, unk_token=0, pad_token=3)    \n",
    "    examples = []\n",
    "    \n",
    "    for (src, tgt) in tqdm.tqdm(data):\n",
    "        \n",
    "        src_ids = tokenizer.encode(src)[1:-1]\n",
    "        tgt_ids = tokenizer.encode(tgt)[1:-1]\n",
    "        \n",
    "        decoder_input = [tokenizer.bos_token_id] + tgt_ids\n",
    "        ground_truth_code = tgt_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "\n",
    "\n",
    "        example = torchtext.data.Example.fromdict({\"encoder_input\":src_ids, \n",
    "                                                   \"ground_truth_code\":ground_truth_code, \n",
    "                                                   \"decoder_input\":decoder_input}, \n",
    "                                                fields={\"encoder_input\":(\"encoder_input\",TEXT_FIELD), \n",
    "                                                        \"ground_truth_code\":(\"ground_truth_code\",TEXT_FIELD),\n",
    "                                                        \"decoder_input\":(\"decoder_input\",TEXT_FIELD)})\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "train_examples = data2dataset(train_data)\n",
    "test_examples = data2dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchtext.data.Dataset(train_examples,fields={\"encoder_input\":TEXT_FIELD, \n",
    "                                                  \"ground_truth_code\":TEXT_FIELD, \n",
    "                                                  \"decoder_input\":TEXT_FIELD})\n",
    "\n",
    "val_dataset = torchtext.data.Dataset(test_examples,fields={\"encoder_input\":TEXT_FIELD, \n",
    "                                                  \"ground_truth_code\":TEXT_FIELD, \n",
    "                                                  \"decoder_input\":TEXT_FIELD})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_iterator = BucketIterator(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    repeat=True,\n",
    "    shuffle=True,\n",
    "    sort=True,\n",
    "    sort_key = lambda x: len(x.encoder_input)+len(x.decoder_input),\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input    : ['try', ',', '[PAD]']\n",
      "decoder_input    : ['[unused0]', 'try', ':']\n",
      "ground_truth_code: ['try', ':', '[unused1]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    idx = 0\n",
    "#     print([SRC_TEXT.vocab.itos[id] for id in batch.src.cpu().numpy()[:,idx]])\n",
    "    encoder_input = batch.encoder_input.cpu()[:,idx].tolist()\n",
    "    decoder_input = batch.decoder_input.cpu()[:,idx].tolist()\n",
    "    ground_truth_code = batch.ground_truth_code.cpu()[:,idx].tolist()\n",
    "    \n",
    "    print(\"encoder_input    :\",tokenizer.convert_ids_to_tokens(encoder_input))\n",
    "    print(\"decoder_input    :\",tokenizer.convert_ids_to_tokens(decoder_input))\n",
    "    print(\"ground_truth_code:\",tokenizer.convert_ids_to_tokens(ground_truth_code))\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Integrity test returns mean -9.449563975749697e-08 and norm 0.0017513044876977801\n"
     ]
    }
   ],
   "source": [
    "model = CopyGeneratorTransformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 30522])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "src = torch.randint(0, vocab_size, (3,2)).to(device)\n",
    "tgt = torch.randint(0, vocab_size, (5,2)).to(device)\n",
    "\n",
    "outputs = model(src, tgt)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmid(arr, r_id):\n",
    "    return [x for x in arr if x != r_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_bleu(refrence, prediction):\n",
    "    \"\"\"\n",
    "    Implementation from ReCode\n",
    "    and moses multi belu script sets BLEU to 0.0 if len(toks) < 4\n",
    "    \"\"\"\n",
    "    ngram_weights = [0.25] * min(4, len(refrence))\n",
    "    return sentence_bleu([refrence], prediction, weights=ngram_weights, \n",
    "                          smoothing_function=SmoothingFunction().method3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iterator = BucketIterator(val_dataset,\n",
    "    batch_size = 16,\n",
    "    sort=True,\n",
    "    sort_key = lambda x: len(x.encoder_input)+len(x.decoder_input),\n",
    "    device = device)\n",
    "\n",
    "def batch_filter_ids(batch_list):\n",
    "    SOS_token=tokenizer.bos_token_id\n",
    "    EOS_token=tokenizer.eos_token_id\n",
    "    PAD_token=tokenizer.pad_token_id\n",
    "    return [[id for id in l if id not in [SOS_token,EOS_token,PAD_token]] for l in batch_list]\n",
    "\n",
    "def evaluate(beam_size=1, log=False):\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    with torch.no_grad(), open(\"BERT_code_generator.out\", \"w\", encoding=\"utf-8\") as out_fp:\n",
    "        BLEU_scores = []\n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "            batch_size = batch.encoder_input.shape[1]\n",
    "            \n",
    "            encoder_inputs = batch.encoder_input\n",
    "            predictions = beam_search.beam_search_decode(model,\n",
    "                              batch_encoder_ids=encoder_inputs,\n",
    "                              SOS_token=tokenizer.bos_token_id,\n",
    "                              EOS_token=tokenizer.eos_token_id,\n",
    "                              PAD_token=tokenizer.pad_token_id,\n",
    "                              beam_size=beam_size,\n",
    "                              max_length=30,\n",
    "                              num_out=1)\n",
    "            \n",
    "            sources = encoder_inputs.transpose(0,1).cpu().tolist()\n",
    "            sources = batch_filter_ids(sources)\n",
    "            \n",
    "            predictions = [t[0].view(-1).cpu().tolist() for t in predictions]\n",
    "            predictions = batch_filter_ids(predictions)\n",
    "            \n",
    "            targets = batch.ground_truth_code.transpose(0,1).cpu().tolist()\n",
    "            targets = batch_filter_ids(targets)\n",
    "            \n",
    "            if i % int(len(valid_iterator)/3) == 0:\n",
    "                print(\"| EVALUATION | {:5d}/{:5d} batches |\".format(i, len(valid_iterator)))\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                BLEU = nltk_bleu(targets[j], predictions[j])\n",
    "                BLEU_scores.append(BLEU)\n",
    "                \n",
    "                out_fp.write(\"SRC  :\" + \" \".join(tokenizer.convert_ids_to_tokens(sources[j])) + \"\\n\")\n",
    "                out_fp.write(\"TGT  :\" + \" \".join(tokenizer.convert_ids_to_tokens(targets[j])) + \"\\n\")\n",
    "                out_fp.write(\"PRED :\" + \" \".join(tokenizer.convert_ids_to_tokens(predictions[j])) + \"\\n\")\n",
    "                out_fp.write(\"BLEU :\" + str(BLEU) + \"\\n\")\n",
    "                out_fp.write(\"\\n\")\n",
    "        out_fp.write(\"\\n\\n| EVALUATION | BLEU: {:5.2f} |\\n\".format(np.average(BLEU_scores)))\n",
    "        print(\"| EVALUATION | BLEU: {:5.3f} |\".format(np.average(BLEU_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EVALUATION |     0/  118 batches |\n",
      "| EVALUATION |    39/  118 batches |\n",
      "| EVALUATION |    78/  118 batches |\n",
      "| EVALUATION |   117/  118 batches |\n",
      "| EVALUATION | BLEU: 0.015 |\n"
     ]
    }
   ],
   "source": [
    "evaluate(beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    tgt_vocab_size = tokenizer.vocab_size\n",
    "    encoder_input = batch.encoder_input\n",
    "    decoder_input = batch.decoder_input\n",
    "    targets = batch.ground_truth_code\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(encoder_input, decoder_input)\n",
    "\n",
    "    loss = criterion(output.view(-1, tgt_vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    elapsed = time.time() - start_time\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "lr = 0.005 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e35465e4714df481cee5f2c1c7955d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1050), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "| EVALUATION |     0/  118 batches |\n",
      "| EVALUATION |    39/  118 batches |\n",
      "| EVALUATION |    78/  118 batches |\n",
      "| EVALUATION |   117/  118 batches |\n",
      "| EVALUATION | BLEU: 0.161 |\n",
      "Evaluating model\n",
      "| EVALUATION |     0/  118 batches |\n",
      "| EVALUATION |    39/  118 batches |\n",
      "| EVALUATION |    78/  118 batches |\n",
      "| EVALUATION |   117/  118 batches |\n",
      "| EVALUATION | BLEU: 0.141 |\n",
      "Evaluating model\n",
      "| EVALUATION |     0/  118 batches |\n",
      "| EVALUATION |    39/  118 batches |\n",
      "| EVALUATION |    78/  118 batches |\n",
      "| EVALUATION |   117/  118 batches |\n",
      "| EVALUATION | BLEU: 0.184 |\n",
      "Evaluating model\n",
      "| EVALUATION |     0/  118 batches |\n",
      "| EVALUATION |    39/  118 batches |\n",
      "| EVALUATION |    78/  118 batches |\n",
      "| EVALUATION |   117/  118 batches |\n",
      "| EVALUATION | BLEU: 0.162 |\n",
      "Evaluating model\n",
      "| EVALUATION |     0/  118 batches |\n",
      "| EVALUATION |    39/  118 batches |\n",
      "| EVALUATION |    78/  118 batches |\n",
      "| EVALUATION |   117/  118 batches |\n",
      "| EVALUATION | BLEU: 0.203 |\n",
      "Evaluating model\n",
      "| EVALUATION |     0/  118 batches |\n",
      "| EVALUATION |    39/  118 batches |\n",
      "| EVALUATION |    78/  118 batches |\n",
      "| EVALUATION |   117/  118 batches |\n",
      "| EVALUATION | BLEU: 0.187 |\n",
      "Evaluating model\n",
      "| EVALUATION |     0/  118 batches |\n",
      "| EVALUATION |    39/  118 batches |\n",
      "| EVALUATION |    78/  118 batches |\n",
      "| EVALUATION |   117/  118 batches |\n",
      "| EVALUATION | BLEU: 0.217 |\n"
     ]
    }
   ],
   "source": [
    "def train(steps=10000, log_interval=200, learning_interval=4000, eval_interval=1000):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    step = 1\n",
    "    pbar = tqdm.tqdm(train_iterator)\n",
    "    for batch in pbar:\n",
    "        loss = train_step(batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            pbar.set_description(f\"{elapsed * 1000 / log_interval:5.2f} | {cur_loss:5.2f}, {math.exp(cur_loss):5.2f}\")\n",
    "#             print('| {:5d}/{:5d} steps | '\n",
    "#                   'lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "#                   'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "#                     step, steps, scheduler.get_lr()[0],\n",
    "#                     elapsed * 1000 / log_interval,\n",
    "#                     cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        if step % eval_interval == 0:\n",
    "            print(\"Evaluating model\")\n",
    "            evaluate()\n",
    "            model.train()\n",
    "        \n",
    "        if step % learning_interval == 0:\n",
    "            scheduler.step()\n",
    "        \n",
    "        step += 1\n",
    "        if step >= steps:\n",
    "            print(\"Finished training\")\n",
    "\n",
    "\n",
    "\n",
    "            return\n",
    "\n",
    "train(steps=500000, log_interval=50, eval_interval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
