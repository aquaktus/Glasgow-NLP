{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can Language Models produce correct structured output?\n",
    "The goaal of this notebook is to explorer the nature of the mistakes a Language Modell can make when producing structured output like code. The idea is that it is prone to making syntactic errors that could be avoided when scaling up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.RawDataLoaders import CodeSearchNet_RawDataLoader, CoNaLa_RawDataLoader\n",
    "from src.models_and_transforms.text_transforms import Numericalise_Transform, Rename_Transform\n",
    "from src.models_and_transforms.complex_transforms import *\n",
    "from src.models_and_transforms.BART_models import BART_Simple\n",
    "from src.Experiments import Sequence_BLEU_Experiment\n",
    "from src.models_and_transforms.SOTA_transforms import CoNaLa_SOTA_Transform, Django_SOTA_Transform\n",
    "from src.pipe_datasets import BART_Pipe_Dataset\n",
    "from src.useful_utils import chunks, download_from_url, Validate_and_Save_Callback\n",
    "\n",
    "from pytorch_lightning import Trainer, Callback, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateLogger\n",
    "from pytorch_lightning.callbacks.base import Callback\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from tokenizers import processors, Tokenizer\n",
    "from transformers import BartConfig\n",
    "import torch\n",
    "import ray\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "import random\n",
    "import itertools\n",
    "import cloudpickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c736662f3a9543ee95fb13307ccc4c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loading: sonl/train', max=14.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8b7c25daa24817b4321a946d2ac793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loading: sonl/valid', max=1.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9aba74ca43d4c8f8e3b380e3e5b66d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='loading: jsonl/test', max=1.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "code_search_net_RawDataLoader = CodeSearchNet_RawDataLoader(\"./datasets/code_search_net\", language=\"python\", max_chars=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_RawDataLoader = CoNaLa_RawDataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SOTA model on CoNaLa and Django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load parser from [src/external_repos/external-knowledge-codegen/best_pretrained_models/finetune.mined.retapi.distsmpl.dr0.3.lr0.001.lr_de0.5.lr_da15.beam15.seed0.mined_100000.intent_count100k_topk1_temp5.bin]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading reranker...\n"
     ]
    }
   ],
   "source": [
    "conala_test_samples = conala_RawDataLoader.get_samples('test')\n",
    "conala_transform = CoNaLa_SOTA_Transform(cuda=False, fields={'input_field':'description', 'output_field':'pred_code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baeb8fb653214bafa275e7bed99ee4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Tranx:', max=10.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.system('str_0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(str_0.decode('str_0', 'ignore'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i for i in var_0 if i == i]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\" \"\"\".join(str(x) for x in str(var_0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unicodedata.normalize('NFKD', string).encode('utf8')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(var_0.values())\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(var_0.values())\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subprocess.call('str_0', **subprocess.check_output('str_0'), shell=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\"\"\".join([str_0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.concat([var_0] * 5, ignore_index=True)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "# takes around 5 mins for inference\n",
    "samples = conala_transform(conala_test_samples[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f023e66ebf704947bab088e0119632e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Tranx:', max=1.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_0 = [(i * j) for i, j in zip(var_0, var_0)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing rerank features for hypotheses...\n",
      "initializing features...\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'foo = [(x * y) for x in foo]'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples = [{'description':'multiply each element of list `foo` by 2'}]\n",
    "test_samples = conala_transform(test_samples)\n",
    "test_samples[0]['pred_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'send a signal `signal.SIGUSR1` to the current process',\n",
       "  'code': 'os.kill(os.getpid(), signal.SIGUSR1)',\n",
       "  'pred_code': \"os.system('<unk> <unk> <unk>')\",\n",
       "  'BLEU': 0.0},\n",
       " {'description': \"decode a hex string '4a4b4c' to UTF-8.\",\n",
       "  'code': \"bytes.fromhex('4a4b4c').decode('utf-8')\",\n",
       "  'pred_code': \"print(4a4b4c.decode('4a4b4c'))\",\n",
       "  'BLEU': 0.0},\n",
       " {'description': 'check if all elements in list `myList` are identical',\n",
       "  'code': 'all(x == myList[0] for x in myList)',\n",
       "  'pred_code': '[i for i in myList if i == 2]',\n",
       "  'BLEU': 0.0},\n",
       " {'description': 'format number of spaces between strings `Python`, `:` and `Very Good` to be `20`',\n",
       "  'code': \"print('%*s : %*s' % (20, 'Python', 20, 'Very Good'))\",\n",
       "  'pred_code': '\"\"\" \"\"\".join(str(x) for x in str(Python))',\n",
       "  'BLEU': 0.0},\n",
       " {'description': 'How to convert a string from CP-1251 to UTF-8?',\n",
       "  'code': \"d.decode('cp1251').encode('utf8')\",\n",
       "  'pred_code': \"unicodedata.normalize('NFKD', string).encode('utf8')\",\n",
       "  'BLEU': 0.27901593935858265},\n",
       " {'description': 'get rid of None values in dictionary `kwargs`',\n",
       "  'code': 'res = {k: v for k, v in list(kwargs.items()) if v is not None}',\n",
       "  'pred_code': 'list(kwargs.values())',\n",
       "  'BLEU': 1.0},\n",
       " {'description': 'get rid of None values in dictionary `kwargs`',\n",
       "  'code': 'res = dict((k, v) for k, v in kwargs.items() if v is not None)',\n",
       "  'pred_code': 'list(kwargs.values())',\n",
       "  'BLEU': 0.0},\n",
       " {'description': 'capture final output of a chain of system commands `ps -ef | grep something | wc -l`',\n",
       "  'code': \"subprocess.check_output('ps -ef | grep something | wc -l', shell=True)\",\n",
       "  'pred_code': 'sys.stdout.flush()',\n",
       "  'BLEU': 0.0},\n",
       " {'description': \"concatenate a list of strings `['a', 'b', 'c']`\",\n",
       "  'code': '\"\"\"\"\"\".join([\\'a\\', \\'b\\', \\'c\\'])',\n",
       "  'pred_code': '\"\"\"\"\"\".join([[\\'a\\', \\'b\\', \\'c\\']])',\n",
       "  'BLEU': 0.0},\n",
       " {'description': 'find intersection data between series `s1` and series `s2`',\n",
       "  'code': 'pd.Series(list(set(s1).intersection(set(s2))))',\n",
       "  'pred_code': 'pd.concat([s1] * 5, ignore_index=True)',\n",
       "  'BLEU': 0.0}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simi_expr = Sequence_BLEU_Experiment(fields={'target_seq':'code', 'predicted_seq':'pred_code'})\n",
    "simi_expr.__transform__(samples)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsmoothed_official_BLEU: 0.22539098849201342\n",
      "nltk_BLEU: 0.2529552430370321\n"
     ]
    }
   ],
   "source": [
    "samples = simi_expr(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(samples, open(\"CoNaLa_TranX_predictions.json\", 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training BART on CoNaLa\n",
    "### Using the BPE tokens trained from code search net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training BART codeBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 200000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'sort a list of lists `the_list` by the second item in each list',\n",
       " 'code': \"sorted(the_list, key=lambda x: int(x.split('_')[1]))\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conala_RawDataLoader = CoNaLa_RawDataLoader()\n",
    "conala_samples = conala_RawDataLoader.get_samples('mined_GPT3', max_char_len=200)[:200000]\n",
    "print('Dataset size:',len(conala_samples))\n",
    "conala_samples[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericaliser. Ex: 'This is a test' -> [0, 3863, 343, 276, 1772, 2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e626549b5b46a0bbaf741cc647cac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0756c93073314900b7dd54329ec880d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='pre-sort processing', max=200000.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_numericalise_transform = Numericalise_Transform(numericaliser='Code32k', fields=[('description','input_ids'),('code','target_ids')])\n",
    "\n",
    "conala_dataset = BART_Pipe_Dataset(conala_samples, \n",
    "                                         sort_key_fn=lambda sample_obj: len(sample_obj['target_ids']),\n",
    "#                                          valid_sample_fn=lambda sample_obj: len(sample_obj['target_ids'])<80 and len(sample_obj['input_ids'])<80,\n",
    "                                         shuffle=True,\n",
    "                                         batch_bucket_size=1, \n",
    "                                         slow_pipe=[], real_time_pipe=[code_numericalise_transform])\n",
    "conala_dataloader = conala_dataset.to_dataloader(16)\n",
    "lengths = [len(s['target_ids']) for s in conala_dataset]\n",
    "plt.scatter(range(len(lengths)), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig(vocab_size=32000)\n",
    "model = BART_Simple(from_pretrained=False, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import EvalResult\n",
    "import wandb\n",
    "def CoNaLa_valid_fn(model):\n",
    "    conala_RawDataLoader = CoNaLa_RawDataLoader()\n",
    "    BART_cond_gen_transform = BART_Conditional_Generator_Transform(model, \n",
    "                                                                    numericaliser='Code32k', \n",
    "                                                                    denumericaliser='Code32k',\n",
    "                                                                    device='cuda',\n",
    "                                                                    debug=False,\n",
    "                                                                    chunk_size=32,\n",
    "                                                                    max_length=40)\n",
    "    valid_samples = conala_RawDataLoader.get_samples('valid')\n",
    "    test_samples = conala_RawDataLoader.get_samples('test')\n",
    "    train_samples = conala_RawDataLoader.get_samples('mined_GPT3')[:100]\n",
    "    return_dict = {}\n",
    "    \n",
    "    for samples, name in [(valid_samples, 'valid'), (test_samples, 'test'), (train_samples, 'train')]:\n",
    "        samples = Rename_Transform(fields=[('description', 'input_text')])(samples)\n",
    "        samples = BART_cond_gen_transform(samples)\n",
    "        samples = Sequence_BLEU_Experiment(fields = {'predicted_seq':'pred_text', 'target_seq':'code'}, debug=False)(samples)\n",
    "        \n",
    "        nltk_BLEU = np.average([s[\"nltk_BLEU\"] for s in samples])\n",
    "        table = wandb.Table(columns=[\"Desc\", \"True\", \"Pred\"])\n",
    "        for sample_obj in samples:\n",
    "            table.add_data(sample_obj['input_text'], sample_obj['code'], sample_obj['pred_text'])\n",
    "        \n",
    "        return_dict[f'{name}_nltk_BLEU'] = nltk_BLEU\n",
    "        return_dict[f'{name}_samples'] = table\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_cb = Validate_and_Save_Callback(filepath='saved_models/BART_CoNaLa/',\n",
    "                                       prefix='BART_codeBPE_mined200k_step_LR',\n",
    "                                       validate_fn=CoNaLa_valid_fn,\n",
    "                                       monitor='valid_nltk_BLEU',\n",
    "                                       interval=0.1)\n",
    "wandb_logger = WandbLogger(name='BART_codeBPE_mined200k_step_LR',project='pytorchlightning')\n",
    "lr_logger_cb = LearningRateLogger(logging_interval='step')\n",
    "\n",
    "trainer = Trainer(gpus=[0], gradient_clip_val=0.5, amp_level='O1', max_epochs=200, callbacks=[saving_cb, lr_logger_cb], logger=wandb_logger, \n",
    "                  accumulate_grad_batches=1)\n",
    "trainer.fit(model, conala_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BART_Simple. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BartForConditionalGeneration. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BartModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BartEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LearnedPositionalEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BartDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save({'model':model, 'train_dataset':train_dataset}, 'BART_6k_chars.pickle', pickle_module=cloudpickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericaliser. Ex: 'This is a test' -> [0, 3863, 343, 276, 1772, 2]\n",
      "Denumericaliser. Ex: [0,1,2,3,4,5,6,7,8,9] -> !\"#$%\n"
     ]
    }
   ],
   "source": [
    "config = BartConfig(vocab_size=32000)\n",
    "BART_cond_gen_transform = BART_Conditional_Generator_Transform(\"saved_models/BART_CoNaLa/BART_codeBPE_mined20k_set_lowLR_{'nltk_BLEU': 0.2149831961314364, 'valid_samples': <wandb.data_types.Table object at 0x7f669336c1d0>}_step_64875.ckpt\", \n",
    "                                                                numericaliser='Code32k', \n",
    "                                                                denumericaliser='Code32k',\n",
    "                                                                device='cuda',\n",
    "                                                                from_pretrained=False,\n",
    "                                                                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1542ad2c4084fcc8e2bd4ab7408c024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='BART is thinking:', max=1.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input_text': 'split a string `foo` by comma',\n",
       "  'input_ids': [0, 961, 276, 711, 613, 3797, 68, 626, 6155, 2],\n",
       "  'pred_ids': [0, 3797, 277, 6773, 18, 961, 401, 3797, 4022, 438],\n",
       "  'pred_text': \"foo = foo.split('foo bar')\"}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conala_RawDataLoader = CoNaLa_RawDataLoader()\n",
    "# test_samples = conala_RawDataLoader.get_samples('train')[:10]\n",
    "# test_samples = conala_samples[:10]\n",
    "test_samples = [{'description':'split a string `foo` by comma'}]\n",
    "test_samples = Rename_Transform(fields=[('description', 'input_text')])(test_samples)\n",
    "\n",
    "test_samples = BART_cond_gen_transform(test_samples)\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Training BART on Code Search Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fecbd04b88a4c5a9ce120416293ce15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Filtering max chars: 600', max=412178.0, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b487ec0d4ece4a6d809b9278359fbdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='deleting bload fields', max=184916.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrain_samples = code_search_net_RawDataLoader.get_samples(\"train\", fields=['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_train_samples = conala_RawDataLoader.get_samples('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'swap values in a tuple/list inside a list `mylist`',\n",
       " 'code': 'map(lambda t: (t[1], t[0]), mylist)'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conala_samples[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7d4fb64cc746bba243b6b763bbd4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conala_train_dataset = BART_Pipe_Dataset(conala_train_samples, slow_pipe=[], real_time_pipe=[\n",
    "    BART_Numericalise_Transform(fields=[('description','input_ids'),('code','target_ids')])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_train_dataloader = conala_train_dataset.to_dataloader(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BART_Simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type                         | Params\n",
      "------------------------------------------------------\n",
      "0 | BART | BartForConditionalGeneration | 406 M \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b8e7e7c0554c4ca7a101a32005165d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(gpus=[1], gradient_clip_val=0.5, amp_level='O1', max_epochs=100, \n",
    "                  callbacks=[saving_cb])\n",
    "trainer.fit(model, conala_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('saved_models/BART_CoNaLa/BART_CodeBPE_CoNaLa_100_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model on cuda\n"
     ]
    }
   ],
   "source": [
    "BART_cond_gen_transform = BART_Conditional_Generator_Transform(\"saved_models/BART_CoNaLa/BART_CoNaLa_100_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7029d2a8d71047caaa2a7ef656167538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='BART is thinking:', max=1.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'l = list(itertools.chain.from_iterable(iterable([0, 1], repeat=False]))([0] for i in range(0, len(l)) for a item in l])\\nelse:\\n    l   pass'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples = [{'input_text':\"l is an empty list\"}]\n",
    "BART_cond_gen_transform(test_samples)[0]['pred_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46b5cbf84e54e4d831df389925ba37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BART_Span_Prediction_dataset(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = train_dataset.to_dataloader(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  9232, 18134, 41087,  1215,   560,  1215, 48408,  1640, 41087,\n",
      "          3256, 50118,  1437,  1437,  1437, 49434, 50118,  1437,  1437,  1437,\n",
      "         32988,    10,   385, 34748,   128, 41087,   108,  7626,     7,    10,\n",
      "         10798, 49683,    11,    36,  8766,     6,   235,     6,  2576,     6,\n",
      "           314,    43,   645, 50140,  1437,  1437,  1437,  4832, 46669, 20705,\n",
      "            35,    10,   385, 34748,   128, 41087,   108,  7626, 50118,  1437,\n",
      "          1437,  1437,  4832, 30921,    35,    10, 10798, 49683,  8985,     9,\n",
      "             5, 20705,    11,    36,  8766,     6,   235,     6,  2576,     6,\n",
      "           314,    43,   645, 50118,  1437,  1437,  1437, 49434, 50118,  1437,\n",
      "          1437,  1437,   671, 20705,     4,  8766, 49196, 20705,     4,  4070,\n",
      "         49196, 20705,     4, 23724, 49196, 20705,     4,  6960, 43048,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    0,  9232, 18134,  4328,   757,  1215, 48408,  1215,   560,  1215,\n",
      "           428, 12363,  1640, 48408,     6,  2274,  1215, 43882,  3256, 50118,\n",
      "          1437,  1437,  1437, 49434, 50118,  1437,  1437,  1437,  5293,   686,\n",
      "            10, 49683,    11,    36,  8766,     6,   235,     6,  2576,     6,\n",
      "           314,    43,   645,    16,   624,     5, 22772,     9,     5,  2274,\n",
      "             4, 50140,  1437,  1437,  1437,  4832, 46669,   740,  7485,    35,\n",
      "          1437, 10798, 49683,  8985,     9,     5, 20705,    11,    36,  8766,\n",
      "             6,   235,     6,  2576,     6,   314,    43,   645, 50118,  1437,\n",
      "          1437,  1437,  4832, 46669,  2274,  1215, 43882,    35,   295, 35187,\n",
      "          3989,     9,     5,  2274,  8932, 50118,  1437,  1437,  1437,  4832,\n",
      "         30921,    35,    10, 20856, 10798, 49683,  8985,     9,     5, 20705,\n",
      "            11,    36,  8766,     6,   235,     6,  2576,     6,   314,    43,\n",
      "           645, 50118,  1437,  1437,  1437, 49434, 50118,  1437,  1437,  1437,\n",
      "           671, 19220,  1640, 48408, 10975,   288,  7479,   321,   238,  5251,\n",
      "          1640, 48408, 10975,   134,  7479,  2274,  1215, 43882, 10975,   134,\n",
      "         48677,  5251,  1640, 48408, 10975,   176,  7479,  2274,  1215, 43882,\n",
      "         10975,   288, 48677, 19220,  1640, 48408, 10975,   246,  7479,   321,\n",
      "            43]])\n",
      "tensor([[ 9232, 18134, 41087,  1215,   560,  1215, 48408,  1640, 41087,  3256,\n",
      "         50118,  1437,  1437,  1437, 49434, 50118,  1437,  1437,  1437, 32988,\n",
      "            10,   385, 34748,   128, 41087,   108,  7626,     7,    10, 10798,\n",
      "         49683,    11,    36,  8766,     6,   235,     6,  2576,     6,   314,\n",
      "            43,   645, 50140,  1437,  1437,  1437,  4832, 46669, 20705,    35,\n",
      "            10,   385, 34748,   128, 41087,   108,  7626, 50118,  1437,  1437,\n",
      "          1437,  4832, 30921,    35,    10, 10798, 49683,  8985,     9,     5,\n",
      "         20705,    11,    36,  8766,     6,   235,     6,  2576,     6,   314,\n",
      "            43,   645, 50118,  1437,  1437,  1437, 49434, 50118,  1437,  1437,\n",
      "          1437,   671, 20705,     4,  8766, 49196, 20705,     4,  4070, 49196,\n",
      "         20705,     4, 23724, 49196, 20705,     4,  6960, 43048,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [ 9232, 18134,  4328,   757,  1215, 48408,  1215,   560,  1215,   428,\n",
      "         12363,  1640, 48408,     6,  2274,  1215, 43882,  3256, 50118,  1437,\n",
      "          1437,  1437, 49434, 50118,  1437,  1437,  1437,  5293,   686,    10,\n",
      "         49683,    11,    36,  8766,     6,   235,     6,  2576,     6,   314,\n",
      "            43,   645,    16,   624,     5, 22772,     9,     5,  2274,     4,\n",
      "         50140,  1437,  1437,  1437,  4832, 46669,   740,  7485,    35,  1437,\n",
      "         10798, 49683,  8985,     9,     5, 20705,    11,    36,  8766,     6,\n",
      "           235,     6,  2576,     6,   314,    43,   645, 50118,  1437,  1437,\n",
      "          1437,  4832, 46669,  2274,  1215, 43882,    35,   295, 35187,  3989,\n",
      "             9,     5,  2274,  8932, 50118,  1437,  1437,  1437,  4832, 30921,\n",
      "            35,    10, 20856, 10798, 49683,  8985,     9,     5, 20705,    11,\n",
      "            36,  8766,     6,   235,     6,  2576,     6,   314,    43,   645,\n",
      "         50118,  1437,  1437,  1437, 49434, 50118,  1437,  1437,  1437,   671,\n",
      "         19220,  1640, 48408, 10975,   288,  7479,   321,   238,  5251,  1640,\n",
      "         48408, 10975,   134,  7479,  2274,  1215, 43882, 10975,   134, 48677,\n",
      "          5251,  1640, 48408, 10975,   176,  7479,  2274,  1215, 43882, 10975,\n",
      "           288, 48677, 19220,  1640, 48408, 10975,   246,  7479,   321,    43,\n",
      "             2]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([ 9232, 18134, 41087,  1215,   560,  1215, 48408,  1640, 41087,  3256,\n",
      "        50118,  1437,  1437,  1437, 49434, 50118,  1437,  1437,  1437, 32988,\n",
      "           10,   385, 34748,   128, 41087,   108,  7626,     7,    10, 10798,\n",
      "        49683,    11,    36,  8766,     6,   235,     6,  2576,     6,   314,\n",
      "           43,   645, 50140,  1437,  1437,  1437,  4832, 46669, 20705,    35,\n",
      "           10,   385, 34748,   128, 41087,   108,  7626, 50118,  1437,  1437,\n",
      "         1437,  4832, 30921,    35,    10, 10798, 49683,  8985,     9,     5,\n",
      "        20705,    11,    36,  8766,     6,   235,     6,  2576,     6,   314,\n",
      "           43,   645, 50118,  1437,  1437,  1437, 49434, 50118,  1437,  1437,\n",
      "         1437,   671, 20705,     4,  8766, 49196, 20705,     4,  4070, 49196,\n",
      "        20705,     4, 23724, 49196, 20705,     4,  6960, 43048,     2,  9232,\n",
      "        18134,  4328,   757,  1215, 48408,  1215,   560,  1215,   428, 12363,\n",
      "         1640, 48408,     6,  2274,  1215, 43882,  3256, 50118,  1437,  1437,\n",
      "         1437, 49434, 50118,  1437,  1437,  1437,  5293,   686,    10, 49683,\n",
      "           11,    36,  8766,     6,   235,     6,  2576,     6,   314,    43,\n",
      "          645,    16,   624,     5, 22772,     9,     5,  2274,     4, 50140,\n",
      "         1437,  1437,  1437,  4832, 46669,   740,  7485,    35,  1437, 10798,\n",
      "        49683,  8985,     9,     5, 20705,    11,    36,  8766,     6,   235,\n",
      "            6,  2576,     6,   314,    43,   645, 50118,  1437,  1437,  1437,\n",
      "         4832, 46669,  2274,  1215, 43882,    35,   295, 35187,  3989,     9,\n",
      "            5,  2274,  8932, 50118,  1437,  1437,  1437,  4832, 30921,    35,\n",
      "           10, 20856, 10798, 49683,  8985,     9,     5, 20705,    11,    36,\n",
      "         8766,     6,   235,     6,  2576,     6,   314,    43,   645, 50118,\n",
      "         1437,  1437,  1437, 49434, 50118,  1437,  1437,  1437,   671, 19220,\n",
      "         1640, 48408, 10975,   288,  7479,   321,   238,  5251,  1640, 48408,\n",
      "        10975,   134,  7479,  2274,  1215, 43882, 10975,   134, 48677,  5251,\n",
      "         1640, 48408, 10975,   176,  7479,  2274,  1215, 43882, 10975,   288,\n",
      "        48677, 19220,  1640, 48408, 10975,   246,  7479,   321,    43,     2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['decoder_input_ids'])\n",
    "    print(batch['decoder_target_ids'])\n",
    "    print(batch['target_attention_mask'])\n",
    "    print(torch.masked_select(batch['decoder_target_ids'], batch['target_attention_mask'].type(torch.bool)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3031936809b7430d82272d54053f3466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(train_dataset[i]['input_ids']) for i in tqdm(range(10000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type                         | Params\n",
      "------------------------------------------------------\n",
      "0 | BART | BartForConditionalGeneration | 406 M \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e347e177964a4f1281d32aea76fecbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(gpus=[1], gradient_clip_val=0.5, amp_level='O1', limit_train_batches=0.001, max_epochs=1, checkpoint_callback=checkpoint_callback, \n",
    "                  callbacks=[saving_cb])\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model on cuda\n",
      "Numericaliser. Ex: 'This is a test' -> [0, 713, 16, 10, 1296, 2]\n",
      "Denumericaliser. Ex: [0,1,2,3,4,5,6,7,8,9] -> <s><pad></s><unk>. the, to and of\n"
     ]
    }
   ],
   "source": [
    "BART_cond_gen_transform = BART_Conditional_Generator_Transform(\"./saved_models/BART_CodeSearchNet/BART_600_step_21960.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4113ebf62a9e47b39ff6e3ec8ae89612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='BART is thinking:', max=1.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "</s><s>def contains_number(x):\n",
      "    \"\"\"A function to check if x is an integer\"\"\"\n",
      "  # # #  #   return x\n",
      "  %   >>> x.is_int(x)\n",
      "\n",
      "  # return x.size()\n"
     ]
    }
   ],
   "source": [
    "# Complete the string bellow\n",
    "test_samples = [{'input_text':\n",
    "                 '''def contains_number(x):\n",
    "    \"\"\"A function to check if x is an integer\"\"\"\n",
    "'''}]\n",
    "returned_samples = BART_cond_gen_transform(test_samples)\n",
    "print(returned_samples[0]['pred_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38b7ae26a804b54b23ffb9b4ceac4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "numer_samples = BART_Numericalise_Transform(fields=[('code', 'input_ids')], use_ray=False)(samples[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s['input_ids']) for s in numer_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foo'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-08 15:44:44,635\tERROR worker.py:666 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6765d68cbae49ce89564c48a9860d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Augmenting code with Ray cores', max=184916.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_samples = Code_Sample_Augmentation_Transform()(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lengths = [len(sample['code']) for sample in augmented_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6261e6dcf2504cc18d9b3094fe9f21dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "large_samples = list([{'code':'foo '*40}]*4000)\n",
    "large_samples += list([{'code':'foo '*40}]*20)\n",
    "dataset = BART_Span_Prediction_dataset(large_samples)\n",
    "max_batch_dataloader = dataset.to_dataloader(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(s['input_ids']) for s in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch_size = 1\n",
    "model.train_dataloader = lambda: dataset.to_dataloader(model.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type                         | Params\n",
      "------------------------------------------------------\n",
      "0 | BART | BartForConditionalGeneration | 406 M \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7672afd229b40cc9b5f3efbe0fccf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(gpus=1, gradient_clip_val=0.5, amp_level='O1', max_epochs=100)# , auto_scale_batch_size='binsearch')\n",
    "trainer.fit(model, max_batch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
