{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_and_trainers.copy_gen_transformer import CopyGeneratorTransformer\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "from utils.useful_utils import string_split_v3, string_split_v2, string_split_v1, nltk_bleu\n",
    "from utils.dataset_loaders import SRC_TGT_pairs\n",
    "import utils.beam_search as beam_search\n",
    "import tqdm.notebook as tqdm \n",
    "\n",
    "from models_and_trainers.trainers import Model_Trainer\n",
    "\n",
    "from utils.vocab_classes import Shared_Vocab\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.set_device(0) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train_fp = \"datasets/django_folds/django.fold1-10.train.src\"\n",
    "tgt_train_fp = \"datasets/django_folds/django.fold1-10.train.tgt\"\n",
    "src_test_fp = \"datasets/django_folds/django.fold1-10.test.src\"\n",
    "tgt_test_fp = \"datasets/django_folds/django.fold1-10.test.tgt\"\n",
    "\n",
    "max_seq_len = 50\n",
    "\n",
    "train_samples = SRC_TGT_pairs(src_train_fp, tgt_train_fp, max_seq_len=max_seq_len).samples\n",
    "test_samples = SRC_TGT_pairs(src_test_fp, tgt_test_fp, max_seq_len=max_seq_len).samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Shared_Vocab(train_samples, vocab_size, string_split_v3, use_OOVs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2dataset(data):\n",
    "    TEXT_FIELD = Field(sequential=True, use_vocab=False, unk_token=0, init_token=1,eos_token=2, pad_token=3)\n",
    "    OOV_TEXT_FIELD = Field(sequential=True, use_vocab=False, pad_token=3)\n",
    "\n",
    "    OOV_stoi = {}\n",
    "    OOV_itos = {}\n",
    "    OOV_starter_count = 30000\n",
    "    OOV_count = OOV_starter_count\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    for (src, tgt) in data:\n",
    "        src_ids, OOV_ids = vocab.encode_input(src)\n",
    "#         print(src, src_ids)\n",
    "        tgt_ids = vocab.encode_output(tgt, OOV_ids)\n",
    "#         OOV_ids = []\n",
    "\n",
    "#         for OOV in OOVs:\n",
    "#             try:\n",
    "#                 idx = OOV_stoi[OOV]\n",
    "#                 OOV_ids.append(idx)\n",
    "#             except KeyError as e:\n",
    "#                 OOV_count += 1\n",
    "#                 OOV_stoi[OOV] = OOV_count\n",
    "#                 OOV_itos[OOV_count] = OOV\n",
    "#                 OOV_ids.append(OOV_count)\n",
    "\n",
    "        examples.append(torchtext.data.Example.fromdict({\"src\":src_ids, \"tgt\":tgt_ids, \"OOVs\":OOV_ids}, \n",
    "                                                        fields={\"src\":(\"src\",TEXT_FIELD), \"tgt\":(\"tgt\",TEXT_FIELD), \"OOVs\":(\"OOVs\", OOV_TEXT_FIELD)}))\n",
    "    dataset = torchtext.data.Dataset(examples,fields={\"src\":TEXT_FIELD, \"tgt\":TEXT_FIELD, \"OOVs\":OOV_TEXT_FIELD})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data2dataset(train_samples)\n",
    "test_dataset = data2dataset(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_iterator = BucketIterator(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    repeat=True,\n",
    "    shuffle=True,\n",
    "    sort=True,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "test_iterator = BucketIterator(\n",
    "    test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    sort=True,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self . _request(COPY) = datastructures(COPY) . MergeDict(COPY) ( self . POST(COPY) , self . GET(COPY) )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_dataset))\n",
    "vocab.decode_input(sample.tgt, sample.OOVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input    : <sos> try , <eos> <pad>\n",
      "decoder_input    : <sos> try : <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    for idx in range(batch_size):\n",
    "        encoder_input = batch.src.cpu()[:,idx].tolist()\n",
    "        decoder_input = batch.tgt.cpu()[:,idx].tolist()\n",
    "        OOVs = batch.OOVs.cpu()[:,idx].tolist()\n",
    "\n",
    "        print(\"encoder_input    :\",vocab.decode_input(encoder_input, OOVs))\n",
    "        print(\"decoder_input    :\",vocab.decode_output(decoder_input, OOVs))\n",
    "        print()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vocab_size = vocab.size+max_seq_len\n",
    "model = CopyGeneratorTransformer(vocab_size=output_vocab_size, embed_dim=512, att_heads=8, layers=4, dim_feedforward=1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])\n",
    "params = model.parameters()\n",
    "\n",
    "def train_step(batch):\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    encoder_input = batch.src\n",
    "    decoder_input = batch.tgt[:-1]\n",
    "    targets = batch.tgt[1:]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(encoder_input, decoder_input)\n",
    "\n",
    "    loss = criterion(output.view(-1, output_vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(params, 0.5)\n",
    "    optimizer.step()\n",
    "    elapsed = time.time() - start_time\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_filter_ids(batch_list):\n",
    "    SOS_token=vocab.stoi[\"<sos>\"]\n",
    "    EOS_token=vocab.stoi[\"<eos>\"]\n",
    "    PAD_token=vocab.stoi[\"<pad>\"]\n",
    "    return [[id for id in l if id not in [SOS_token,EOS_token,PAD_token]] for l in batch_list]\n",
    "\n",
    "def evaluate(iterator):\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    with torch.no_grad(), open(\"BERT_code_generator.out\", \"w\", encoding=\"utf-8\") as out_fp:\n",
    "        BLEU_scores = []\n",
    "        pbar = tqdm.tqdm(enumerate(iterator), total=len(iterator))\n",
    "        for i, batch in pbar:\n",
    "            batch_size = batch.src.shape[1]\n",
    "            \n",
    "            encoder_inputs = batch.src\n",
    "            predictions = beam_search.beam_search_decode(model,\n",
    "                              batch_encoder_ids=encoder_inputs,\n",
    "                              SOS_token=vocab.stoi[\"<sos>\"],\n",
    "                              EOS_token=vocab.stoi[\"<eos>\"],\n",
    "                              PAD_token=vocab.stoi[\"<pad>\"],\n",
    "                              beam_size=1,\n",
    "                              max_length=max_seq_len,\n",
    "                              num_out=1)\n",
    "            \n",
    "            sources = encoder_inputs.transpose(0,1).cpu().tolist()\n",
    "            sources = batch_filter_ids(sources)\n",
    "            \n",
    "            predictions = [t[0].view(-1).cpu().tolist() for t in predictions]\n",
    "            predictions = batch_filter_ids(predictions)\n",
    "            \n",
    "            targets = batch.tgt.transpose(0,1).cpu().tolist()\n",
    "            targets = batch_filter_ids(targets)\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                BLEU = nltk_bleu(targets[j], predictions[j])\n",
    "                BLEU_scores.append(BLEU)\n",
    "                OOV_ids = batch.OOVs.cpu()[:,idx].tolist()\n",
    "                \n",
    "                out_fp.write(\"SRC  :\" + vocab.decode(sources[j],OOV_ids) + \"\\n\")\n",
    "                out_fp.write(\"TGT  :\" + vocab.decode(targets[j],OOV_ids) + \"\\n\")\n",
    "                out_fp.write(\"PRED :\" + vocab.decode(predictions[j],OOV_ids) + \"\\n\")\n",
    "                out_fp.write(\"BLEU :\" + str(BLEU) + \"\\n\")\n",
    "                out_fp.write(\"\\n\")\n",
    "            pbar.set_description(f\"BLEU:{np.average(BLEU_scores):5.2f}\")\n",
    "        final_BLEU = np.average(BLEU_scores)\n",
    "        \n",
    "        out_fp.write(\"\\n\\n| EVALUATION | BLEU: {:5.2f} |\\n\".format(final_BLEU))\n",
    "        print(\"| EVALUATION | BLEU: {:5.3f} |\".format(final_BLEU))\n",
    "    return (final_BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "trainer = Model_Trainer(optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fe4f4708194b0db17d17aec13809bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230c45a31e8f49828a16369823ee3ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.331 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fb4ae8eb0b4f568be441ec201109af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.399 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe6a5ed1f714176947af093e3ba42de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.553 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33169267fd1d4570989880076489e777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.594 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a5f810d54d49968407778aa9f3a234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.646 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508835588a7149168e2a7646d7beef83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.719 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310083e9e862408f965ba0175abacb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.757 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100510101daa45c89b7b01b89b21686f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.783 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a19589a5cce498285499d0243fcd2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.787 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2055b53324b14720b6eb6803afb06dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.796 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0f6ff7eda04949bf7eefd0c307d4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.806 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ebb1880f6d4441824a394e257b9d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.806 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5c0e099db446e78b903a4d3d391014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.809 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e38481d39b4394a4888bc75fad9399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.809 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6887a3d49b95498a86d55edf8dd0511d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.811 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0470dc4d7c949a78fc287fa87d5df0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.820 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b168a7557144c429c760dfe8bb4650f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.818 |\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a860cbeca141f7a1531dcbc01164ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.819 |\n"
     ]
    }
   ],
   "source": [
    "scores = trainer.train(model,train_iterator,train_step,500000,test_iterator=test_iterator,eval_fn=evaluate, eval_interval=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1():\n",
    "    a = 3\n",
    "    time.sleep(0.1)\n",
    "    return a\n",
    "    \n",
    "def func2():\n",
    "    b = \"foo\"\n",
    "    c = func1()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.100124 s\n",
       "File: <ipython-input-37-f0ccff687a43>\n",
       "Function: func1 at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def func1():\n",
       "     2         1          0.0      0.0      0.0      a = 3\n",
       "     3         1     100121.0 100121.0    100.0      time.sleep(0.1)\n",
       "     4         1          3.0      3.0      0.0      return a\n",
       "\n",
       "Total time: 0.100139 s\n",
       "File: <ipython-input-37-f0ccff687a43>\n",
       "Function: func2 at line 6\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     6                                           def func2():\n",
       "     7         1          3.0      3.0      0.0      b = \"foo\"\n",
       "     8         1     100135.0 100135.0    100.0      c = func1()\n",
       "     9         1          1.0      1.0      0.0      return c"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f func1 -f func2 func2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4a0725e60143e98a00748bbc4a27c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.529808 s\n",
       "File: /nfs/phd_by_carlos/notebooks/models_and_trainers/exposed_transformer.py\n",
       "Function: forward at line 73\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    73                                               def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
       "    74                                                           memory_mask=None, src_key_padding_mask=None,\n",
       "    75                                                           tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
       "    76                                                   r\"\"\"Take in and process masked source/target sequences.\n",
       "    77                                           \n",
       "    78                                                   Args:\n",
       "    79                                                       src: the sequence to the encoder (required).\n",
       "    80                                                       tgt: the sequence to the decoder (required).\n",
       "    81                                                       src_mask: the additive mask for the src sequence (optional).\n",
       "    82                                                       tgt_mask: the additive mask for the tgt sequence (optional).\n",
       "    83                                                       memory_mask: the additive mask for the encoder output (optional).\n",
       "    84                                                       src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
       "    85                                                       tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
       "    86                                                       memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
       "    87                                           \n",
       "    88                                                   Shape:\n",
       "    89                                                       - src: :math:`(S, N, E)`.\n",
       "    90                                                       - tgt: :math:`(T, N, E)`.\n",
       "    91                                                       - src_mask: :math:`(S, S)`.\n",
       "    92                                                       - tgt_mask: :math:`(T, T)`.\n",
       "    93                                                       - memory_mask: :math:`(T, S)`.\n",
       "    94                                                       - src_key_padding_mask: :math:`(N, S)`.\n",
       "    95                                                       - tgt_key_padding_mask: :math:`(N, T)`.\n",
       "    96                                                       - memory_key_padding_mask: :math:`(N, S)`.\n",
       "    97                                           \n",
       "    98                                                       Note: [src/tgt/memory]_mask should be filled with\n",
       "    99                                                       float('-inf') for the masked positions and float(0.0) else. These masks\n",
       "   100                                                       ensure that predictions for position i depend only on the unmasked positions\n",
       "   101                                                       j and are applied identically for each sequence in a batch.\n",
       "   102                                                       [src/tgt/memory]_key_padding_mask should be a ByteTensor where True values are positions\n",
       "   103                                                       that should be masked with float('-inf') and False values will be unchanged.\n",
       "   104                                                       This mask ensures that no information will be taken from position i if\n",
       "   105                                                       it is masked, and has a separate mask for each sequence in a batch.\n",
       "   106                                           \n",
       "   107                                                       - output: :math:`(T, N, E)`.\n",
       "   108                                           \n",
       "   109                                                       Note: Due to the multi-head attention architecture in the transformer model,\n",
       "   110                                                       the output sequence length of a transformer is same as the input sequence\n",
       "   111                                                       (i.e. target) length of the decode.\n",
       "   112                                           \n",
       "   113                                                       where S is the source sequence length, T is the target sequence length, N is the\n",
       "   114                                                       batch size, E is the feature number\n",
       "   115                                           \n",
       "   116                                                   Examples:\n",
       "   117                                                       >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
       "   118                                                   \"\"\"\n",
       "   119                                           \n",
       "   120        49        216.0      4.4      0.0          if src.size(1) != tgt.size(1):\n",
       "   121                                                       raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
       "   122                                           \n",
       "   123        49         85.0      1.7      0.0          if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
       "   124                                                       raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
       "   125                                           \n",
       "   126        49     208735.0   4259.9     39.4          memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
       "   127        49        276.0      5.6      0.1          output, atts = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
       "   128        49         25.0      0.5      0.0                                tgt_key_padding_mask=tgt_key_padding_mask,\n",
       "   129        49     320405.0   6538.9     60.5                                memory_key_padding_mask=memory_key_padding_mask)\n",
       "   130        49         66.0      1.3      0.0          return output, atts"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.transformer.forward trainer.train(model,train_iterator,train_step,50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.src]:[torch.cuda.LongTensor of size 28x32 (GPU 0)]\n",
       "\t[.tgt]:[torch.cuda.LongTensor of size 22x32 (GPU 0)]\n",
       "\t[.OOVs]:[torch.cuda.LongTensor of size 2x32 (GPU 0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(it)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.14072 s\n",
       "File: /nfs/phd_by_carlos/notebooks/models_and_trainers/copy_gen_transformer.py\n",
       "Function: forward at line 202\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   202                                               def forward(self, src, tgt):\n",
       "   203         1      49978.0  49978.0     35.5          self.tgt_mask = self._generate_square_subsequent_mask(len(tgt)).to(self.device) if self.masked_look_ahead_att else None\n",
       "   204                                                   \n",
       "   205                                           \n",
       "   206         1        993.0    993.0      0.7          src_emb = self.src_embedder(src) * math.sqrt(self.embedding_size)\n",
       "   207                                                   \n",
       "   208         1       1350.0   1350.0      1.0          tgt_emb = self.tgt_embedder(tgt) * math.sqrt(self.embedding_size)\n",
       "   209                                                   \n",
       "   210         1      86841.0  86841.0     61.7          output, atts = self.transformer(src_emb, tgt_emb, tgt_mask=self.tgt_mask)\n",
       "   211         1         97.0     97.0      0.1          vocab_output = self.decoder(output)\n",
       "   212                                                   \n",
       "   213         1          2.0      2.0      0.0          if self.use_copy:\n",
       "   214         1          7.0      7.0      0.0              src_scat = src.transpose(0,1)\n",
       "   215         1          6.0      6.0      0.0              src_scat = src_scat.unsqueeze(0)\n",
       "   216         1       1063.0   1063.0      0.8              src_scat = torch.repeat_interleave(src_scat, tgt.shape[0], dim=0)\n",
       "   217                                               #         print(\"src_scat.shqape\", src_scat.shape)\n",
       "   218                                           \n",
       "   219         1        148.0    148.0      0.1              p_gens = self.p_generator(output).sigmoid()\n",
       "   220         1          8.0      8.0      0.0              atts = atts.transpose(0,1)\n",
       "   221                                               #         print(\"att.shqape\", atts.shape)\n",
       "   222         1        101.0    101.0      0.1              atts = atts * (1 - p_gens)\n",
       "   223                                           \n",
       "   224                                           #             output = self.decoder(output)\n",
       "   225                                               #         output[:,:,12:] = -np.inf\n",
       "   226         1         26.0     26.0      0.0              vocab_output = vocab_output.softmax(-1)\n",
       "   227         1         22.0     22.0      0.0              vocab_output = vocab_output * p_gens\n",
       "   228                                           \n",
       "   229         1         45.0     45.0      0.0              vocab_output = vocab_output.scatter_add_(2,src_scat,atts)\n",
       "   230         1         32.0     32.0      0.0              vocab_output = vocab_output.log()\n",
       "   231                                                   \n",
       "   232         1          1.0      1.0      0.0          return vocab_output"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.forward train_step(batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274688748e9745249a2b37f0ac3b436e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.611 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6105721971123488"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into define'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.decode([50, 40],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
