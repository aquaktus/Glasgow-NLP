{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy-Generator Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm.notebook as tqdm \n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import operator\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "from models_and_trainers.base_transformer import TransformerModel, PositionalEncoding\n",
    "from models_and_trainers.copy_gen_transformer import Transformer, TransformerDecoderLayer, TransformerDecoder\n",
    "from models_and_trainers.retrieval import PyLuceneRetriever\n",
    "# import beam_search\n",
    "from utils.edit_tagger import build_matrix, single_step_edits, perform_edits, get_tags\n",
    "\n",
    "from IPython.core.debugger import set_trace as tr\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def super_print(filename):\n",
    "    '''filename is the file where output will be written'''\n",
    "    def wrap(func):\n",
    "        '''func is the function you are \"overriding\", i.e. wrapping'''\n",
    "        def wrapped_func(*args,**kwargs):\n",
    "            '''*args and **kwargs are the arguments supplied \n",
    "            to the overridden function'''\n",
    "            #use with statement to open, write to, and close the file safely\n",
    "            with open(filename,'a') as outputfile:\n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                outputfile.write(\"[{}] \".format(dt_string))\n",
    "                outputfile.write(\" \".join(str(x) for x in args))\n",
    "                outputfile.write(\"\\n\")\n",
    "            #now original function executed with its arguments as normal\n",
    "            return func(*args,**kwargs)\n",
    "        return wrapped_func\n",
    "    return wrap\n",
    "\n",
    "print = super_print('logs-copy-gen.txt')(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.set_device(0) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['create', 'variable', 'student_names', 'with', 'string', \"'foo bar baz'\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"create variable student_names with string 'foo bar baz'\"\n",
    "\n",
    "def string_split(s):\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(_|\\W)', s))) # this will chunk all code properly by plits strings with quotes\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|_|\\W)', s))) # this keeps the strings intact\n",
    "    return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|\\W)', s)))\n",
    "\n",
    "print(string_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_array(src_fp, tgt_fp):\n",
    "    lines = []\n",
    "    with open(src_fp, \"r\") as src_file, open(tgt_fp, \"r\") as tgt_file:\n",
    "        for src, tgt in zip(src_file, tgt_file):\n",
    "            lines.append((src, tgt))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(data, max_seq_length=200, tokenizer=string_split):\n",
    "    return [(src, tgt) for src, tgt in data if len(string_split(src)) <= max_seq_length and len(string_split(tgt)) <= max_seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_dataset(samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples: [(src_string),(tgt_string)]\n",
    "        src/tgt_tokenizer: a func that takes a string and returns an array of strings\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    TEXT_FIELD = Field(sequential=True, use_vocab=False, init_token='<sos>',eos_token='<eos>')\n",
    "    \n",
    "    for sample in samples:\n",
    "        src_string, tgt_string = sample\n",
    "        examples.append(torchtext.data.Example.fromdict({\"src\":src_string, \"tgt\":tgt_string}, \n",
    "                                        fields={\"src\":(\"src\",TEXT_FIELD), \"tgt\":(\"tgt\",TEXT_FIELD)}))\n",
    "        \n",
    "    dataset = torchtext.data.Dataset(examples,fields={\"src\":src_field, \"tgt\":tgt_field})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src length: 557\n",
      "Max tgt length: 527\n",
      "Full train dataset size: 16924\n",
      "Limited train dataset size: 16797\n"
     ]
    }
   ],
   "source": [
    "train_data = corpus_to_array(\"datasets/django_folds/django.fold1-10.train.src\", \"datasets/django_folds/django.fold1-10.train.tgt\")\n",
    "# data = corpus_to_array(\"datasets/all.desc\", \"datasets/all.code\")\n",
    "random.shuffle(train_data)\n",
    "print(\"Max src length:\", max([len(string_split(src)) for src, tgt in train_data]))\n",
    "print(\"Max tgt length:\", max([len(string_split(tgt)) for src, tgt in train_data]))\n",
    "\n",
    "print(\"Full train dataset size:\", len(train_data))\n",
    "max_seq_length=50\n",
    "train_data = filter_corpus(train_data, max_seq_length=max_seq_length, tokenizer=string_split)\n",
    "train_data = [(\" \".join(string_split(src)),\" \".join(string_split(tgt))) for src, tgt in train_data]\n",
    "print(\"Limited train dataset size:\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited train dataset size: 1874\n"
     ]
    }
   ],
   "source": [
    "test_data = corpus_to_array(\"datasets/django_folds/django.fold1-10.test.src\", \"datasets/django_folds/django.fold1-10.test.tgt\")\n",
    "test_data = filter_corpus(test_data, max_seq_length=max_seq_length, tokenizer=string_split)\n",
    "test_data = [(\" \".join(string_split(src)),\" \".join(string_split(tgt))) for src, tgt in test_data]\n",
    "print(\"Limited train dataset size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {\"<unk>\":0, \"<sos>\":1, \"<eos>\":2, \"<pad>\":3, \"<gen>\":4}\n",
    "max_vocab = 2000 - len(stoi)\n",
    "\n",
    "all_toks = []\n",
    "for (src, tgt) in train_data:\n",
    "    all_toks += string_split(src)\n",
    "    all_toks += string_split(tgt)\n",
    "\n",
    "most_freq = collections.Counter(all_toks).most_common(max_vocab)\n",
    "\n",
    "for tok, count in most_freq:\n",
    "    stoi[tok] = len(stoi)\n",
    "    \n",
    "itos = [k for k,v in sorted(stoi.items(), key=lambda kv: kv[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(string, from_list=False):\n",
    "    OOVs = []\n",
    "    IDs = []\n",
    "    if not from_list:\n",
    "        words = string_split(string)\n",
    "    else:\n",
    "        words = string\n",
    "    for word in words:\n",
    "        try:\n",
    "            id = stoi[word]\n",
    "            IDs.append(id)\n",
    "        except KeyError as e:\n",
    "            # word is OOV\n",
    "            IDs.append(len(stoi) + len(OOVs))\n",
    "            OOVs.append(word)\n",
    "    return IDs, OOVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(string, OOVs, from_list=False):\n",
    "    IDs = []\n",
    "    if from_list:\n",
    "        words = string\n",
    "    else:\n",
    "        words = string_split(string)\n",
    "    for word in words:\n",
    "        try:\n",
    "            id = stoi[word]\n",
    "            IDs.append(id)\n",
    "        except KeyError as e:\n",
    "            # word is OOV\n",
    "            try:\n",
    "                IDs.append(len(stoi) + OOVs.index(word))\n",
    "            except ValueError as e:\n",
    "                IDs.append(stoi[\"<unk>\"])\n",
    "    return IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, OOVs):\n",
    "    extended_itos = itos.copy()\n",
    "    extended_itos += [OOV+\"(COPY)\" for OOV in OOVs]\n",
    "    return \" \".join([extended_itos[id] for id in ids if id<len(extended_itos)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval \n",
    "We want to find the most appropriate code to the input description. For this we use PyLucene to provide a BM25 search over the english descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('if match is false or call to the method match . group with an argument integer 1 evaluates to boolean false ,',\n",
       " 'if not match or not match . group ( 1 ) :')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = PyLuceneRetriever()\n",
    "\n",
    "src_data = [src for src,tgt in train_data]\n",
    "retriever.add_multiple_docs(src_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 best matched samples\n",
      "Description: otherwise if common_prefix is None ,\n",
      "Code       : elif common_prefix is None :\n",
      "\n",
      "Description: common_prefix is None .\n",
      "Code       : common_prefix = None\n",
      "\n",
      "Description: otherwise if prefix is not equal to common_prefix ,\n",
      "Code       : elif prefix ! = common_prefix :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_ranking = retriever.BM25_search(\"otherwise if common_prefix is None\")\n",
    "doc_ids = [x.doc for x in doc_ranking]\n",
    "retrieved_samples = [train_data[i] for i in doc_ids]\n",
    "print(\"10 best matched samples\")\n",
    "for doc in retrieved_samples[:10]:\n",
    "    print(f\"Description: {doc[0]}\")\n",
    "    print(f\"Code       : {doc[1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Edit steps\n",
    "While there is a vocabulary for the shared english and code. The editing tokens also need to be converted to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_stoi = {\"K\":0, \"D\":1, \"R\":2, \"<pad>\":3}\n",
    "edit_itos = {0:\"K\", 1:\"D\", 2:\"R\", 3:\"<pad>\"}\n",
    "max_insertions = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_commands(commands):\n",
    "    return [edit_stoi[command] for command in commands]\n",
    "\n",
    "def decode_commands(commands):\n",
    "    return [edit_itos[command] for command in commands]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single edit example\n",
    "Let's make a complete sample from the dataset.\n",
    "![alt text](./images/edit_transformer_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'if', 'not', 'match', 'or', 'not', 'match', '.', 'group', '(', '1', ')', ':'] ['<sos>', 'if', 'not', 'self', '.', 'user_regex', '.', 'match', '(', 'user_part', ')', ':']\n",
      "[{'code_target': ['<sos>',\n",
      "                  'if',\n",
      "                  'not',\n",
      "                  'match',\n",
      "                  'or',\n",
      "                  'not',\n",
      "                  'match',\n",
      "                  '.',\n",
      "                  'group',\n",
      "                  '(',\n",
      "                  '1',\n",
      "                  ')',\n",
      "                  ':'],\n",
      "  'decoder_input': ['<sos>',\n",
      "                    'if',\n",
      "                    'not',\n",
      "                    'self',\n",
      "                    '.',\n",
      "                    'user_regex',\n",
      "                    '.',\n",
      "                    'match',\n",
      "                    '(',\n",
      "                    'user_part',\n",
      "                    ')',\n",
      "                    ':'],\n",
      "  'encoder_input': ['if',\n",
      "                    'match',\n",
      "                    'is',\n",
      "                    'false',\n",
      "                    'or',\n",
      "                    'call',\n",
      "                    'to',\n",
      "                    'the',\n",
      "                    'method',\n",
      "                    'match',\n",
      "                    '.',\n",
      "                    'group',\n",
      "                    'with',\n",
      "                    'an',\n",
      "                    'argument',\n",
      "                    'integer',\n",
      "                    '1',\n",
      "                    'evaluates',\n",
      "                    'to',\n",
      "                    'boolean',\n",
      "                    'false',\n",
      "                    ','],\n",
      "  'target_commands': ['K',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'R',\n",
      "                      'R',\n",
      "                      'R',\n",
      "                      'K',\n",
      "                      'R',\n",
      "                      'K',\n",
      "                      'R',\n",
      "                      'K',\n",
      "                      'K'],\n",
      "  'target_insertions': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "  'target_replacements': ['<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          'or',\n",
      "                          'not',\n",
      "                          'match',\n",
      "                          '<pad>',\n",
      "                          'group',\n",
      "                          '<pad>',\n",
      "                          '1',\n",
      "                          '<pad>',\n",
      "                          '<pad>']},\n",
      " {'code_target': ['<sos>',\n",
      "                  'if',\n",
      "                  'not',\n",
      "                  'match',\n",
      "                  'or',\n",
      "                  'not',\n",
      "                  'match',\n",
      "                  '.',\n",
      "                  'group',\n",
      "                  '(',\n",
      "                  '1',\n",
      "                  ')',\n",
      "                  ':'],\n",
      "  'decoder_input': ['<sos>',\n",
      "                    'if',\n",
      "                    'not',\n",
      "                    '<gen>',\n",
      "                    'or',\n",
      "                    'not',\n",
      "                    'match',\n",
      "                    '.',\n",
      "                    'group',\n",
      "                    '(',\n",
      "                    '1',\n",
      "                    ')',\n",
      "                    ':'],\n",
      "  'encoder_input': ['if',\n",
      "                    'match',\n",
      "                    'is',\n",
      "                    'false',\n",
      "                    'or',\n",
      "                    'call',\n",
      "                    'to',\n",
      "                    'the',\n",
      "                    'method',\n",
      "                    'match',\n",
      "                    '.',\n",
      "                    'group',\n",
      "                    'with',\n",
      "                    'an',\n",
      "                    'argument',\n",
      "                    'integer',\n",
      "                    '1',\n",
      "                    'evaluates',\n",
      "                    'to',\n",
      "                    'boolean',\n",
      "                    'false',\n",
      "                    ','],\n",
      "  'target_commands': ['K',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'R',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'K',\n",
      "                      'K'],\n",
      "  'target_insertions': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "  'target_replacements': ['<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          'match',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>',\n",
      "                          '<pad>']}]\n"
     ]
    }
   ],
   "source": [
    "x, y = train_data[0]\n",
    "doc_ranking = retriever.BM25_search(x)\n",
    "top_2_doc = train_data[doc_ranking[2].doc] # get the second best document since the best doc is the same from the description\n",
    "x_, y_ = top_2_doc\n",
    "\n",
    "# convert to token array\n",
    "x = string_split(x)\n",
    "y = [\"<sos>\"] + string_split(y)\n",
    "x_ = string_split(x_)\n",
    "y_ = [\"<sos>\"] + string_split(y_)\n",
    "\n",
    "print(y, y_)\n",
    "\n",
    "dataset_edit_samples = []\n",
    "while y_ != y:\n",
    "    edit_steps = single_step_edits(y_, y)\n",
    "    commands, insertions, replacements = edit_steps\n",
    "    sample = {\n",
    "        \"encoder_input\": x,\n",
    "        \"decoder_input\": y_.copy(),\n",
    "        \"target_commands\": commands,\n",
    "        \"target_insertions\": insertions,\n",
    "        \"target_replacements\": replacements,\n",
    "        \"code_target\": y\n",
    "    }\n",
    "    dataset_edit_samples.append(sample)\n",
    "    y_ = perform_edits(y_, edit_steps, gen_tok_id=\"<gen>\")\n",
    "\n",
    "pprint(dataset_edit_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the dataset\n",
    "The trick to modularizing models effectively is to make important functions that are necessary to them. `data2dataset()` is one suuch example. Taking in the dataset provided from the paper and converting it into the format needed to train our edit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_stoi = {}\n",
    "OOV_itos = {}\n",
    "OOV_starter_count = 30000\n",
    "OOV_count = OOV_starter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f179f64da3948369d3be54b8bd60b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16797), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d103f0d1114484a5be511af7daaa9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1874), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT_FIELD = Field(sequential=True, use_vocab=False, unk_token=0, pad_token=3)\n",
    "OOV_TEXT_FIELD = Field(sequential=True, use_vocab=False, pad_token=3)\n",
    "\n",
    "def data2dataset(data, desc_rank=[1], token_insertions=1, single_step=False, max_insertions=5):\n",
    "    global OOV_count\n",
    "    TEXT_FIELD = Field(sequential=True, use_vocab=False, unk_token=0, pad_token=3)\n",
    "    OOV_TEXT_FIELD = Field(sequential=True, use_vocab=False, pad_token=3)\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    for (src, tgt) in tqdm.tqdm(data):\n",
    "        \n",
    "        for rank in desc_rank:\n",
    "            doc_ranking = retriever.BM25_search(src)\n",
    "            if len(doc_ranking) > rank:\n",
    "                top_2_doc = train_data[doc_ranking[rank].doc]\n",
    "                x_, y_ = top_2_doc\n",
    "            else:\n",
    "                x_, y_ = \"\", \"\"\n",
    "            src_ids, OOVs = encode_input(src)\n",
    "            decoder_input = encode_output(y_, OOVs)\n",
    "            retrieved_code = decoder_input.copy()\n",
    "            ground_truth_code = encode_output(tgt, OOVs)\n",
    "\n",
    "            decoder_input = [stoi[\"<sos>\"]] + decoder_input\n",
    "            ground_truth_code = [stoi[\"<sos>\"]] + ground_truth_code\n",
    "\n",
    "            ran_once = False\n",
    "\n",
    "            while decoder_input != ground_truth_code or not ran_once:\n",
    "                ran_once = True\n",
    "                edit_steps = single_step_edits(decoder_input, \n",
    "                                               ground_truth_code, \n",
    "                                               pad_token=stoi[\"<pad>\"], \n",
    "                                               token_insertions=token_insertions,\n",
    "                                               max_insertions=max_insertions)\n",
    "                commands, target_insertions, target_replacements = edit_steps\n",
    "\n",
    "                target_commands = encode_commands(commands)\n",
    "\n",
    "                OOV_ids = []\n",
    "\n",
    "                for OOV in OOVs:\n",
    "                    try:\n",
    "                        idx = OOV_stoi[OOV]\n",
    "                        OOV_ids.append(idx)\n",
    "                    except KeyError as e:\n",
    "                        OOV_count += 1\n",
    "                        OOV_stoi[OOV] = OOV_count\n",
    "                        OOV_itos[OOV_count] = OOV\n",
    "                        OOV_ids.append(OOV_count)\n",
    "\n",
    "\n",
    "                if \"<DELETE_ME>\" in decoder_input:\n",
    "                    print(decoder_input)\n",
    "\n",
    "                example = torchtext.data.Example.fromdict({\"encoder_input\":src_ids, \n",
    "                                                           \"ground_truth_code\":ground_truth_code, \n",
    "                                                           \"retrieved_code\":retrieved_code,\n",
    "                                                           \"OOVs\":OOV_ids, \n",
    "                                                           \"decoder_input\":decoder_input,\n",
    "                                                           \"target_commands\":target_commands, \n",
    "                                                           \"target_insertions\": target_insertions,\n",
    "                                                           \"target_replacements\":target_replacements}, \n",
    "                                                        fields={\"encoder_input\":(\"encoder_input\",TEXT_FIELD), \n",
    "                                                                \"ground_truth_code\":(\"ground_truth_code\",TEXT_FIELD),\n",
    "                                                                \"retrieved_code\":(\"retrieved_code\",TEXT_FIELD),\n",
    "                                                                \"OOVs\":(\"OOVs\", OOV_TEXT_FIELD), \n",
    "                                                                \"decoder_input\":(\"decoder_input\",TEXT_FIELD),\n",
    "                                                                \"target_commands\":(\"target_commands\",TEXT_FIELD), \n",
    "                                                                \"target_insertions\": (\"target_insertions\",TEXT_FIELD),\n",
    "                                                                \"target_replacements\":(\"target_replacements\",TEXT_FIELD)})\n",
    "                examples.append(example)\n",
    "                decoder_input = perform_edits(decoder_input, edit_steps, gen_tok_id=stoi[\"<gen>\"])\n",
    "                if single_step:\n",
    "                    break\n",
    "    return examples\n",
    "\n",
    "train_examples = data2dataset(train_data, token_insertions=-1, desc_rank=[1],max_insertions=max_insertions)\n",
    "test_examples = data2dataset(test_data, desc_rank=[0], single_step=True) # single step here since we don't want to duplicate multi edit samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV_itos size: 6168\n"
     ]
    }
   ],
   "source": [
    "print(\"OOV_itos size:\", len(OOV_itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples in train dataset: 21350\n",
      "Num samples in test dataset: 1874\n"
     ]
    }
   ],
   "source": [
    "print(\"Num samples in train dataset:\", len(train_examples))\n",
    "print(\"Num samples in test dataset:\", len(test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchtext.data.Dataset(train_examples,fields={\"encoder_input\":TEXT_FIELD, \n",
    "                                                  \"ground_truth_code\":TEXT_FIELD, \n",
    "                                                  \"retrieved_code\":TEXT_FIELD,\n",
    "                                                  \"OOVs\":OOV_TEXT_FIELD, \n",
    "                                                  \"decoder_input\":TEXT_FIELD, \n",
    "                                                  \"target_commands\":TEXT_FIELD, \n",
    "                                                  \"target_insertions\":TEXT_FIELD, \n",
    "                                                  \"target_replacements\":TEXT_FIELD})\n",
    "\n",
    "val_dataset = torchtext.data.Dataset(test_examples,fields={\"encoder_input\":TEXT_FIELD, \n",
    "                                                  \"ground_truth_code\":TEXT_FIELD, \n",
    "                                                  \"retrieved_code\":TEXT_FIELD,\n",
    "                                                  \"OOVs\":OOV_TEXT_FIELD, \n",
    "                                                  \"decoder_input\":TEXT_FIELD, \n",
    "                                                  \"target_commands\":TEXT_FIELD, \n",
    "                                                  \"target_insertions\":TEXT_FIELD, \n",
    "                                                  \"target_replacements\":TEXT_FIELD})\n",
    "# train_dataset, val_dataset = dataset.split([0.9,0.1])\n",
    "# train_dataset = val_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmpad(arr):\n",
    "    pad_id = stoi[\"<pad>\"]\n",
    "    return [x for x in arr if x != pad_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input    : value under the ORDERING_FIELD_NAME(COPY) key of the form . fields is an instance of IntegerField class , created with 3 arguments :\n",
      "decoder_input    : <sos> form . fields [ ORDERING_FIELD_NAME(COPY) ] = IntegerField ( label = _ ( <unk> ) , required = False )\n",
      "ground_truth_code: <sos> form . fields [ ORDERING_FIELD_NAME(COPY) ] = IntegerField ( label = _ ( <unk> ) , initial = index + 1 , required = False )\n",
      "target_insertions  : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0]\n",
      "target_replacements  : [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_iterator = BucketIterator(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    repeat=True,\n",
    "    shuffle=True,\n",
    "    sort_key = lambda x: len(x.encoder_input)+len(x.decoder_input), # this doesn't seem to work, check it out later\n",
    "    device = device)\n",
    "\n",
    "# The iterator generates batches with padded length for sequences with similar sizes, a batch is [seq_length, batch_size]\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    idx = 7\n",
    "#     print([SRC_TEXT.vocab.itos[id] for id in batch.src.cpu().numpy()[:,idx]])\n",
    "    OOVs = [OOV_itos[OOV] for OOV in batch.OOVs.cpu()[:,idx].tolist() if OOV != 3] # 3 is the <pad> token\n",
    "    encoder_input = batch.encoder_input.cpu()[:,idx].tolist()\n",
    "    decoder_input = batch.decoder_input.cpu()[:,idx].tolist()\n",
    "    ground_truth_code = batch.ground_truth_code.cpu()[:,idx].tolist()\n",
    "    target_commands = batch.target_commands.cpu()[:,idx].tolist()\n",
    "    target_insertions = batch.target_insertions.cpu()[:,idx].tolist()\n",
    "    target_replacements = batch.target_replacements.cpu()[:,idx].tolist()\n",
    "    \n",
    "    print(\"encoder_input    :\",decode(rmpad(encoder_input), OOVs))\n",
    "    print(\"decoder_input    :\",decode(rmpad(decoder_input), OOVs))\n",
    "    print(\"ground_truth_code:\",decode(rmpad(ground_truth_code), OOVs))\n",
    "    print(\"target_insertions  :\", rmpad(target_insertions))\n",
    "    print(\"target_replacements  :\", target_replacements)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size=512, dropout=0.5):\n",
    "        super(CopyModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, dropout)\n",
    "        self.src_encoder = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.tgt_encoder = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.transformer = Transformer(d_model=embedding_size, nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024)\n",
    "        self.replacement_decoder = nn.Linear(embedding_size, vocab_size)\n",
    "        self.insertion_decoder = nn.Linear(embedding_size, max_insertions+1)\n",
    "        self.command_decoder = nn.Linear(embedding_size, len(edit_stoi))\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.p_generator = nn.Linear(embedding_size,1)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.tgt_mask = None\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.src_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.replacement_decoder.bias.data.zero_()\n",
    "        self.replacement_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.insertion_decoder.bias.data.zero_()\n",
    "        self.insertion_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.command_decoder.bias.data.zero_()\n",
    "        self.command_decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "#         noise_e = 0.05 if self.training else 0.0 # this is code to add noise to the decoding process during training\n",
    "        noise_e = 0.0 if self.training else 0.0\n",
    "        noise_mask = (torch.rand(sz,sz) > noise_e).float()\n",
    "\n",
    "        mask = (torch.triu(torch.ones(sz,sz))).transpose(0, 1)\n",
    "        mask = torch.mul(mask, noise_mask)\n",
    "        v = (torch.sum(mask, dim=-1) == 0).float()\n",
    "\n",
    "        fix_mask = torch.zeros(sz,sz)\n",
    "        fix_mask[:,0] = 1.0\n",
    "        v = v.repeat(sz, 1).transpose(0,1)\n",
    "        fix_mask = torch.mul(fix_mask,v)\n",
    "\n",
    "        mask += fix_mask\n",
    "        \n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "#         self.tgt_mask = self._generate_square_subsequent_mask(len(tgt)).to(self.device)\n",
    "        \n",
    "\n",
    "        src_emb = self.src_encoder(src) * math.sqrt(self.embedding_size)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        \n",
    "        tgt_emb = self.tgt_encoder(tgt) * math.sqrt(self.embedding_size)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "                \n",
    "        output, atts = self.transformer(src_emb, tgt_emb)\n",
    "        \n",
    "        \n",
    "        src_scat = src.transpose(0,1)\n",
    "        src_scat = src_scat.unsqueeze(0)\n",
    "        src_scat = torch.repeat_interleave(src_scat, tgt.shape[0], dim=0)\n",
    "#         print(\"src_scat.shqape\", src_scat.shape)\n",
    "        \n",
    "        p_gens = self.p_generator(output).sigmoid()\n",
    "        atts = atts.transpose(0,1)\n",
    "#         print(\"att.shqape\", atts.shape)\n",
    "        atts = atts * (1 - p_gens)\n",
    "                \n",
    "        target_replacements = self.replacement_decoder(output)\n",
    "#         output[:,:,12:] = -np.inf\n",
    "        target_replacements = target_replacements.softmax(-1)\n",
    "        target_replacements = target_replacements * p_gens\n",
    "        \n",
    "        target_replacements = target_replacements.scatter_add_(2,src_scat,atts)\n",
    "        \n",
    "        target_insertions = self.insertion_decoder(output)\n",
    "        \n",
    "        target_commands = self.command_decoder(output)\n",
    "        \n",
    "        return target_commands, target_insertions, target_replacements.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi[\"','\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2, 4]), torch.Size([5, 2, 6]), torch.Size([5, 2, 2050]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos) + max_seq_length\n",
    "\n",
    "model = CopyModel(vocab_size).to(device) \n",
    "src = torch.randint(0, vocab_size, (3,2)).to(device)\n",
    "tgt = torch.randint(0, vocab_size, (5,2)).to(device)\n",
    "\n",
    "target_commands, target_insertions, target_replacements = model(src, tgt)\n",
    "target_commands.shape, target_insertions.shape, target_replacements.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_bleu(refrence, prediction):\n",
    "    \"\"\"\n",
    "    Implementation from ReCode\n",
    "    and moses multi belu script sets BLEU to 0.0 if len(toks) < 4\n",
    "    \"\"\"\n",
    "    ngram_weights = [0.25] * min(4, len(refrence))\n",
    "    return sentence_bleu([refrence], prediction, weights=ngram_weights, \n",
    "                          smoothing_function=SmoothingFunction().method3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input      : app_config , connection . alias and include_auto_created set to boolean True . call the method connection . creation . with arguments model , style and known_models , substitute the result for output and .\n",
      "ground_truth_code  : <sos> output , = connection . creation . ( model , style , known_models )\n",
      "decoder_input      : <sos> output , = connection . creation . ( model , style <gen> known_models )\n",
      "gt_target_commands : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0]\n",
      "gt_target_insertions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "gt_target_replacements: ,\n",
      "gt_edited_code     : <sos> output , = connection . creation . ( model , style , known_models )\n",
      "\n",
      "encoder_input      : define the method with arguments self , name , , value , base , sysid , pubid and notation_name .\n",
      "ground_truth_code  : <sos> def ( self , name , , value , base , sysid , pubid , notation_name ) :\n",
      "decoder_input      : <sos> def ( self , name , <gen> <gen> value , base , sysid , pubid , notation_name ) :\n",
      "gt_target_commands : [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "gt_target_insertions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "gt_target_replacements: ,\n",
      "gt_edited_code     : <sos> def ( self , name , , value , base , sysid , pubid , notation_name ) :\n",
      "\n",
      "encoder_input      : if last element of args is not equal to a string ,\n",
      "ground_truth_code  : <sos> if args [ - 1 ] ! = :\n",
      "decoder_input      : <sos> if django . <unk> [ - 2 ] ! = <unk> :\n",
      "gt_target_commands : [0, 0, 1, 1, 2, 0, 0, 2, 0, 0, 0, 2, 0]\n",
      "gt_target_insertions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "gt_target_replacements: args 1\n",
      "gt_edited_code     : <sos> if args [ - 1 ] ! = :\n",
      "\n",
      "encoder_input      : while start is lesser than end .\n",
      "ground_truth_code  : <sos> while start < end :\n",
      "decoder_input      : <sos> if end < 0 :\n",
      "gt_target_commands : [0, 2, 2, 0, 2, 0]\n",
      "gt_target_insertions: [0, 0, 0, 0, 0, 0]\n",
      "gt_target_replacements: while start end\n",
      "gt_edited_code     : <sos> while start < end :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def outputs2code(target_commands, target_insertions, target_replacements, decoder_input):\n",
    "    batch_size = target_commands.shape[1]\n",
    "    \n",
    "    edited_code_samples = []\n",
    "    for i in range(batch_size):\n",
    "        code_to_edit = rmpad(decoder_input[:,i].tolist())\n",
    "        code_length = len(code_to_edit)\n",
    "        \n",
    "        sample_commands = target_commands[:code_length,i].view(-1).tolist()\n",
    "#         print(sample_commands)\n",
    "        sample_commands = decode_commands(sample_commands)\n",
    "        sample_insertions = target_insertions[:code_length,i].view(-1).tolist()\n",
    "        sample_replacements = target_replacements[:code_length,i].view(-1).tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        edits = (sample_commands, sample_insertions, sample_replacements)\n",
    "        \n",
    "        edited_code = perform_edits(code_to_edit, edits, gen_tok_id=stoi[\"<gen>\"])\n",
    "        \n",
    "        edited_code_samples.append(edited_code)\n",
    "        \n",
    "    max_sample_len = max([len(sample) for sample in edited_code_samples])\n",
    "\n",
    "    edited_code_samples = [(sample + max_sample_len * [stoi[\"<pad>\"]])[:max_sample_len] for sample in edited_code_samples]\n",
    "    return torch.tensor(edited_code_samples).T.to(device)\n",
    "    \n",
    "        \n",
    "batch = next(iter(train_iterator))\n",
    "decoder_input = batch.decoder_input\n",
    "encoder_input = batch.encoder_input\n",
    "ground_truth_code = batch.ground_truth_code\n",
    "OOVss = batch.OOVs\n",
    "\n",
    "gt_target_commands = batch.target_commands\n",
    "gt_target_insertions = batch.target_insertions\n",
    "gt_target_replacements = batch.target_replacements\n",
    "\n",
    "gt_edited_code = outputs2code(gt_target_commands, gt_target_insertions, gt_target_replacements, decoder_input)\n",
    "\n",
    "target_commands, target_insertions, target_replacements = model(encoder_input, decoder_input)\n",
    "\n",
    "_, argmax_target_commands = target_commands.max(2)\n",
    "_, argmax_target_insertions = target_insertions.max(2)\n",
    "_, argmax_target_replacements = target_replacements.max(2)\n",
    "\n",
    "outputs2code(argmax_target_commands, argmax_target_insertions, argmax_target_replacements, decoder_input)\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"encoder_input      :\", decode(rmpad(encoder_input[:,i].tolist()), []))\n",
    "    print(\"ground_truth_code  :\",decode(rmpad(ground_truth_code[:,i].tolist()), []))\n",
    "    print(\"decoder_input      :\",decode(rmpad(decoder_input[:,i].tolist()), []))\n",
    "    print(\"gt_target_commands :\", rmpad(gt_target_commands[:,i].tolist()))\n",
    "    print(\"gt_target_insertions:\", rmpad(gt_target_insertions[:,i].tolist()))\n",
    "    print(\"gt_target_replacements:\", decode(rmpad(gt_target_replacements[:,i].tolist()), []))\n",
    "    print(\"gt_edited_code     :\",decode(rmpad(gt_edited_code[:,i].tolist()), []))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4310125255226661"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLEU_scores = []\n",
    "for src, tgt in test_data:\n",
    "    doc_ranking = retriever.BM25_search(src)\n",
    "    if len(doc_ranking) > 0:\n",
    "        top_2_doc = train_data[doc_ranking[0].doc]\n",
    "        x_, y_ = top_2_doc\n",
    "    else:\n",
    "        x_, y_ = \"\", \"\"\n",
    "    \n",
    "    encoder_input, OOVs = encode_input(src)\n",
    "    decoder_input = encode_output(tgt, OOVs)\n",
    "    retrieved = encode_output(y_, OOVs)\n",
    "    BLEU = nltk_bleu(decoder_input, retrieved)\n",
    "    BLEU_scores.append(BLEU)\n",
    "np.average(BLEU_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iterator = BucketIterator(val_dataset,\n",
    "    batch_size = 32,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "def batch_filter_ids(batch_list):\n",
    "    return [[id for id in l if id not in [1,2,3]] for l in batch_list]\n",
    "\n",
    "def evaluate(beam_size=1, log=False):\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    with torch.no_grad(), open(\"out.txt\", \"w\") as out_fp:\n",
    "        BLEU_scores = []\n",
    "        ret_BLEU_scores = []\n",
    "        \n",
    "        for i, batch in enumerate(tqdm.tqdm(valid_iterator)):\n",
    "            batch_size = batch.encoder_input.shape[1]\n",
    "            max_iter = 8\n",
    "            \n",
    "            code_edits_finished = [None] * batch_size\n",
    "            \n",
    "            decoder_input = batch.decoder_input\n",
    "            encoder_input = batch.encoder_input            \n",
    "            OOVss = [[OOV_itos[OOV] for OOV in batch.OOVs.cpu()[:,idx].tolist() if OOV != 3] for idx in range(batch_size)]\n",
    "            \n",
    "            while not all(code_edits_finished) and max_iter > 0:\n",
    "                max_iter -= 1\n",
    "                target_commands, target_insertions, target_replacements = model(encoder_input, decoder_input)\n",
    "                \n",
    "                _, argmax_target_commands = target_commands.max(2)\n",
    "                _, argmax_target_insertions = target_insertions.max(2)\n",
    "                _, argmax_target_replacements = target_replacements.max(2)\n",
    "                                \n",
    "                code_samples = outputs2code(argmax_target_commands, argmax_target_insertions, argmax_target_replacements, decoder_input)\n",
    "                                \n",
    "                for j in range(batch_size):\n",
    "                    if (stoi[\"<gen>\"] not in code_samples[:,j] and code_edits_finished[j] == None) or max_iter == 0:\n",
    "                        code_edits_finished[j] = rmpad(code_samples[:,j].cpu().tolist())\n",
    "                        BLEU = nltk_bleu(\n",
    "                            rmpad(batch.ground_truth_code[1:,j].cpu().tolist()),\n",
    "                            code_edits_finished[j][1:])\n",
    "                        retrieval_BLEU = nltk_bleu(\n",
    "                            rmpad(batch.ground_truth_code[1:,j].cpu().tolist()),\n",
    "                            rmpad(batch.decoder_input[1:,j].cpu().tolist()))\n",
    "                        BLEU_scores.append(BLEU)\n",
    "                        ret_BLEU_scores.append(retrieval_BLEU)\n",
    "                        \n",
    "                        out_fp.write(f\"Decoder input: {decode(rmpad(batch.decoder_input[:,j].cpu().tolist()), OOVss[j])}\\n\")\n",
    "                        out_fp.write(f\"Target       : {decode(rmpad(batch.ground_truth_code[:,j].cpu().tolist()), OOVss[j])}\\n\")\n",
    "                        out_fp.write(f\"Prediction   : {decode(rmpad(code_samples[:,j].cpu().tolist()), OOVss[j])}\\n\")\n",
    "                        out_fp.write(f\"BLEU         : {BLEU:5.3f}\\n\")\n",
    "                        out_fp.write(f\"Ret BLEU     : {retrieval_BLEU:5.3f}\\n\")\n",
    "                        out_fp.write(\"\\n\")\n",
    "                \n",
    "                decoder_input = code_samples[:max_seq_length]\n",
    "                \n",
    "            \n",
    "            ground_truth_code = batch.ground_truth_code\n",
    "                \n",
    "        out_fp.write(\"\\n\\n| EVALUATION | BLEU: {:5.2f} | ret_BLEU: {:5.3f}\\n\".format(np.average(BLEU_scores), np.average(ret_BLEU_scores)))\n",
    "        print(\"| EVALUATION | BLEU: {:5.3f} | ret_BLEU: {:5.3f}\".format(np.average(BLEU_scores), np.average(ret_BLEU_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9663e85bbf947c7a3bb38737c2a72b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.431 | ret_BLEU: 0.431\n"
     ]
    }
   ],
   "source": [
    "evaluate(beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> if <unk> :\n",
      "<sos> if <unk> :\n",
      "<sos> if <unk> :\n",
      "<sos> if <unk> :\n",
      "<sos> if <unk> :\n",
      "<sos> if <unk> :\n",
      "<sos> if <unk> :\n",
      "<sos> if <unk> :\n"
     ]
    }
   ],
   "source": [
    "def manual_eval(description, rertrieved_code):\n",
    "    max_iter = 8\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_input, OOVs = encode_input(description)\n",
    "        encoder_input = torch.tensor(encoder_input).view(-1,1).to(device)\n",
    "        \n",
    "        decoder_input = encode_output(rertrieved_code, OOVs)\n",
    "        decoder_input = [stoi[\"<sos>\"]] + decoder_input\n",
    "        decoder_input = torch.tensor(decoder_input).view(-1,1).to(device)\n",
    "        \n",
    "        while max_iter > 0:\n",
    "            max_iter -= 1\n",
    "            target_commands, target_insertions, target_replacements = model(encoder_input, decoder_input)\n",
    "            \n",
    "            _, argmax_target_commands = target_commands.max(2)\n",
    "            _, argmax_target_insertions = target_insertions.max(2)\n",
    "            _, argmax_target_replacements = target_replacements.max(2)\n",
    "\n",
    "            code_samples = outputs2code(argmax_target_commands, argmax_target_insertions, argmax_target_replacements, decoder_input)\n",
    "            \n",
    "            print(decode(code_samples.view(-1).cpu().tolist(), OOVs))\n",
    "            \n",
    "            decoder_input = code_samples[:max_seq_length]\n",
    "\n",
    "            \n",
    "description = \"if KeyError exception is caught,\"\n",
    "target = \"except KeyError :\"\n",
    "rertrieved_code = \"except foo :\"\n",
    "\n",
    "manual_eval(description, rertrieved_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=stoi['<pad>'])\n",
    "lr = 0.005 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    vocab_size = len(itos) + max_seq_length\n",
    "    encoder_input = batch.encoder_input\n",
    "    decoder_input = batch.decoder_input\n",
    "    ground_truth_commands = batch.target_commands\n",
    "    ground_truth_insertions = batch.target_insertions\n",
    "    ground_truth_replacements = batch.target_replacements\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    target_commands, target_insertions, target_replacements = model(encoder_input, decoder_input)\n",
    "\n",
    "    \n",
    "    command_loss = criterion(target_commands.view(-1, len(edit_stoi)), ground_truth_commands.view(-1))\n",
    "    insertion_loss = criterion(target_insertions.view(-1, max_insertions+1), ground_truth_insertions.view(-1))\n",
    "    replacement_loss = criterion(target_replacements.view(-1, vocab_size), ground_truth_replacements.view(-1))\n",
    "    \n",
    "    loss = command_loss + insertion_loss + replacement_loss\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    elapsed = time.time() - start_time\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   400/1000000 steps | lr 0.0050 | ms/batch 32.52 | loss  4.80 | ppl   121.50\n",
      "|   800/1000000 steps | lr 0.0050 | ms/batch 32.32 | loss  4.66 | ppl   106.14\n",
      "|  1200/1000000 steps | lr 0.0050 | ms/batch 33.16 | loss  4.56 | ppl    96.06\n",
      "|  1600/1000000 steps | lr 0.0050 | ms/batch 32.39 | loss  4.48 | ppl    88.05\n",
      "|  2000/1000000 steps | lr 0.0050 | ms/batch 32.81 | loss  4.43 | ppl    83.94\n",
      "|  2400/1000000 steps | lr 0.0050 | ms/batch 32.27 | loss  4.39 | ppl    81.02\n",
      "|  2800/1000000 steps | lr 0.0050 | ms/batch 32.39 | loss  4.33 | ppl    76.03\n",
      "|  3200/1000000 steps | lr 0.0050 | ms/batch 32.49 | loss  4.32 | ppl    75.38\n",
      "|  3600/1000000 steps | lr 0.0050 | ms/batch 32.37 | loss  4.28 | ppl    72.17\n",
      "|  4000/1000000 steps | lr 0.0050 | ms/batch 32.23 | loss  4.28 | ppl    72.10\n",
      "|  4400/1000000 steps | lr 0.0050 | ms/batch 32.40 | loss  4.22 | ppl    68.20\n",
      "|  4800/1000000 steps | lr 0.0050 | ms/batch 32.37 | loss  4.22 | ppl    68.09\n",
      "|  5200/1000000 steps | lr 0.0050 | ms/batch 32.13 | loss  4.17 | ppl    64.48\n",
      "|  5600/1000000 steps | lr 0.0050 | ms/batch 32.40 | loss  4.18 | ppl    65.40\n",
      "|  6000/1000000 steps | lr 0.0050 | ms/batch 32.23 | loss  4.14 | ppl    62.78\n",
      "|  6400/1000000 steps | lr 0.0050 | ms/batch 32.46 | loss  4.13 | ppl    62.16\n",
      "|  6800/1000000 steps | lr 0.0050 | ms/batch 32.43 | loss  4.10 | ppl    60.41\n",
      "|  7200/1000000 steps | lr 0.0050 | ms/batch 32.36 | loss  4.10 | ppl    60.64\n",
      "|  7600/1000000 steps | lr 0.0050 | ms/batch 32.33 | loss  4.10 | ppl    60.07\n",
      "|  8000/1000000 steps | lr 0.0050 | ms/batch 32.57 | loss  4.05 | ppl    57.22\n",
      "|  8400/1000000 steps | lr 0.0049 | ms/batch 32.24 | loss  4.05 | ppl    57.20\n",
      "|  8800/1000000 steps | lr 0.0049 | ms/batch 32.52 | loss  4.04 | ppl    57.10\n",
      "|  9200/1000000 steps | lr 0.0049 | ms/batch 32.54 | loss  4.03 | ppl    56.21\n",
      "|  9600/1000000 steps | lr 0.0049 | ms/batch 32.43 | loss  4.00 | ppl    54.57\n",
      "| 10000/1000000 steps | lr 0.0049 | ms/batch 32.42 | loss  4.00 | ppl    54.44\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01060e0141e742cc954457f26c3d5535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.444 | ret_BLEU: 0.430\n",
      "| 10400/1000000 steps | lr 0.0049 | ms/batch 39.32 | loss  3.98 | ppl    53.64\n",
      "| 10800/1000000 steps | lr 0.0049 | ms/batch 32.30 | loss  3.97 | ppl    53.04\n",
      "| 11200/1000000 steps | lr 0.0049 | ms/batch 32.11 | loss  3.98 | ppl    53.46\n",
      "| 11600/1000000 steps | lr 0.0049 | ms/batch 32.57 | loss  3.93 | ppl    50.70\n",
      "| 12000/1000000 steps | lr 0.0049 | ms/batch 32.37 | loss  3.95 | ppl    51.71\n",
      "| 12400/1000000 steps | lr 0.0049 | ms/batch 32.59 | loss  3.93 | ppl    50.98\n",
      "| 12800/1000000 steps | lr 0.0049 | ms/batch 32.28 | loss  3.92 | ppl    50.47\n",
      "| 13200/1000000 steps | lr 0.0049 | ms/batch 32.34 | loss  3.90 | ppl    49.44\n",
      "| 13600/1000000 steps | lr 0.0049 | ms/batch 32.27 | loss  3.88 | ppl    48.43\n",
      "| 14000/1000000 steps | lr 0.0049 | ms/batch 32.55 | loss  3.88 | ppl    48.52\n",
      "| 14400/1000000 steps | lr 0.0049 | ms/batch 32.73 | loss  3.89 | ppl    48.90\n",
      "| 14800/1000000 steps | lr 0.0049 | ms/batch 32.56 | loss  3.86 | ppl    47.29\n",
      "| 15200/1000000 steps | lr 0.0049 | ms/batch 32.34 | loss  3.86 | ppl    47.67\n",
      "| 15600/1000000 steps | lr 0.0049 | ms/batch 32.65 | loss  3.83 | ppl    46.19\n",
      "| 16000/1000000 steps | lr 0.0049 | ms/batch 32.28 | loss  3.85 | ppl    46.81\n",
      "| 16400/1000000 steps | lr 0.0048 | ms/batch 32.38 | loss  3.83 | ppl    45.92\n",
      "| 16800/1000000 steps | lr 0.0048 | ms/batch 32.76 | loss  3.83 | ppl    45.88\n",
      "| 17200/1000000 steps | lr 0.0048 | ms/batch 32.41 | loss  3.81 | ppl    45.30\n",
      "| 17600/1000000 steps | lr 0.0048 | ms/batch 32.36 | loss  3.81 | ppl    45.03\n",
      "| 18000/1000000 steps | lr 0.0048 | ms/batch 32.35 | loss  3.79 | ppl    44.13\n",
      "| 18400/1000000 steps | lr 0.0048 | ms/batch 32.42 | loss  3.78 | ppl    43.97\n",
      "| 18800/1000000 steps | lr 0.0048 | ms/batch 32.25 | loss  3.78 | ppl    43.93\n",
      "| 19200/1000000 steps | lr 0.0048 | ms/batch 32.21 | loss  3.79 | ppl    44.23\n",
      "| 19600/1000000 steps | lr 0.0048 | ms/batch 32.75 | loss  3.75 | ppl    42.42\n",
      "| 20000/1000000 steps | lr 0.0048 | ms/batch 32.43 | loss  3.78 | ppl    43.94\n",
      "Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73699d544bb4f54b5ef354d21cb0f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| EVALUATION | BLEU: 0.431 | ret_BLEU: 0.432\n",
      "| 20400/1000000 steps | lr 0.0048 | ms/batch 40.09 | loss  3.76 | ppl    42.99\n",
      "| 20800/1000000 steps | lr 0.0048 | ms/batch 32.45 | loss  3.75 | ppl    42.61\n",
      "| 21200/1000000 steps | lr 0.0048 | ms/batch 32.24 | loss  3.73 | ppl    41.78\n",
      "| 21600/1000000 steps | lr 0.0048 | ms/batch 32.59 | loss  3.74 | ppl    42.22\n",
      "| 22000/1000000 steps | lr 0.0048 | ms/batch 32.35 | loss  3.74 | ppl    42.07\n",
      "| 22400/1000000 steps | lr 0.0048 | ms/batch 32.35 | loss  3.73 | ppl    41.76\n",
      "| 22800/1000000 steps | lr 0.0048 | ms/batch 32.68 | loss  3.73 | ppl    41.51\n",
      "| 23200/1000000 steps | lr 0.0048 | ms/batch 32.54 | loss  3.72 | ppl    41.20\n",
      "| 23600/1000000 steps | lr 0.0048 | ms/batch 32.30 | loss  3.69 | ppl    40.12\n",
      "| 24000/1000000 steps | lr 0.0048 | ms/batch 32.48 | loss  3.72 | ppl    41.30\n",
      "| 24400/1000000 steps | lr 0.0047 | ms/batch 32.57 | loss  3.70 | ppl    40.46\n",
      "| 24800/1000000 steps | lr 0.0047 | ms/batch 32.46 | loss  3.70 | ppl    40.26\n",
      "| 25200/1000000 steps | lr 0.0047 | ms/batch 32.34 | loss  3.71 | ppl    40.91\n",
      "| 25600/1000000 steps | lr 0.0047 | ms/batch 32.59 | loss  3.69 | ppl    39.85\n",
      "| 26000/1000000 steps | lr 0.0047 | ms/batch 32.35 | loss  3.68 | ppl    39.80\n",
      "| 26400/1000000 steps | lr 0.0047 | ms/batch 32.44 | loss  3.66 | ppl    39.01\n",
      "| 26800/1000000 steps | lr 0.0047 | ms/batch 32.28 | loss  3.65 | ppl    38.43\n"
     ]
    }
   ],
   "source": [
    "def train(steps=10000, log_interval=200, learning_interval=4000, eval_interval=1000):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    step = 1\n",
    "    for batch in train_iterator:\n",
    "        loss = train_step(batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} steps | '\n",
    "                  'lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    step, steps, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        if step % eval_interval == 0:\n",
    "            print(\"Evaluating model\")\n",
    "            evaluate()\n",
    "            model.train()\n",
    "        \n",
    "        if step % learning_interval == 0:\n",
    "            scheduler.step()\n",
    "        \n",
    "        step += 1\n",
    "        if step >= steps:\n",
    "            print(\"Finished training\")\n",
    "\n",
    "\n",
    "\n",
    "            return\n",
    "\n",
    "train(steps=1000000,eval_interval=10000,log_interval=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
