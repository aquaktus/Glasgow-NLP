{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchtext\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exists\n",
      "--2019-10-18 13:17:39--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1382085 (1.3M) [text/plain]\n",
      "Saving to: './datasets/all.desc'\n",
      "\n",
      "./datasets/all.desc 100%[===================>]   1.32M  5.40MB/s    in 0.2s    \n",
      "\n",
      "2019-10-18 13:17:40 (5.40 MB/s) - './datasets/all.desc' saved [1382085/1382085]\n",
      "\n",
      "--2019-10-18 13:17:40--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 906732 (885K) [text/plain]\n",
      "Saving to: './datasets/all.code'\n",
      "\n",
      "./datasets/all.code 100%[===================>] 885.48K  4.35MB/s    in 0.2s    \n",
      "\n",
      "2019-10-18 13:17:41 (4.35 MB/s) - './datasets/all.code' saved [906732/906732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"./datasets\")\n",
    "except FileExistsError:\n",
    "    print(\"Directories already exists\")\n",
    "\n",
    "# getting descriptions\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno -O ./datasets/all.desc\n",
    "\n",
    "# getting code\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code -O ./datasets/all.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a token text encoder\n",
    "An encoder will take a file and a splitting function and return an object able to encode and decode a string. It will also be able to save a vocab file and retrieve from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel_to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'substitute', 'the', 'result', 'for', 'field_type', '.', ')']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (substitute the result for field_type.)\"\n",
    "\n",
    "# looks like code split need parenthesis to be matched in the same string, if not it gives an error...\n",
    "def code_split(s):\n",
    "    return [x.string for x in tokenize(BytesIO(s.encode('utf-8')).readline) if x.string != '' and x.string != \"\\n\" and not x.string.isspace()][1:]\n",
    "\n",
    "print(code_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel', '_', 'to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'subs', '_', '_', 'titute', 'the', 'result', \"'\", 'for', 'field', '_', 'type', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (subs__titute the result' for field_type.\"\n",
    "\n",
    "def string_split(s):\n",
    "    return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(_|\\W)', s))) # this will chunk all code properly by plits strings with quotes\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|_|\\W)', s))) # this keeps the strings intact\n",
    "\n",
    "print(string_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, vocab_size=sys.maxsize, tokenizer=str.split, vocab=[]):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.t2id = {}\n",
    "        vocab.append(\"<UNK>\")\n",
    "        for tok in vocab:\n",
    "            self.t2id[tok] = len(self.t2id)\n",
    "        self.id2t = {v:k for k,v in self.t2id.items()}\n",
    "    \n",
    "    def encode(self,s):\n",
    "        ids = []\n",
    "        for tok in self.tokenizer(s):\n",
    "            if tok in self.t2id:\n",
    "                ids.append(self.t2id[tok])\n",
    "            else:\n",
    "                ids.append(self.t2id[\"<UNK>\"])\n",
    "        return ids\n",
    "        \n",
    "    def decode(self,arr):\n",
    "        return [self.id2t[id] for id in arr]\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.id2t)\n",
    "        \n",
    "    def build_vocab_from_corpus(self,fp):\n",
    "        from collections import Counter\n",
    "        all_toks = []\n",
    "        with open(fp, \"r\") as corpus_file:\n",
    "            for line in corpus_file.readlines():\n",
    "                for tok in self.tokenizer(line):\n",
    "                    all_toks.append(tok)\n",
    "        counter = Counter(all_toks)\n",
    "        unique_toks = [x for _,x in sorted(zip(counter.values(),counter.keys()))][::-1][:self.vocab_size]\n",
    "        for tok in unique_toks:\n",
    "            self.t2id[tok] = len(self.t2id)\n",
    "        self.id2t = {v:k for k,v in self.t2id.items()}\n",
    "        \n",
    "                \n",
    "        \n",
    "    def save_vocab(self,fp):\n",
    "        with open(fp, \"w\") as vocab_file:\n",
    "            for i in range(len(self.id2t)):\n",
    "                vocab_file.write(self.id2t[i]+\"\\n\")\n",
    "            \n",
    "        \n",
    "    def load_vocab(self,fp):\n",
    "        self.t2id = {}\n",
    "        with open(fp, \"r\") as vocab_file:\n",
    "            for line in vocab_file.readlines():\n",
    "                tok = line[:-1]\n",
    "                self.t2id[tok] = len(self.t2id)\n",
    "        self.id2t = {v:k for k,v in self.t2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(vocab=[\"<START>\", \"<END>\", \"<PAD>\"], tokenizer=string_split)\n",
    "tokenizer_code = Tokenizer(vocab=[\"<START>\", \"<END>\", \"<PAD>\"], tokenizer=string_split) # turns out this works well for code too\n",
    "\n",
    "tokenizer_en.build_vocab_from_corpus(\"./datasets/all.desc\")\n",
    "tokenizer_code.build_vocab_from_corpus(\"./datasets/all.code\")\n",
    "\n",
    "tokenizer_en.save_vocab(\"./desc_vocab.txt\")\n",
    "tokenizer_code.save_vocab(\"./code_vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[315, 12, 3, 9, 509, 894, 2183]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.encode(\"make an array with 10 random numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'te', '_', '<UNK>']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode([214, 11, 34, 1864, 5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
