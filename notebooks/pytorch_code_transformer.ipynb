{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 28 17:36:33 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.40       Driver Version: 430.40       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:61:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    12W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exists\n",
      "--2019-10-28 17:36:34--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1382085 (1.3M) [text/plain]\n",
      "Saving to: './datasets/all.desc'\n",
      "\n",
      "./datasets/all.desc 100%[===================>]   1.32M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2019-10-28 17:36:35 (17.9 MB/s) - './datasets/all.desc' saved [1382085/1382085]\n",
      "\n",
      "--2019-10-28 17:36:35--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 906732 (885K) [text/plain]\n",
      "Saving to: './datasets/all.code'\n",
      "\n",
      "./datasets/all.code 100%[===================>] 885.48K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2019-10-28 17:36:35 (13.5 MB/s) - './datasets/all.code' saved [906732/906732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"./datasets\")\n",
    "except FileExistsError:\n",
    "    print(\"Directories already exists\")\n",
    "\n",
    "# getting descriptions\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno -O ./datasets/all.desc\n",
    "\n",
    "# getting code\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code -O ./datasets/all.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a token text encoder\n",
    "An encoder will take a file and a splitting function and return an object able to encode and decode a string. It will also be able to save a vocab file and retrieve from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel_to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'substitute', 'the', 'result', 'for', 'field_type', '.', ')']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (substitute the result for field_type.)\"\n",
    "\n",
    "# looks like code split need parenthesis to be matched in the same string, if not it gives an error...\n",
    "def code_split(s):\n",
    "    return [x.string for x in tokenize(BytesIO(s.encode('utf-8')).readline) if x.string != '' and x.string != \"\\n\" and not x.string.isspace()][1:]\n",
    "\n",
    "print(code_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel', '_', 'to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'subs', '_', '_', 'titute', 'the', 'result', \"'\", 'for', 'field', '_', 'type', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (subs__titute the result' for field_type.\"\n",
    "\n",
    "def string_split(s):\n",
    "    return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(_|\\W)', s))) # this will chunk all code properly by plits strings with quotes\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|_|\\W)', s))) # this keeps the strings intact\n",
    "\n",
    "print(string_split(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_array(src_fp, tgt_fp):\n",
    "    lines = []\n",
    "    with open(src_fp, \"r\") as src_file, open(tgt_fp, \"r\") as tgt_file:\n",
    "        for src, tgt in zip(src_file, tgt_file):\n",
    "            lines.append((src, tgt))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(data, max_seq_length=200, tokenizer=string_split):\n",
    "    return [(src, tgt) for src, tgt in data if len(string_split(src)) <= max_seq_length and len(string_split(tgt)) <= max_seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_dataset(samples, src_field, tgt_field):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples: [(src_string),(tgt_string)]\n",
    "        src/tgt_tokenizer: a func that takes a string and returns an array of strings\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        src_string, tgt_string = sample\n",
    "        examples.append(torchtext.data.Example.fromdict({\"src\":src_string, \"tgt\":tgt_string}, \n",
    "                                        fields={\"src\":(\"src\",src_field), \"tgt\":(\"tgt\",tgt_field)}))\n",
    "        \n",
    "    dataset = torchtext.data.Dataset(examples,fields={\"src\":src_field, \"tgt\":tgt_field})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corpus_to_array(\"datasets/all.desc\", \"datasets/all.code\")\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src length: 586\n",
      "Max tgt length: 1087\n"
     ]
    }
   ],
   "source": [
    "print(\"Max src length:\", max([len(string_split(src)) for src, tgt in data]))\n",
    "print(\"Max tgt length:\", max([len(string_split(tgt)) for src, tgt in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 18805\n",
      "Limited dataset size: 18781\n"
     ]
    }
   ],
   "source": [
    "print(\"Full dataset size:\", len(data))\n",
    "max_seq_length=200\n",
    "data = filter_corpus(data, max_seq_length=200, tokenizer=string_split)\n",
    "print(\"Limited dataset size:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "TGT_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "\n",
    "dataset = samples_to_dataset(data, SRC_TEXT, TGT_TEXT)\n",
    "\n",
    "train_dataset, val_dataset = dataset.split([0.9,0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging dataset\n",
    "This will be a small dataset of only 3 or 4 sentences to ensure the model can overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_data = [\n",
    "    (\"my favourite foods are banana and toast\",\"would you like banana and toast ?\"),\n",
    "    (\"my favourite foods are eggs and bacon and beans\",\"would you like eggs and bacon and beans ?\"),\n",
    "    (\"my favourite food is chocolate\",\"would you like chocolate ?\"),\n",
    "    (\"my favourite food is avocado\",\"would you like avocado ?\")\n",
    "]\n",
    "\n",
    "other_data = [\n",
    "    (\"what age is she ?\", \"she is 8 years old\"),\n",
    "    (\"what age is he ?\", \"he is 4 years old\"),\n",
    "    (\"how old are you ?\", \"i am 22 years old\"),\n",
    "    (\"how old am i ?\", \"you are 28 years old\")\n",
    "]\n",
    "\n",
    "SRC_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "TGT_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "\n",
    "train_dataset = val_dataset = samples_to_dataset(other_data, SRC_TEXT, TGT_TEXT)\n",
    "\n",
    "# train_dataset, val_dataset = dataset.split([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if -> 15\n",
      "value -> 25\n",
      "is -> 11\n",
      "an -> 12\n",
      "instance -> 60\n",
      "of -> 18\n",
      "Field -> 594\n",
      "class -> 31\n",
      ", -> 6\n"
     ]
    }
   ],
   "source": [
    "SRC_TEXT.build_vocab(train_dataset)\n",
    "TGT_TEXT.build_vocab(train_dataset)\n",
    "\n",
    "\n",
    "sample = dataset[2].src\n",
    "for tok, id in zip(sample, SRC_TEXT.numericalize([sample])):\n",
    "    print(\"{} -> {}\".format(tok, id.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'if', 'not', ',', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[ 2 15 38  6  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n",
      "[ 2 34 11  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_dataset, val_dataset),\n",
    "    batch_size = batch_size,\n",
    "#     repeat=True,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "# The iterator generates batches with padded length for sequences with similar sizes, a batch is [seq_length, batch_size]\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    idx = 3\n",
    "    print([SRC_TEXT.vocab.itos[id] for id in batch.src.cpu().numpy()[:,idx]])\n",
    "    print(batch.src.cpu().numpy()[:,idx])\n",
    "    print(batch.tgt.cpu().numpy()[:,idx])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample transformer without positional encoding, it uses the built in transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_transformer_model = nn.Transformer() # uses default hyperparameters\n",
    "src = torch.rand((10, 32, 512)) # [src_seq_length, batch_size, embedding_size]\n",
    "tgt = torch.rand((20, 32, 512)) # [tgt_seq_length, batch_size, embedding_size]\n",
    "rand_transformer_model(src, tgt).shape # [tgt_seq_length, batch_size, embedding_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_size=512, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, dropout)\n",
    "        self.src_encoder = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.tgt_encoder = nn.Embedding(tgt_vocab_size, embedding_size)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model=embedding_size, nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024)\n",
    "        self.decoder = nn.Linear(embedding_size, tgt_vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.src_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        self.tgt_mask = self._generate_square_subsequent_mask(len(tgt)).to(device)\n",
    "\n",
    "        src = self.src_encoder(src) * math.sqrt(self.embedding_size)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        tgt = self.tgt_encoder(tgt) * math.sqrt(self.embedding_size)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        output = self.transformer(src, tgt, tgt_mask=self.tgt_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [-inf, 0., 0., 0.],\n",
       "        [-inf, -inf, 0., 0.],\n",
       "        [-inf, -inf, -inf, 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._generate_square_subsequent_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(SRC_TEXT.vocab.stoi)\n",
    "tgt_vocab_size = len(TGT_TEXT.vocab.stoi)\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, dropout=0.2).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 8, 4173])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.randint(0, src_vocab_size, (10,8), device=device) # [src_seq_length, batch_size]\n",
    "tgt = torch.randint(0, tgt_vocab_size, (33,8), device=device) # [src_seq_length, batch_size]\n",
    "model(src, tgt).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TGT_TEXT.vocab.stoi['<pad>'])\n",
    "lr = 0.005 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    tgt_vocab_size = len(TGT_TEXT.vocab.stoi)\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        encoder_input = batch.src\n",
    "        decoder_input = batch.tgt[:-1]\n",
    "        targets = batch.tgt[1:]\n",
    "#         print(\"Decoder input:\", decoder_input)\n",
    "#         print(\"Decoder targets:\", targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(encoder_input, decoder_input)\n",
    "#         print(\"Predicted:\", output)\n",
    "#         print(\"Predicted ids\", output.argmax(dim=-1))\n",
    "#         print(targets.shape)\n",
    "        loss = criterion(output.view(-1, tgt_vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, i, len(train_iterator), scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(2, 0))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[],[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  529 batches | lr 0.01 | ms/batch 50.14 | loss  4.00 | ppl    54.53\n",
      "| epoch   1 |   400/  529 batches | lr 0.01 | ms/batch 47.97 | loss  3.95 | ppl    51.84\n",
      "| epoch   1 |   600/  529 batches | lr 0.01 | ms/batch 47.43 | loss  3.90 | ppl    49.48\n",
      "| epoch   1 |   800/  529 batches | lr 0.01 | ms/batch 49.07 | loss  3.88 | ppl    48.61\n",
      "| epoch   1 |  1000/  529 batches | lr 0.01 | ms/batch 50.39 | loss  3.83 | ppl    46.04\n",
      "| epoch   1 |  1200/  529 batches | lr 0.01 | ms/batch 50.34 | loss  3.84 | ppl    46.74\n",
      "| epoch   1 |  1400/  529 batches | lr 0.01 | ms/batch 49.55 | loss  3.82 | ppl    45.63\n",
      "| epoch   1 |  1600/  529 batches | lr 0.01 | ms/batch 47.97 | loss  3.78 | ppl    43.75\n",
      "| epoch   1 |  1800/  529 batches | lr 0.01 | ms/batch 49.44 | loss  3.76 | ppl    42.85\n",
      "| epoch   1 |  2000/  529 batches | lr 0.01 | ms/batch 48.04 | loss  3.71 | ppl    41.05\n",
      "| epoch   1 |  2200/  529 batches | lr 0.01 | ms/batch 49.76 | loss  3.69 | ppl    40.18\n",
      "| epoch   1 |  2400/  529 batches | lr 0.01 | ms/batch 49.58 | loss  3.68 | ppl    39.77\n",
      "| epoch   1 |  2600/  529 batches | lr 0.01 | ms/batch 49.62 | loss  3.69 | ppl    39.86\n",
      "| epoch   1 |  2800/  529 batches | lr 0.01 | ms/batch 48.34 | loss  3.61 | ppl    37.13\n",
      "| epoch   1 |  3000/  529 batches | lr 0.01 | ms/batch 50.03 | loss  3.61 | ppl    36.82\n",
      "| epoch   1 |  3200/  529 batches | lr 0.01 | ms/batch 49.15 | loss  3.61 | ppl    37.05\n",
      "| epoch   1 |  3400/  529 batches | lr 0.01 | ms/batch 51.34 | loss  3.55 | ppl    34.78\n",
      "| epoch   1 |  3600/  529 batches | lr 0.01 | ms/batch 51.72 | loss  3.55 | ppl    34.81\n",
      "| epoch   1 |  3800/  529 batches | lr 0.01 | ms/batch 49.57 | loss  3.54 | ppl    34.55\n",
      "| epoch   1 |  4000/  529 batches | lr 0.01 | ms/batch 49.56 | loss  3.51 | ppl    33.59\n",
      "| epoch   1 |  4200/  529 batches | lr 0.01 | ms/batch 49.54 | loss  3.48 | ppl    32.33\n",
      "| epoch   1 |  4400/  529 batches | lr 0.01 | ms/batch 49.63 | loss  3.46 | ppl    31.90\n",
      "| epoch   1 |  4600/  529 batches | lr 0.01 | ms/batch 50.24 | loss  3.46 | ppl    31.72\n",
      "| epoch   1 |  4800/  529 batches | lr 0.01 | ms/batch 48.26 | loss  3.41 | ppl    30.36\n",
      "| epoch   1 |  5000/  529 batches | lr 0.01 | ms/batch 49.40 | loss  3.41 | ppl    30.27\n",
      "| epoch   1 |  5200/  529 batches | lr 0.01 | ms/batch 49.83 | loss  3.39 | ppl    29.80\n",
      "| epoch   1 |  5400/  529 batches | lr 0.01 | ms/batch 49.14 | loss  3.38 | ppl    29.40\n",
      "| epoch   1 |  5600/  529 batches | lr 0.01 | ms/batch 49.84 | loss  3.34 | ppl    28.17\n",
      "| epoch   1 |  5800/  529 batches | lr 0.01 | ms/batch 49.78 | loss  3.35 | ppl    28.64\n",
      "| epoch   1 |  6000/  529 batches | lr 0.01 | ms/batch 48.03 | loss  3.32 | ppl    27.74\n",
      "| epoch   1 |  6200/  529 batches | lr 0.01 | ms/batch 53.65 | loss  3.32 | ppl    27.73\n",
      "| epoch   1 |  6400/  529 batches | lr 0.01 | ms/batch 52.12 | loss  3.29 | ppl    26.93\n",
      "| epoch   1 |  6600/  529 batches | lr 0.01 | ms/batch 50.61 | loss  3.24 | ppl    25.59\n",
      "| epoch   1 |  6800/  529 batches | lr 0.01 | ms/batch 50.99 | loss  3.28 | ppl    26.59\n",
      "| epoch   1 |  7000/  529 batches | lr 0.01 | ms/batch 51.66 | loss  3.25 | ppl    25.73\n",
      "| epoch   1 |  7200/  529 batches | lr 0.01 | ms/batch 49.33 | loss  3.23 | ppl    25.36\n",
      "| epoch   1 |  7400/  529 batches | lr 0.01 | ms/batch 50.23 | loss  3.24 | ppl    25.65\n",
      "| epoch   1 |  7600/  529 batches | lr 0.01 | ms/batch 48.53 | loss  3.18 | ppl    24.10\n",
      "| epoch   1 |  7800/  529 batches | lr 0.01 | ms/batch 47.84 | loss  3.20 | ppl    24.42\n",
      "| epoch   1 |  8000/  529 batches | lr 0.01 | ms/batch 50.54 | loss  3.17 | ppl    23.91\n",
      "| epoch   1 |  8200/  529 batches | lr 0.01 | ms/batch 51.89 | loss  3.18 | ppl    24.02\n",
      "| epoch   1 |  8400/  529 batches | lr 0.01 | ms/batch 49.28 | loss  3.15 | ppl    23.26\n",
      "| epoch   1 |  8600/  529 batches | lr 0.01 | ms/batch 48.02 | loss  3.12 | ppl    22.69\n",
      "| epoch   1 |  8800/  529 batches | lr 0.01 | ms/batch 50.27 | loss  3.14 | ppl    23.03\n",
      "| epoch   1 |  9000/  529 batches | lr 0.01 | ms/batch 48.82 | loss  3.10 | ppl    22.24\n",
      "| epoch   1 |  9200/  529 batches | lr 0.01 | ms/batch 49.87 | loss  3.10 | ppl    22.29\n",
      "| epoch   1 |  9400/  529 batches | lr 0.01 | ms/batch 48.69 | loss  3.09 | ppl    21.90\n",
      "| epoch   1 |  9600/  529 batches | lr 0.01 | ms/batch 47.90 | loss  3.03 | ppl    20.76\n",
      "| epoch   1 |  9800/  529 batches | lr 0.01 | ms/batch 49.60 | loss  3.07 | ppl    21.52\n",
      "| epoch   1 | 10000/  529 batches | lr 0.01 | ms/batch 50.83 | loss  3.06 | ppl    21.42\n",
      "| epoch   1 | 10200/  529 batches | lr 0.01 | ms/batch 48.27 | loss  3.00 | ppl    20.11\n",
      "| epoch   1 | 10400/  529 batches | lr 0.01 | ms/batch 49.91 | loss  3.03 | ppl    20.71\n",
      "| epoch   1 | 10600/  529 batches | lr 0.01 | ms/batch 49.45 | loss  3.02 | ppl    20.53\n",
      "| epoch   1 | 10800/  529 batches | lr 0.01 | ms/batch 49.13 | loss  3.01 | ppl    20.21\n",
      "| epoch   1 | 11000/  529 batches | lr 0.01 | ms/batch 49.95 | loss  2.97 | ppl    19.52\n",
      "| epoch   1 | 11200/  529 batches | lr 0.01 | ms/batch 49.81 | loss  2.98 | ppl    19.61\n",
      "| epoch   1 | 11400/  529 batches | lr 0.01 | ms/batch 48.15 | loss  2.95 | ppl    19.02\n",
      "| epoch   1 | 11600/  529 batches | lr 0.01 | ms/batch 50.17 | loss  2.98 | ppl    19.77\n",
      "| epoch   1 | 11800/  529 batches | lr 0.01 | ms/batch 49.45 | loss  2.95 | ppl    19.18\n",
      "| epoch   1 | 12000/  529 batches | lr 0.01 | ms/batch 47.71 | loss  2.92 | ppl    18.53\n",
      "| epoch   1 | 12200/  529 batches | lr 0.01 | ms/batch 49.93 | loss  2.92 | ppl    18.48\n",
      "| epoch   1 | 12400/  529 batches | lr 0.01 | ms/batch 49.25 | loss  2.90 | ppl    18.26\n",
      "| epoch   1 | 12600/  529 batches | lr 0.01 | ms/batch 48.36 | loss  2.88 | ppl    17.88\n",
      "| epoch   1 | 12800/  529 batches | lr 0.01 | ms/batch 50.08 | loss  2.91 | ppl    18.35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-489aa0dba4cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"######## Finished Epoch {} #########\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     val_loss = evaluate(model, val_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-b1a1ea612958>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         print(targets.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 20 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    print(\"######## Finished Epoch {} #########\".format(epoch))\n",
    "#     val_loss = evaluate(model, val_data)\n",
    "#     print('-' * 89)\n",
    "#     print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "#           'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "#                                      val_loss, math.exp(val_loss)))\n",
    "#     print('-' * 89)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC ids shape: tensor([[ 2],\n",
      "        [87],\n",
      "        [ 6],\n",
      "        [ 3]], device='cuda:0')\n",
      "last pred: if 14\n",
      "last pred: > 58\n",
      "last pred: < 67\n",
      "last pred: < 67\n",
      "last pred: < 67\n",
      "last pred: < 67\n",
      "last pred: < 67\n",
      "last pred: < 67\n",
      "last pred: < 67\n",
      "last pred: < 67\n"
     ]
    }
   ],
   "source": [
    "def translate(s):\n",
    "    src_ids = SRC_TEXT.numericalize([[\"<sos>\"] + SRC_TEXT.preprocess(s) + [\"<eos>\"]], device=device)\n",
    "    print(\"SRC ids shape:\",src_ids)\n",
    "    with torch.no_grad():\n",
    "        sos_id = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "        decoder_input = torch.zeros((1, 1), dtype=torch.long, device=device).fill_(sos_id)\n",
    "#         decoder_input = torch.tensor(np.array([[ 2],[13],[6],[9],[5],[4]]), device=device)\n",
    "#         print(\"Decoder input shape:\", decoder_input.shape)\n",
    "        \n",
    "        for i in range(10):\n",
    "#             print(\"Decoder input\", decoder_input)\n",
    "            output = model(src_ids, decoder_input)\n",
    "#             print(\"output shape:\", output)\n",
    "#             print(\"predicted ids:\", output.argmax(dim=-1))\n",
    "            last_pred = output[-1:].argmax(dim=2)\n",
    "#             decoder_input[i+1][0] = last_pred\n",
    "            print(\"last pred:\", TGT_TEXT.vocab.itos[last_pred.cpu().numpy()[0][0]], last_pred.cpu().numpy()[0][0])\n",
    "            \n",
    "            decoder_input = torch.cat((decoder_input, last_pred))\n",
    "#             print(\"Decoder input\", decoder_input)\n",
    "\n",
    "translate(\"try,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2740311596835683"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nltk_bleu(refrence, prediction):\n",
    "    \"\"\"\n",
    "    Implementation from ReCode\n",
    "    and moses multi belu script sets BLEU to 0.0 if len(toks) < 4\n",
    "    \"\"\"\n",
    "    ngram_weights = [0.25] * min(4, len(refrence))\n",
    "    return sentence_bleu([refrence], prediction, weights=ngram_weights, \n",
    "                          smoothing_function=SmoothingFunction().method3)\n",
    "\n",
    "nltk_bleu([1,2,3,4,5,6], [1,2,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, valid_iterator):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    tgt_vocab_size = len(TGT_TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        results = []\n",
    "        for j, batch in enumerate(valid_iterator):\n",
    "            encoder_input = batch.src\n",
    "            sos_id = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "            decoder_input = torch.zeros((1, batch.tgt.shape[1]), dtype=torch.long, device=device).fill_(sos_id)\n",
    "            targets = batch.tgt[1:]\n",
    "            \n",
    "#             for i in range(max_seq_length):\n",
    "            for i in range(10):\n",
    "                output = eval_model(encoder_input, decoder_input)\n",
    "                last_pred = output[-1:].argmax(dim=2)\n",
    "                # top k sampling\n",
    "                # indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "                # F.softmax()\n",
    "                decoder_input = torch.cat((decoder_input, last_pred))\n",
    "            \n",
    "            if ((j+1) % 40 == 0):\n",
    "                print(\"Done {}/{}\".format(j, len(valid_iterator)))\n",
    "            \n",
    "            for i in range(decoder_input.shape[1]):\n",
    "                ref_ids = [id for id in batch.tgt.transpose(0,1)[i] if id not in []]\n",
    "                pred_ids = [id for id in decoder_input.transpose(0,1)[i] if id not in []]\n",
    "                src_ids = [id for id in batch.src.transpose(0,1)[i] if id not in []]\n",
    "                \n",
    "                src_sent = \" \".join([SRC_TEXT.vocab.itos[id] for id in src_ids])\n",
    "                predicted_sent = \" \".join([TGT_TEXT.vocab.itos[id] for id in pred_ids])\n",
    "                ref_sent = \" \".join([TGT_TEXT.vocab.itos[id] for id in ref_ids])\n",
    "                                \n",
    "                result = {\n",
    "                            \"ref_ids\": ref_ids,\n",
    "                            \"pred_ids\": pred_ids,\n",
    "                            \"BLEU\": nltk_bleu(ref_ids, pred_ids),\n",
    "                    \n",
    "                            \"src_sent\": src_sent, \n",
    "                            \"ref_sent\": ref_sent, \n",
    "                            \"pred_sent\": predicted_sent\n",
    "                         }\n",
    "                results.append(result)\n",
    "        \n",
    "        overall_BLEU = np.mean([r[\"BLEU\"] for r in results])\n",
    "        return results, overall_BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moses Multi-BLEU perl script returns 0.0 for any sentence less than 4 tokens long.\n",
    "It will be best to use a function by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_moses_multi_bleu([\"this is a test\"], [\"this is a for\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<sos>',\n",
       " '<eos>',\n",
       " 'old',\n",
       " 'years',\n",
       " 'is',\n",
       " '22',\n",
       " '28',\n",
       " '4',\n",
       " '8',\n",
       " 'am',\n",
       " 'are',\n",
       " 'he',\n",
       " 'i',\n",
       " 'she',\n",
       " 'you']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TGT_TEXT.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 39/59\n"
     ]
    }
   ],
   "source": [
    "results, BLEU = evaluate(model, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query\t\t: <sos> try , <eos> <pad>\n",
      "True Translation: <sos> try : <eos>\n",
      "Prediction\t: <sos> if > < < < < < < < <\n",
      "\n",
      "\n",
      "Query\t\t: <sos> try , <eos> <pad>\n",
      "True Translation: <sos> try : <eos>\n",
      "Prediction\t: <sos> if > < < < < < < < <\n",
      "\n",
      "\n",
      "Query\t\t: <sos> do nothing . <eos>\n",
      "True Translation: <sos> pass <eos> <pad>\n",
      "Prediction\t: <sos> 1 1 1 1 1 1 1 1 1 1\n",
      "\n",
      "\n",
      "Query\t\t: <sos> if not , <eos> <pad>\n",
      "True Translation: <sos> else : <eos>\n",
      "Prediction\t: <sos> if not not not not not not not not not\n",
      "\n",
      "\n",
      "Overall BLEU: 0.0\n"
     ]
    }
   ],
   "source": [
    "for r in results[:100]:\n",
    "    print(\"Query\\t\\t: {}\\nTrue Translation: {}\\nPrediction\\t: {}\\n\\n\".format(r[\"src_sent\"], r[\"ref_sent\"], r[\"pred_sent\"]))\n",
    "\n",
    "print(\"Overall BLEU:\",BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
