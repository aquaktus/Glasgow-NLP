{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import operator\n",
    "\n",
    "from base_transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.set_device(0) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 13 12:00:12 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.26       Driver Version: 430.26       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 41%   36C    P8     5W / 280W |     10MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exists\n",
      "--2019-11-13 12:00:12--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1382085 (1.3M) [text/plain]\n",
      "Saving to: './datasets/all.desc'\n",
      "\n",
      "./datasets/all.desc 100%[===================>]   1.32M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2019-11-13 12:00:12 (15.4 MB/s) - './datasets/all.desc' saved [1382085/1382085]\n",
      "\n",
      "--2019-11-13 12:00:13--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 906732 (885K) [text/plain]\n",
      "Saving to: './datasets/all.code'\n",
      "\n",
      "./datasets/all.code 100%[===================>] 885.48K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2019-11-13 12:00:13 (11.6 MB/s) - './datasets/all.code' saved [906732/906732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"./datasets\")\n",
    "except FileExistsError:\n",
    "    print(\"Directories already exists\")\n",
    "\n",
    "# getting descriptions\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno -O ./datasets/all.desc\n",
    "\n",
    "# getting code\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code -O ./datasets/all.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a token text encoder\n",
    "An encoder will take a file and a splitting function and return an object able to encode and decode a string. It will also be able to save a vocab file and retrieve from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel_to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'substitute', 'the', 'result', 'for', 'field_type', '.', ')']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (substitute the result for field_type.)\"\n",
    "\n",
    "# looks like code split need parenthesis to be matched in the same string, if not it gives an error...\n",
    "def code_split(s):\n",
    "    return [x.string for x in tokenize(BytesIO(s.encode('utf-8')).readline) if x.string != '' and x.string != \"\\n\" and not x.string.isspace()][1:]\n",
    "\n",
    "print(code_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel', '_', 'to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'subs', '_', '_', 'titute', 'the', 'result', \"'\", 'for', 'field', '_', 'type', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (subs__titute the result' for field_type.\"\n",
    "\n",
    "def string_split(s):\n",
    "    return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(_|\\W)', s))) # this will chunk all code properly by plits strings with quotes\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|_|\\W)', s))) # this keeps the strings intact\n",
    "\n",
    "print(string_split(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_array(src_fp, tgt_fp):\n",
    "    lines = []\n",
    "    with open(src_fp, \"r\") as src_file, open(tgt_fp, \"r\") as tgt_file:\n",
    "        for src, tgt in zip(src_file, tgt_file):\n",
    "            lines.append((src, tgt))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(data, max_seq_length=200, tokenizer=string_split):\n",
    "    return [(src, tgt) for src, tgt in data if len(string_split(src)) <= max_seq_length and len(string_split(tgt)) <= max_seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_dataset(samples, src_field, tgt_field):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples: [(src_string),(tgt_string)]\n",
    "        src/tgt_tokenizer: a func that takes a string and returns an array of strings\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        src_string, tgt_string = sample\n",
    "        examples.append(torchtext.data.Example.fromdict({\"src\":src_string, \"tgt\":tgt_string}, \n",
    "                                        fields={\"src\":(\"src\",src_field), \"tgt\":(\"tgt\",tgt_field)}))\n",
    "        \n",
    "    dataset = torchtext.data.Dataset(examples,fields={\"src\":src_field, \"tgt\":tgt_field})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corpus_to_array(\"datasets/all.desc\", \"datasets/all.code\")\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src length: 586\n",
      "Max tgt length: 1087\n"
     ]
    }
   ],
   "source": [
    "print(\"Max src length:\", max([len(string_split(src)) for src, tgt in data]))\n",
    "print(\"Max tgt length:\", max([len(string_split(tgt)) for src, tgt in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 18805\n",
      "Limited dataset size: 18781\n"
     ]
    }
   ],
   "source": [
    "print(\"Full dataset size:\", len(data))\n",
    "max_seq_length=200\n",
    "data = filter_corpus(data, max_seq_length=200, tokenizer=string_split)\n",
    "print(\"Limited dataset size:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    SRC_TEXT = torch.load(\"./src_vocab.vcb\")\n",
    "except:\n",
    "    SRC_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "\n",
    "try:\n",
    "    TGT_TEXT = torch.load(\"./tgt_vocab.vcb\")\n",
    "except:\n",
    "    TGT_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "\n",
    "dataset = samples_to_dataset(data, SRC_TEXT, TGT_TEXT)\n",
    "\n",
    "train_dataset, val_dataset = dataset.split([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call -> 19\n",
      "the -> 7\n",
      "method -> 16\n",
      "operator -> 764\n",
      ". -> 4\n",
      "attrgetter -> 1799\n",
      "with -> 9\n",
      "an -> 12\n",
      "argument -> 20\n",
      "_ -> 5\n",
      "func -> 142\n",
      "_ -> 5\n",
      "code -> 141\n",
      ", -> 6\n",
      "substitute -> 21\n",
      "the -> 7\n",
      "result -> 22\n",
      "for -> 13\n",
      "get -> 50\n",
      "_ -> 5\n",
      "method -> 16\n",
      "_ -> 5\n",
      "code -> 141\n",
      ". -> 4\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(SRC_TEXT, \"vocab\"):\n",
    "    print(\"creating src vocab\")\n",
    "    SRC_TEXT.build_vocab(train_dataset)\n",
    "if not hasattr(TGT_TEXT, \"vocab\"):\n",
    "    print(\"creating tgt vocab\")\n",
    "    TGT_TEXT.build_vocab(train_dataset)\n",
    "\n",
    "\n",
    "sample = dataset[2].src\n",
    "for tok, id in zip(sample, SRC_TEXT.numericalize([sample])):\n",
    "    print(\"{} -> {}\".format(tok, id.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset iterator\n",
    "This will create a finction returning a different batch. The `train_iterator` is infinitely repeating. while the validation one is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'if', 'format', 'is', 'not', 'contained', 'in', '_', 'serializers', ',', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[  2  15 108  11  39 106  38   5 416   6   3   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1]\n",
      "[  2  14  81  29  24   4 326  11   3   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_iterator = BucketIterator(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    repeat=True,\n",
    "#     shuffle=True,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "valid_iterator = BucketIterator(val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "# The iterator generates batches with padded length for sequences with similar sizes, a batch is [seq_length, batch_size]\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    idx = 0\n",
    "    print([SRC_TEXT.vocab.itos[id] for id in batch.src.cpu().numpy()[:,idx]])\n",
    "    print(batch.src.cpu().numpy()[:,idx])\n",
    "    print(batch.tgt.cpu().numpy()[:,idx])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample transformer without positional encoding, it uses the built in transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_transformer_model = nn.Transformer() # uses default hyperparameters\n",
    "src = torch.rand((10, 32, 512)) # [src_seq_length, batch_size, embedding_size]\n",
    "tgt = torch.rand((20, 32, 512)) # [tgt_seq_length, batch_size, embedding_size]\n",
    "rand_transformer_model(src, tgt).shape # [tgt_seq_length, batch_size, embedding_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(SRC_TEXT.vocab.itos)\n",
    "tgt_vocab_size = len(TGT_TEXT.vocab.itos)\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, dropout=0.2).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_batch_ids(encoder_input, max_seq_length=50):\n",
    "    batch_len = encoder_input.shape[1]\n",
    "    sos_id = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "    decoder_input = torch.zeros((1, batch_len), dtype=torch.long, device=device).fill_(sos_id)\n",
    "\n",
    "    for i in range(max_seq_length):\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        last_pred = output[-1:].argmax(dim=2)\n",
    "\n",
    "        decoder_input = torch.cat((decoder_input, last_pred))\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f beam_decode beam_decode(model, batch_size=1, encoder_states=src_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.1062,  0.5920, -1.4175,  ...,  0.2437,  0.2524, -1.2225]],\n",
      "\n",
      "        [[ 2.4309,  0.8538, -1.8844,  ...,  0.6236, -0.1974, -1.6820]],\n",
      "\n",
      "        [[ 2.7053,  0.8010, -0.7450,  ...,  1.4385, -0.7941, -1.2559]],\n",
      "\n",
      "        [[ 1.7970,  0.0554, -0.6959,  ...,  0.8729,  0.0657, -1.1797]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "['def', 'self', 'raise', '(']\n"
     ]
    }
   ],
   "source": [
    "s = \"for every log in existing ,\"\n",
    "sent1 = [\"<sos>\"] + SRC_TEXT.preprocess(s) + [\"<eos>\"]\n",
    "src_ids = SRC_TEXT.numericalize([sent1], device=device)\n",
    "\n",
    "decode_ids = SRC_TEXT.numericalize([['<sos>', 'self', '.', 'name']], device=device)\n",
    "\n",
    "output = model(src_ids, decode_ids)\n",
    "print(output)\n",
    "print([TGT_TEXT.vocab.itos[f] for f in output.argmax(dim=-1).view(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2],\n",
      "        [17, 17],\n",
      "        [10, 10],\n",
      "        [12, 12],\n",
      "        [ 5,  5],\n",
      "        [ 4,  4],\n",
      "        [20, 20],\n",
      "        [ 6,  6],\n",
      "        [12, 12],\n",
      "        [ 7,  8],\n",
      "        [ 3, 25],\n",
      "        [10,  8],\n",
      "        [ 9, 25],\n",
      "        [ 7,  7],\n",
      "        [ 3, 11],\n",
      "        [ 7,  3],\n",
      "        [ 3,  8],\n",
      "        [ 3, 25],\n",
      "        [ 3,  7],\n",
      "        [ 3, 11],\n",
      "        [ 3,  3]], device='cuda:0')\n",
      "['<sos>', '<sos>', 'def', 'def', '=', '=', 'self', 'self', '.', '.', '_', '_', 'name', 'name', '(', '(', 'self', 'self', ')', ',', '<eos>', '*', '=', ',', \"'\", '*', ')', ')', '<eos>', ':', ')', '<eos>', '<eos>', ',', '<eos>', '*', '<eos>', ')', '<eos>', ':', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "output = greedy_decode_batch_ids(src_ids, max_seq_length=20)\n",
    "print(output)\n",
    "print([TGT_TEXT.vocab.itos[f] for f in output.view(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND, -8.140298843383789\n",
      "FOUND, -8.911083221435547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 4.829 s\n",
       "File: /nfs/phd_by_carlos/notebooks/beam_search.py\n",
       "Function: beam_search_decode at line 44\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    44                                           def beam_search_decode(model,TGT_TEXT, batch_encoder_ids, beam_size=3, num_out=3, max_length=10, SOS_token=1,EOS_token=2, PAD_token=3):\n",
       "    45                                               '''\n",
       "    46                                               :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
       "    47                                               :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
       "    48                                               :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
       "    49                                               :return: decoded_batch\n",
       "    50                                               '''\n",
       "    51         1        690.0    690.0      0.0      model.eval()\n",
       "    52         1         21.0     21.0      0.0      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
       "    53         1          1.0      1.0      0.0      decoded_batch = []\n",
       "    54         1          6.0      6.0      0.0      batch_size = batch_encoder_ids.shape[1]\n",
       "    55         1          2.0      2.0      0.0      batch_endnodes = [[] for i in range(batch_size)]\n",
       "    56                                           #     batch_trees = [PriorityQueue() for i in range(batch_size)]\n",
       "    57         1          2.0      2.0      0.0      batch_trees = [[] for i in range(batch_size)]\n",
       "    58                                               \n",
       "    59         1          2.0      2.0      0.0      beam_sizes = [beam_size] * batch_size\n",
       "    60                                               \n",
       "    61         1        176.0    176.0      0.0      first_decoder_id = torch.LongTensor([[SOS_token]]).to(device)\n",
       "    62                                               \n",
       "    63         3          5.0      1.7      0.0      for i, decode_tree in enumerate(batch_trees):\n",
       "    64         2         91.0     45.5      0.0          root_node = BeamSearchNode(first_decoder_id, batch_encoder_ids[:,i].view(-1,1), None, SOS_token, 0, 1,i)\n",
       "    65         2          9.0      4.5      0.0          heappush(decode_tree, (-root_node.eval(), root_node))\n",
       "    66                                           #         decode_tree.put((-root_node.eval(), root_node))\n",
       "    67                                               \n",
       "    68         1          1.0      1.0      0.0      while True:\n",
       "    69                                           #         print(\"STEP\")\n",
       "    70        20       1334.0     66.7      0.0          working_nodes = []\n",
       "    71        60        814.0     13.6      0.0          for tree, sample_endnode_list, sample_beam_size in zip(batch_trees, batch_endnodes, beam_sizes):\n",
       "    72       802        924.0      1.2      0.0              for i in range(sample_beam_size):\n",
       "    73                                           #                 if tree.empty():\n",
       "    74       764        916.0      1.2      0.0                  if len(tree) == 0:\n",
       "    75         2          2.0      1.0      0.0                      break\n",
       "    76       762      86078.0    113.0      1.8                  score, node = heappop(tree)\n",
       "    77                                           #                 score, node = tree.get()\n",
       "    78                                           #                 print(node.leng)\n",
       "    79       762      18102.0     23.8      0.4                  if node.last_id == EOS_token or node.leng >= max_length:\n",
       "    80       211        314.0      1.5      0.0                      if len(sample_endnode_list) < num_out:\n",
       "    81         2        399.0    199.5      0.0                          print(\"FOUND,\",node.logp.item())\n",
       "    82         2          4.0      2.0      0.0                          sample_endnode_list.append(node)\n",
       "    83       211        249.0      1.2      0.0                      sample_beam_size -= 1\n",
       "    84                                                           else:\n",
       "    85                                           #                     print(node.batch_num, float(node.logp), TGT_TEXT.vocab.itos[node.last_id])\n",
       "    86       551        812.0      1.5      0.0                      working_nodes.append((score, node))\n",
       "    87                                                       \n",
       "    88        40         65.0      1.6      0.0              del tree[:]\n",
       "    89                                                       # empty the tree to free up memory on the GPU\n",
       "    90                                           #             while not tree.empty():\n",
       "    91                                           #             for j in range(len(tree)):\n",
       "    92                                           #                 score, node = tree.get()\n",
       "    93                                           #                 del tree[j]\n",
       "    94                                           #                 del node.decoder_output\n",
       "    95                                           #                 del node.encoder_input\n",
       "    96                                           #                 del node.last_id\n",
       "    97                                           #                 del node\n",
       "    98                                           #                 del score\n",
       "    99                                                           \n",
       "   100                                                               \n",
       "   101        20         30.0      1.5      0.0          if working_nodes == []:\n",
       "   102         1          2.0      2.0      0.0              break\n",
       "   103                                                   \n",
       "   104                                                   # making the padded input from different batches and sequence lengths\n",
       "   105        19        321.0     16.9      0.0          rough_input = [n.decoder_output for (score, n) in working_nodes]\n",
       "   106                                           #         padding_shapes = [t.shape[0] for t in rough_input]\n",
       "   107        19        169.0      8.9      0.0          max_decoder_size = max([t.shape[0] for t in rough_input])\n",
       "   108        19        604.0     31.8      0.0          padded_decoder_input = torch.zeros((max_decoder_size,len(working_nodes)), dtype=torch.long, device=device).fill_(PAD_token)\n",
       "   109                                                   \n",
       "   110                                                   # we create a correctly sized tensor with all <pad> symbols and fill with the according tokens\n",
       "   111       570        674.0      1.2      0.0          for i in range(len(working_nodes)):\n",
       "   112       551        728.0      1.3      0.0              length = rough_input[i].shape[0]\n",
       "   113       551       7259.0     13.2      0.2              padded_decoder_input[:length,i] = rough_input[i].view(-1)\n",
       "   114                                                   \n",
       "   115        19       3529.0    185.7      0.1          encoder_input = torch.cat([n.encoder_input for (score,n) in working_nodes], dim=1)\n",
       "   116                                                   \n",
       "   117        19     174260.0   9171.6      3.6          decoder_predictions = model(encoder_input, padded_decoder_input)\n",
       "   118                                           #         print(padding_shapes)\n",
       "   119                                           #         print(decoder_predictions.shape)\n",
       "   120                                                   \n",
       "   121       570       5707.0     10.0      0.1          for (score, node), logits in zip(working_nodes, decoder_predictions.transpose(0,1)):\n",
       "   122       551       1339.0      2.4      0.0              last_token_pos = node.decoder_output.shape[0] - 1\n",
       "   123       551       1967.0      3.6      0.0              last_token_logits = logits[last_token_pos] \n",
       "   124                                           #             print(last_token_logits)\n",
       "   125       551       8397.0     15.2      0.2              last_token_log_probs = last_token_logits.log_softmax(0)\n",
       "   126       551      33658.0     61.1      0.7              log_probs, indexes = torch.topk(last_token_log_probs, beam_sizes[node.batch_num])\n",
       "   127     11571      89403.0      7.7      1.9              for log_prob, idx in zip(log_probs, indexes):\n",
       "   128                                           #                 print(TGT_TEXT.vocab.itos[idx])\n",
       "   129     11020     262777.0     23.8      5.4                  new_decoder_output = torch.cat([node.decoder_output, idx.view(-1,1)])\n",
       "   130                                           #                 print([TGT_TEXT.vocab.itos[f] for f in new_decoder_output.view(-1)], -(node.logp+log_prob).item())\n",
       "   131                                           #                 print(log_prob)\n",
       "   132                                           #                 print(TGT_TEXT.vocab.itos[idx])\n",
       "   133     11020     167937.0     15.2      3.5                  new_node = BeamSearchNode(new_decoder_output, node.encoder_input, node, idx, node.logp+log_prob, node.leng+1,node.batch_num)\n",
       "   134                                           #                 batch_trees[node.batch_num].put((-(node.logp+log_prob),new_node))\n",
       "   135     11020     703525.0     63.8     14.6                  heappush(batch_trees[node.batch_num], (-(node.logp+log_prob),new_node))\n",
       "   136       551    3255613.0   5908.6     67.4              batch_trees[node.batch_num] = nsmallest(beam_sizes[node.batch_num],batch_trees[node.batch_num])\n",
       "   137                                                   \n",
       "   138                                                  \n",
       "   139         3          4.0      1.3      0.0      for i in range(len(batch_endnodes)):\n",
       "   140         2         80.0     40.0      0.0          batch_endnodes[i] = [n.decoder_output for n in sorted(batch_endnodes[i], key=lambda node: -node.logp)]\n",
       "   141         1          1.0      1.0      0.0      return batch_endnodes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f beam_search.beam_search_decode beam_search.beam_search_decode(model,TGT_TEXT, batch_encoder_ids=src_ids, SOS_token=SOS_token, EOS_token=EOS_token, PAD_token=PAD_token, beam_size=20, max_length=20, num_out=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([[ 2],\n",
      "        [50],\n",
      "        [11],\n",
      "        [ 3]], device='cuda:0')]]\n",
      "[[2, 50, 11, 3]]\n",
      "['<sos>', 'try', ':', '<eos>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import beam_search\n",
    "import importlib\n",
    "importlib.reload(beam_search)\n",
    "\n",
    "sent1 = [\"<sos>\"] + SRC_TEXT.preprocess(\"try,\") + [\"<eos>\"] + [\"<pad>\"]\n",
    "sent2 = [\"<sos>\"] + SRC_TEXT.preprocess(\"for every log in existing ,\") + [\"<eos>\"]\n",
    "src_ids = SRC_TEXT.numericalize([sent1], device=device)\n",
    "\n",
    "SOS_token = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "EOS_token = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "PAD_token = TGT_TEXT.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "outputs = beam_search.beam_search_decode(model,TGT_TEXT,\n",
    "                              batch_encoder_ids=src_ids,\n",
    "                              SOS_token=SOS_token,\n",
    "                              EOS_token=EOS_token,\n",
    "                              PAD_token=PAD_token,\n",
    "                              beam_size=4,\n",
    "                              max_length=20,\n",
    "                              num_out=1)\n",
    "\n",
    "print(outputs)\n",
    "print([t[0].view(-1).cpu().tolist() for t in outputs])\n",
    "\n",
    "for out in outputs:\n",
    "    for sent in out:\n",
    "        print([TGT_TEXT.vocab.itos[id] for id in sent.view(-1).cpu().tolist()])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2740311596835683"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nltk_bleu(refrence, prediction):\n",
    "    \"\"\"\n",
    "    Implementation from ReCode\n",
    "    and moses multi belu script sets BLEU to 0.0 if len(toks) < 4\n",
    "    \"\"\"\n",
    "    ngram_weights = [0.25] * min(4, len(refrence))\n",
    "    return sentence_bleu([refrence], prediction, weights=ngram_weights, \n",
    "                          smoothing_function=SmoothingFunction().method3)\n",
    "\n",
    "nltk_bleu(np.array([1,2,3,4,5,6]), np.array([1,2,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(beam_size=1):\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        sources = []\n",
    "        results = []\n",
    "        targets = []\n",
    "        BLEU_scores = []\n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "            encoder_inputs = batch.src\n",
    "            target = batch.tgt\n",
    "            \n",
    "            predictions = beam_search.beam_search_decode(model,TGT_TEXT,\n",
    "                              batch_encoder_ids=encoder_inputs,\n",
    "                              SOS_token=TGT_TEXT.vocab.stoi[\"<sos>\"],\n",
    "                              EOS_token=TGT_TEXT.vocab.stoi[\"<eos>\"],\n",
    "                              PAD_token=TGT_TEXT.vocab.stoi[\"<pad>\"],\n",
    "                              beam_size=beam_size,\n",
    "                              max_length=20,\n",
    "                              num_out=1)\n",
    "            results += [t[0].view(-1).cpu().tolist() for t in predictions]\n",
    "#             if beam_size == 1:\n",
    "#                 predictions = greedy_decode_batch_ids(encoder_inputs, max_seq_length=20)\n",
    "#                 results += predictions.transpose(0,1).cpu().tolist()\n",
    "#             else:\n",
    "#                 predictions = beam_decode(model, encoder_inputs)\n",
    "#                 results += [sent[0] for sent in predictions]\n",
    "            \n",
    "            sources += encoder_inputs.transpose(0,1).cpu().tolist()\n",
    "            targets += target.transpose(0,1).cpu().tolist()\n",
    "            if i % 20 == 0:\n",
    "                print(\"| EVALUATION | {:5d}/{:5d} batches |\".format(i, len(valid_iterator)))\n",
    "        \n",
    "        for r_ids, target in zip(results, targets):\n",
    "            eos_id = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "            eos_index = r_ids.index(eos_id) if eos_id in r_ids else None\n",
    "            cut_ids = r_ids[:eos_index]\n",
    "            filtered_ids = [id for id in cut_ids if id not in [0,1,2,3]]\n",
    "            filtered_target_ids = [id for id in target if id not in [0,1,2,3]]\n",
    "            BLEU_scores.append(nltk_bleu(filtered_target_ids, filtered_ids))\n",
    "        \n",
    "        with open(\"out.txt\", \"w\") as out_fp:\n",
    "            for source, result, target, BLEU in zip(sources, results, targets, BLEU_scores):\n",
    "                eos_id = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "                eos_index = result.index(eos_id) if eos_id in result else None\n",
    "                cut_ids = result[:eos_index]\n",
    "                filtered_ids = [id for id in cut_ids if id not in [0,1,2,3]]\n",
    "                filtered_target_ids = [id for id in target if id not in [0,1,2,3]]\n",
    "                filtered_source_ids = [id for id in source if id not in [0,1,2,3]]\n",
    "                \n",
    "                out_fp.write(\"SRC  :\" + \" \".join([SRC_TEXT.vocab.itos[id] for id in filtered_source_ids]) + \"\\n\")\n",
    "                out_fp.write(\"TGT  :\" + \" \".join([TGT_TEXT.vocab.itos[id] for id in filtered_target_ids]) + \"\\n\")\n",
    "                out_fp.write(\"PRED :\" + \" \".join([TGT_TEXT.vocab.itos[id] for id in filtered_ids]) + \"\\n\")\n",
    "                out_fp.write(\"BLEU :\" + str(BLEU) + \"\\n\")\n",
    "                out_fp.write(\"\\n\")\n",
    "            out_fp.write(\"\\n\\n| EVALUATION | BLEU: {:5.2f} |\\n\".format(np.average(BLEU_scores)))\n",
    "                \n",
    "        print(\"| EVALUATION | BLEU: {:5.2f} |\".format(np.average(BLEU_scores)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    tgt_vocab_size = len(TGT_TEXT.vocab.itos)\n",
    "    encoder_input = batch.src\n",
    "    decoder_input = batch.tgt[:-1]\n",
    "    targets = batch.tgt[1:]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(encoder_input, decoder_input)\n",
    "\n",
    "    loss = criterion(output.view(-1, tgt_vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    elapsed = time.time() - start_time\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TGT_TEXT.vocab.stoi['<pad>'])\n",
    "lr = 0.005 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   200/1000000 steps | lr 0.0050 | ms/batch 41.16 | loss  5.40 | ppl   222.04\n",
      "|   400/1000000 steps | lr 0.0050 | ms/batch 41.97 | loss  5.32 | ppl   203.64\n",
      "|   600/1000000 steps | lr 0.0050 | ms/batch 41.82 | loss  5.24 | ppl   188.33\n",
      "|   800/1000000 steps | lr 0.0050 | ms/batch 43.00 | loss  5.17 | ppl   175.42\n",
      "|  1000/1000000 steps | lr 0.0050 | ms/batch 41.55 | loss  5.09 | ppl   162.05\n",
      "|  1200/1000000 steps | lr 0.0050 | ms/batch 41.10 | loss  5.04 | ppl   153.72\n",
      "|  1400/1000000 steps | lr 0.0050 | ms/batch 42.02 | loss  4.98 | ppl   145.27\n",
      "|  1600/1000000 steps | lr 0.0050 | ms/batch 44.35 | loss  4.92 | ppl   137.28\n",
      "|  1800/1000000 steps | lr 0.0050 | ms/batch 43.12 | loss  4.88 | ppl   132.25\n",
      "|  2000/1000000 steps | lr 0.0050 | ms/batch 43.36 | loss  4.83 | ppl   124.93\n",
      "|  2200/1000000 steps | lr 0.0050 | ms/batch 39.92 | loss  4.77 | ppl   117.64\n",
      "|  2400/1000000 steps | lr 0.0050 | ms/batch 43.29 | loss  4.75 | ppl   115.23\n",
      "|  2600/1000000 steps | lr 0.0050 | ms/batch 41.98 | loss  4.68 | ppl   107.95\n",
      "|  2800/1000000 steps | lr 0.0050 | ms/batch 41.35 | loss  4.65 | ppl   105.08\n",
      "|  3000/1000000 steps | lr 0.0050 | ms/batch 44.14 | loss  4.64 | ppl   103.51\n",
      "|  3200/1000000 steps | lr 0.0050 | ms/batch 42.00 | loss  4.57 | ppl    96.49\n",
      "|  3400/1000000 steps | lr 0.0050 | ms/batch 44.49 | loss  4.56 | ppl    95.83\n",
      "|  3600/1000000 steps | lr 0.0050 | ms/batch 44.55 | loss  4.52 | ppl    91.39\n",
      "|  3800/1000000 steps | lr 0.0050 | ms/batch 44.10 | loss  4.50 | ppl    90.01\n",
      "|  4000/1000000 steps | lr 0.0050 | ms/batch 42.50 | loss  4.46 | ppl    86.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:91: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  4200/1000000 steps | lr 0.0050 | ms/batch 42.66 | loss  4.45 | ppl    85.24\n",
      "|  4400/1000000 steps | lr 0.0050 | ms/batch 42.49 | loss  4.40 | ppl    81.18\n",
      "|  4600/1000000 steps | lr 0.0050 | ms/batch 42.43 | loss  4.37 | ppl    78.90\n",
      "|  4800/1000000 steps | lr 0.0050 | ms/batch 44.00 | loss  4.37 | ppl    78.79\n",
      "|  5000/1000000 steps | lr 0.0050 | ms/batch 41.27 | loss  4.32 | ppl    74.82\n",
      "|  5200/1000000 steps | lr 0.0050 | ms/batch 43.67 | loss  4.31 | ppl    74.57\n",
      "|  5400/1000000 steps | lr 0.0050 | ms/batch 43.12 | loss  4.29 | ppl    73.20\n",
      "|  5600/1000000 steps | lr 0.0050 | ms/batch 42.30 | loss  4.27 | ppl    71.47\n",
      "|  5800/1000000 steps | lr 0.0050 | ms/batch 43.55 | loss  4.23 | ppl    68.62\n",
      "|  6000/1000000 steps | lr 0.0050 | ms/batch 42.09 | loss  4.20 | ppl    66.81\n",
      "|  6200/1000000 steps | lr 0.0050 | ms/batch 44.15 | loss  4.20 | ppl    66.62\n",
      "|  6400/1000000 steps | lr 0.0050 | ms/batch 44.26 | loss  4.19 | ppl    66.01\n",
      "|  6600/1000000 steps | lr 0.0050 | ms/batch 43.40 | loss  4.15 | ppl    63.53\n",
      "|  6800/1000000 steps | lr 0.0050 | ms/batch 44.01 | loss  4.15 | ppl    63.47\n",
      "|  7000/1000000 steps | lr 0.0050 | ms/batch 44.38 | loss  4.11 | ppl    60.97\n",
      "|  7200/1000000 steps | lr 0.0050 | ms/batch 42.95 | loss  4.09 | ppl    59.53\n",
      "|  7400/1000000 steps | lr 0.0050 | ms/batch 43.09 | loss  4.10 | ppl    60.14\n",
      "|  7600/1000000 steps | lr 0.0050 | ms/batch 42.66 | loss  4.10 | ppl    60.07\n",
      "|  7800/1000000 steps | lr 0.0050 | ms/batch 42.06 | loss  4.02 | ppl    55.79\n",
      "|  8000/1000000 steps | lr 0.0050 | ms/batch 44.03 | loss  4.04 | ppl    56.78\n",
      "|  8200/1000000 steps | lr 0.0049 | ms/batch 41.42 | loss  4.01 | ppl    55.28\n",
      "|  8400/1000000 steps | lr 0.0049 | ms/batch 42.40 | loss  4.00 | ppl    54.40\n",
      "|  8600/1000000 steps | lr 0.0049 | ms/batch 42.83 | loss  3.98 | ppl    53.58\n",
      "|  8800/1000000 steps | lr 0.0049 | ms/batch 42.94 | loss  3.97 | ppl    52.92\n",
      "|  9000/1000000 steps | lr 0.0049 | ms/batch 42.57 | loss  3.97 | ppl    53.23\n",
      "|  9200/1000000 steps | lr 0.0049 | ms/batch 42.03 | loss  3.93 | ppl    51.10\n",
      "|  9400/1000000 steps | lr 0.0049 | ms/batch 43.30 | loss  3.93 | ppl    50.98\n",
      "|  9600/1000000 steps | lr 0.0049 | ms/batch 41.69 | loss  3.92 | ppl    50.39\n",
      "|  9800/1000000 steps | lr 0.0049 | ms/batch 41.06 | loss  3.90 | ppl    49.28\n",
      "| 10000/1000000 steps | lr 0.0049 | ms/batch 45.27 | loss  3.90 | ppl    49.49\n",
      "| 10200/1000000 steps | lr 0.0049 | ms/batch 46.33 | loss  3.89 | ppl    48.93\n",
      "| 10400/1000000 steps | lr 0.0049 | ms/batch 45.54 | loss  3.90 | ppl    49.19\n",
      "| 10600/1000000 steps | lr 0.0049 | ms/batch 43.32 | loss  3.84 | ppl    46.76\n",
      "| 10800/1000000 steps | lr 0.0049 | ms/batch 44.64 | loss  3.85 | ppl    46.86\n",
      "| 11000/1000000 steps | lr 0.0049 | ms/batch 45.55 | loss  3.84 | ppl    46.35\n",
      "| 11200/1000000 steps | lr 0.0049 | ms/batch 43.30 | loss  3.81 | ppl    45.07\n",
      "| 11400/1000000 steps | lr 0.0049 | ms/batch 42.13 | loss  3.78 | ppl    43.93\n",
      "| 11600/1000000 steps | lr 0.0049 | ms/batch 42.20 | loss  3.83 | ppl    45.89\n",
      "| 11800/1000000 steps | lr 0.0049 | ms/batch 44.35 | loss  3.82 | ppl    45.74\n",
      "| 12000/1000000 steps | lr 0.0049 | ms/batch 42.13 | loss  3.78 | ppl    43.71\n",
      "| 12200/1000000 steps | lr 0.0049 | ms/batch 41.96 | loss  3.76 | ppl    42.77\n",
      "| 12400/1000000 steps | lr 0.0049 | ms/batch 43.57 | loss  3.77 | ppl    43.41\n",
      "| 12600/1000000 steps | lr 0.0049 | ms/batch 42.74 | loss  3.74 | ppl    41.94\n",
      "| 12800/1000000 steps | lr 0.0049 | ms/batch 42.96 | loss  3.73 | ppl    41.65\n",
      "| 13000/1000000 steps | lr 0.0049 | ms/batch 43.60 | loss  3.75 | ppl    42.36\n",
      "| 13200/1000000 steps | lr 0.0049 | ms/batch 42.45 | loss  3.73 | ppl    41.84\n",
      "| 13400/1000000 steps | lr 0.0049 | ms/batch 43.21 | loss  3.69 | ppl    39.95\n",
      "| 13600/1000000 steps | lr 0.0049 | ms/batch 42.09 | loss  3.71 | ppl    40.69\n",
      "| 13800/1000000 steps | lr 0.0049 | ms/batch 42.87 | loss  3.71 | ppl    40.88\n",
      "| 14000/1000000 steps | lr 0.0049 | ms/batch 42.69 | loss  3.71 | ppl    40.69\n",
      "| 14200/1000000 steps | lr 0.0049 | ms/batch 41.59 | loss  3.65 | ppl    38.62\n",
      "| 14400/1000000 steps | lr 0.0049 | ms/batch 43.56 | loss  3.66 | ppl    38.84\n",
      "| 14600/1000000 steps | lr 0.0049 | ms/batch 42.52 | loss  3.66 | ppl    38.90\n",
      "| 14800/1000000 steps | lr 0.0049 | ms/batch 41.91 | loss  3.65 | ppl    38.61\n",
      "| 15000/1000000 steps | lr 0.0049 | ms/batch 41.21 | loss  3.61 | ppl    37.02\n",
      "| 15200/1000000 steps | lr 0.0049 | ms/batch 43.97 | loss  3.64 | ppl    38.15\n",
      "| 15400/1000000 steps | lr 0.0049 | ms/batch 45.54 | loss  3.65 | ppl    38.50\n",
      "| 15600/1000000 steps | lr 0.0049 | ms/batch 44.24 | loss  3.62 | ppl    37.32\n",
      "| 15800/1000000 steps | lr 0.0049 | ms/batch 45.24 | loss  3.63 | ppl    37.59\n",
      "| 16000/1000000 steps | lr 0.0049 | ms/batch 42.10 | loss  3.58 | ppl    35.77\n",
      "| 16200/1000000 steps | lr 0.0048 | ms/batch 43.72 | loss  3.65 | ppl    38.37\n",
      "| 16400/1000000 steps | lr 0.0048 | ms/batch 42.31 | loss  3.57 | ppl    35.52\n",
      "| 16600/1000000 steps | lr 0.0048 | ms/batch 43.02 | loss  3.61 | ppl    36.83\n",
      "| 16800/1000000 steps | lr 0.0048 | ms/batch 41.20 | loss  3.55 | ppl    34.91\n",
      "| 17000/1000000 steps | lr 0.0048 | ms/batch 45.31 | loss  3.59 | ppl    36.20\n",
      "| 17200/1000000 steps | lr 0.0048 | ms/batch 44.15 | loss  3.57 | ppl    35.67\n",
      "| 17400/1000000 steps | lr 0.0048 | ms/batch 41.99 | loss  3.54 | ppl    34.63\n",
      "| 17600/1000000 steps | lr 0.0048 | ms/batch 42.17 | loss  3.54 | ppl    34.44\n",
      "| 17800/1000000 steps | lr 0.0048 | ms/batch 43.02 | loss  3.55 | ppl    34.80\n",
      "| 18000/1000000 steps | lr 0.0048 | ms/batch 42.02 | loss  3.54 | ppl    34.32\n",
      "| 18200/1000000 steps | lr 0.0048 | ms/batch 42.71 | loss  3.53 | ppl    34.12\n",
      "| 18400/1000000 steps | lr 0.0048 | ms/batch 43.81 | loss  3.55 | ppl    34.74\n",
      "| 18600/1000000 steps | lr 0.0048 | ms/batch 41.66 | loss  3.51 | ppl    33.35\n",
      "| 18800/1000000 steps | lr 0.0048 | ms/batch 41.60 | loss  3.52 | ppl    33.71\n",
      "| 19000/1000000 steps | lr 0.0048 | ms/batch 43.26 | loss  3.51 | ppl    33.36\n",
      "| 19200/1000000 steps | lr 0.0048 | ms/batch 43.25 | loss  3.48 | ppl    32.33\n",
      "| 19400/1000000 steps | lr 0.0048 | ms/batch 43.97 | loss  3.53 | ppl    34.06\n",
      "| 19600/1000000 steps | lr 0.0048 | ms/batch 42.89 | loss  3.49 | ppl    32.83\n",
      "| 19800/1000000 steps | lr 0.0048 | ms/batch 44.55 | loss  3.49 | ppl    32.90\n",
      "| 20000/1000000 steps | lr 0.0048 | ms/batch 42.72 | loss  3.48 | ppl    32.46\n",
      "| 20200/1000000 steps | lr 0.0048 | ms/batch 42.56 | loss  3.45 | ppl    31.55\n",
      "| 20400/1000000 steps | lr 0.0048 | ms/batch 43.60 | loss  3.46 | ppl    31.83\n",
      "| 20600/1000000 steps | lr 0.0048 | ms/batch 42.47 | loss  3.47 | ppl    32.09\n",
      "| 20800/1000000 steps | lr 0.0048 | ms/batch 41.73 | loss  3.46 | ppl    31.70\n",
      "| 21000/1000000 steps | lr 0.0048 | ms/batch 42.85 | loss  3.46 | ppl    31.94\n",
      "| 21200/1000000 steps | lr 0.0048 | ms/batch 43.35 | loss  3.42 | ppl    30.71\n",
      "| 21400/1000000 steps | lr 0.0048 | ms/batch 41.72 | loss  3.45 | ppl    31.57\n",
      "| 21600/1000000 steps | lr 0.0048 | ms/batch 42.63 | loss  3.44 | ppl    31.30\n",
      "| 21800/1000000 steps | lr 0.0048 | ms/batch 42.92 | loss  3.43 | ppl    30.99\n",
      "| 22000/1000000 steps | lr 0.0048 | ms/batch 42.54 | loss  3.41 | ppl    30.22\n",
      "| 22200/1000000 steps | lr 0.0048 | ms/batch 42.69 | loss  3.40 | ppl    29.98\n",
      "| 22400/1000000 steps | lr 0.0048 | ms/batch 42.41 | loss  3.43 | ppl    30.96\n",
      "| 22600/1000000 steps | lr 0.0048 | ms/batch 42.26 | loss  3.40 | ppl    29.85\n",
      "| 22800/1000000 steps | lr 0.0048 | ms/batch 43.01 | loss  3.39 | ppl    29.79\n",
      "| 23000/1000000 steps | lr 0.0048 | ms/batch 42.17 | loss  3.37 | ppl    29.15\n",
      "| 23200/1000000 steps | lr 0.0048 | ms/batch 43.00 | loss  3.42 | ppl    30.50\n",
      "| 23400/1000000 steps | lr 0.0048 | ms/batch 42.81 | loss  3.40 | ppl    29.84\n",
      "| 23600/1000000 steps | lr 0.0048 | ms/batch 41.12 | loss  3.36 | ppl    28.88\n",
      "| 23800/1000000 steps | lr 0.0048 | ms/batch 42.71 | loss  3.36 | ppl    28.85\n",
      "| 24000/1000000 steps | lr 0.0048 | ms/batch 42.30 | loss  3.37 | ppl    29.09\n",
      "| 24200/1000000 steps | lr 0.0047 | ms/batch 43.20 | loss  3.36 | ppl    28.65\n",
      "| 24400/1000000 steps | lr 0.0047 | ms/batch 42.92 | loss  3.37 | ppl    29.08\n",
      "| 24600/1000000 steps | lr 0.0047 | ms/batch 42.63 | loss  3.38 | ppl    29.29\n",
      "| 24800/1000000 steps | lr 0.0047 | ms/batch 41.25 | loss  3.32 | ppl    27.54\n",
      "| 25000/1000000 steps | lr 0.0047 | ms/batch 43.95 | loss  3.36 | ppl    28.75\n",
      "| 25200/1000000 steps | lr 0.0047 | ms/batch 42.23 | loss  3.30 | ppl    27.22\n",
      "| 25400/1000000 steps | lr 0.0047 | ms/batch 43.97 | loss  3.34 | ppl    28.33\n",
      "| 25600/1000000 steps | lr 0.0047 | ms/batch 42.24 | loss  3.32 | ppl    27.66\n",
      "| 25800/1000000 steps | lr 0.0047 | ms/batch 43.90 | loss  3.35 | ppl    28.45\n",
      "| 26000/1000000 steps | lr 0.0047 | ms/batch 42.17 | loss  3.31 | ppl    27.26\n",
      "| 26200/1000000 steps | lr 0.0047 | ms/batch 42.10 | loss  3.29 | ppl    26.97\n",
      "| 26400/1000000 steps | lr 0.0047 | ms/batch 42.58 | loss  3.33 | ppl    27.87\n",
      "| 26600/1000000 steps | lr 0.0047 | ms/batch 42.41 | loss  3.30 | ppl    27.00\n",
      "| 26800/1000000 steps | lr 0.0047 | ms/batch 42.65 | loss  3.29 | ppl    26.82\n",
      "| 27000/1000000 steps | lr 0.0047 | ms/batch 42.65 | loss  3.33 | ppl    27.82\n",
      "| 27200/1000000 steps | lr 0.0047 | ms/batch 41.44 | loss  3.29 | ppl    26.92\n",
      "| 27400/1000000 steps | lr 0.0047 | ms/batch 43.56 | loss  3.27 | ppl    26.31\n",
      "| 27600/1000000 steps | lr 0.0047 | ms/batch 42.86 | loss  3.29 | ppl    26.86\n",
      "| 27800/1000000 steps | lr 0.0047 | ms/batch 43.81 | loss  3.29 | ppl    26.83\n",
      "| 28000/1000000 steps | lr 0.0047 | ms/batch 43.45 | loss  3.28 | ppl    26.64\n",
      "| 28200/1000000 steps | lr 0.0047 | ms/batch 43.28 | loss  3.26 | ppl    25.93\n",
      "| 28400/1000000 steps | lr 0.0047 | ms/batch 41.70 | loss  3.28 | ppl    26.54\n",
      "| 28600/1000000 steps | lr 0.0047 | ms/batch 42.74 | loss  3.25 | ppl    25.77\n",
      "| 28800/1000000 steps | lr 0.0047 | ms/batch 43.33 | loss  3.29 | ppl    26.72\n",
      "| 29000/1000000 steps | lr 0.0047 | ms/batch 43.80 | loss  3.25 | ppl    25.86\n",
      "| 29200/1000000 steps | lr 0.0047 | ms/batch 43.85 | loss  3.22 | ppl    25.01\n",
      "| 29400/1000000 steps | lr 0.0047 | ms/batch 44.03 | loss  3.27 | ppl    26.34\n",
      "| 29600/1000000 steps | lr 0.0047 | ms/batch 42.62 | loss  3.23 | ppl    25.34\n",
      "| 29800/1000000 steps | lr 0.0047 | ms/batch 42.75 | loss  3.22 | ppl    25.01\n",
      "| 30000/1000000 steps | lr 0.0047 | ms/batch 42.57 | loss  3.23 | ppl    25.20\n",
      "| 30200/1000000 steps | lr 0.0047 | ms/batch 42.88 | loss  3.25 | ppl    25.69\n",
      "| 30400/1000000 steps | lr 0.0047 | ms/batch 41.42 | loss  3.25 | ppl    25.71\n",
      "| 30600/1000000 steps | lr 0.0047 | ms/batch 43.14 | loss  3.22 | ppl    24.91\n",
      "| 30800/1000000 steps | lr 0.0047 | ms/batch 41.40 | loss  3.21 | ppl    24.78\n",
      "| 31000/1000000 steps | lr 0.0047 | ms/batch 42.60 | loss  3.21 | ppl    24.86\n",
      "| 31200/1000000 steps | lr 0.0047 | ms/batch 42.35 | loss  3.21 | ppl    24.79\n",
      "| 31400/1000000 steps | lr 0.0047 | ms/batch 44.20 | loss  3.19 | ppl    24.20\n",
      "| 31600/1000000 steps | lr 0.0047 | ms/batch 42.43 | loss  3.21 | ppl    24.75\n",
      "| 31800/1000000 steps | lr 0.0047 | ms/batch 41.86 | loss  3.20 | ppl    24.44\n",
      "| 32000/1000000 steps | lr 0.0047 | ms/batch 41.49 | loss  3.15 | ppl    23.41\n",
      "| 32200/1000000 steps | lr 0.0046 | ms/batch 43.76 | loss  3.22 | ppl    25.14\n",
      "| 32400/1000000 steps | lr 0.0046 | ms/batch 44.02 | loss  3.20 | ppl    24.58\n",
      "| 32600/1000000 steps | lr 0.0046 | ms/batch 43.21 | loss  3.19 | ppl    24.18\n",
      "| 32800/1000000 steps | lr 0.0046 | ms/batch 42.97 | loss  3.19 | ppl    24.19\n",
      "| 33000/1000000 steps | lr 0.0046 | ms/batch 42.43 | loss  3.15 | ppl    23.44\n",
      "| 33200/1000000 steps | lr 0.0046 | ms/batch 42.48 | loss  3.18 | ppl    24.06\n",
      "| 33400/1000000 steps | lr 0.0046 | ms/batch 42.74 | loss  3.16 | ppl    23.46\n",
      "| 33600/1000000 steps | lr 0.0046 | ms/batch 43.13 | loss  3.19 | ppl    24.37\n",
      "| 33800/1000000 steps | lr 0.0046 | ms/batch 42.45 | loss  3.16 | ppl    23.59\n",
      "| 34000/1000000 steps | lr 0.0046 | ms/batch 43.71 | loss  3.16 | ppl    23.51\n",
      "| 34200/1000000 steps | lr 0.0046 | ms/batch 42.86 | loss  3.14 | ppl    23.15\n",
      "| 34400/1000000 steps | lr 0.0046 | ms/batch 41.37 | loss  3.16 | ppl    23.50\n",
      "| 34600/1000000 steps | lr 0.0046 | ms/batch 42.39 | loss  3.16 | ppl    23.56\n",
      "| 34800/1000000 steps | lr 0.0046 | ms/batch 43.21 | loss  3.15 | ppl    23.38\n",
      "| 35000/1000000 steps | lr 0.0046 | ms/batch 43.46 | loss  3.14 | ppl    23.19\n",
      "| 35200/1000000 steps | lr 0.0046 | ms/batch 42.51 | loss  3.14 | ppl    23.00\n",
      "| 35400/1000000 steps | lr 0.0046 | ms/batch 41.68 | loss  3.12 | ppl    22.64\n",
      "| 35600/1000000 steps | lr 0.0046 | ms/batch 42.33 | loss  3.14 | ppl    23.10\n",
      "| 35800/1000000 steps | lr 0.0046 | ms/batch 43.21 | loss  3.13 | ppl    22.83\n",
      "| 36000/1000000 steps | lr 0.0046 | ms/batch 41.14 | loss  3.12 | ppl    22.56\n",
      "| 36200/1000000 steps | lr 0.0046 | ms/batch 43.04 | loss  3.12 | ppl    22.57\n",
      "| 36400/1000000 steps | lr 0.0046 | ms/batch 43.47 | loss  3.11 | ppl    22.38\n",
      "| 36600/1000000 steps | lr 0.0046 | ms/batch 42.50 | loss  3.13 | ppl    22.92\n",
      "| 36800/1000000 steps | lr 0.0046 | ms/batch 41.64 | loss  3.11 | ppl    22.34\n",
      "| 37000/1000000 steps | lr 0.0046 | ms/batch 42.83 | loss  3.11 | ppl    22.34\n",
      "| 37200/1000000 steps | lr 0.0046 | ms/batch 41.68 | loss  3.11 | ppl    22.37\n",
      "| 37400/1000000 steps | lr 0.0046 | ms/batch 44.41 | loss  3.08 | ppl    21.72\n",
      "| 37600/1000000 steps | lr 0.0046 | ms/batch 44.78 | loss  3.12 | ppl    22.58\n",
      "| 37800/1000000 steps | lr 0.0046 | ms/batch 42.57 | loss  3.09 | ppl    22.04\n",
      "| 38000/1000000 steps | lr 0.0046 | ms/batch 42.45 | loss  3.06 | ppl    21.37\n",
      "| 38200/1000000 steps | lr 0.0046 | ms/batch 42.04 | loss  3.10 | ppl    22.28\n",
      "| 38400/1000000 steps | lr 0.0046 | ms/batch 43.46 | loss  3.08 | ppl    21.80\n",
      "| 38600/1000000 steps | lr 0.0046 | ms/batch 41.85 | loss  3.08 | ppl    21.75\n",
      "| 38800/1000000 steps | lr 0.0046 | ms/batch 41.50 | loss  3.08 | ppl    21.74\n",
      "| 39000/1000000 steps | lr 0.0046 | ms/batch 43.74 | loss  3.08 | ppl    21.66\n",
      "| 39200/1000000 steps | lr 0.0046 | ms/batch 43.01 | loss  3.06 | ppl    21.33\n",
      "| 39400/1000000 steps | lr 0.0046 | ms/batch 42.21 | loss  3.07 | ppl    21.56\n",
      "| 39600/1000000 steps | lr 0.0046 | ms/batch 43.20 | loss  3.08 | ppl    21.65\n",
      "| 39800/1000000 steps | lr 0.0046 | ms/batch 42.18 | loss  3.05 | ppl    21.18\n",
      "| 40000/1000000 steps | lr 0.0046 | ms/batch 42.52 | loss  3.08 | ppl    21.67\n",
      "| 40200/1000000 steps | lr 0.0045 | ms/batch 41.63 | loss  3.04 | ppl    21.01\n",
      "| 40400/1000000 steps | lr 0.0045 | ms/batch 43.72 | loss  3.04 | ppl    20.90\n",
      "| 40600/1000000 steps | lr 0.0045 | ms/batch 45.50 | loss  3.04 | ppl    20.97\n",
      "| 40800/1000000 steps | lr 0.0045 | ms/batch 44.33 | loss  3.06 | ppl    21.26\n",
      "| 41000/1000000 steps | lr 0.0045 | ms/batch 46.22 | loss  3.05 | ppl    21.20\n",
      "| 41200/1000000 steps | lr 0.0045 | ms/batch 44.33 | loss  3.04 | ppl    20.86\n",
      "| 41400/1000000 steps | lr 0.0045 | ms/batch 44.16 | loss  3.01 | ppl    20.39\n",
      "| 41600/1000000 steps | lr 0.0045 | ms/batch 44.08 | loss  3.03 | ppl    20.75\n",
      "| 41800/1000000 steps | lr 0.0045 | ms/batch 45.21 | loss  3.05 | ppl    21.13\n",
      "| 42000/1000000 steps | lr 0.0045 | ms/batch 42.76 | loss  3.02 | ppl    20.40\n",
      "| 42200/1000000 steps | lr 0.0045 | ms/batch 42.29 | loss  3.02 | ppl    20.52\n",
      "| 42400/1000000 steps | lr 0.0045 | ms/batch 41.27 | loss  3.03 | ppl    20.72\n",
      "| 42600/1000000 steps | lr 0.0045 | ms/batch 43.82 | loss  3.01 | ppl    20.35\n",
      "| 42800/1000000 steps | lr 0.0045 | ms/batch 42.01 | loss  3.01 | ppl    20.36\n",
      "| 43000/1000000 steps | lr 0.0045 | ms/batch 44.49 | loss  3.02 | ppl    20.45\n",
      "| 43200/1000000 steps | lr 0.0045 | ms/batch 44.49 | loss  3.02 | ppl    20.59\n",
      "| 43400/1000000 steps | lr 0.0045 | ms/batch 45.78 | loss  3.02 | ppl    20.47\n",
      "| 43600/1000000 steps | lr 0.0045 | ms/batch 43.64 | loss  3.01 | ppl    20.29\n",
      "| 43800/1000000 steps | lr 0.0045 | ms/batch 44.16 | loss  3.01 | ppl    20.26\n",
      "| 44000/1000000 steps | lr 0.0045 | ms/batch 41.88 | loss  2.98 | ppl    19.74\n",
      "| 44200/1000000 steps | lr 0.0045 | ms/batch 43.05 | loss  3.02 | ppl    20.52\n",
      "| 44400/1000000 steps | lr 0.0045 | ms/batch 41.72 | loss  2.96 | ppl    19.22\n",
      "| 44600/1000000 steps | lr 0.0045 | ms/batch 42.69 | loss  2.99 | ppl    19.90\n",
      "| 44800/1000000 steps | lr 0.0045 | ms/batch 43.29 | loss  2.99 | ppl    19.86\n",
      "| 45000/1000000 steps | lr 0.0045 | ms/batch 41.58 | loss  2.98 | ppl    19.72\n",
      "| 45200/1000000 steps | lr 0.0045 | ms/batch 41.54 | loss  2.98 | ppl    19.62\n",
      "| 45400/1000000 steps | lr 0.0045 | ms/batch 43.14 | loss  3.00 | ppl    20.04\n",
      "| 45600/1000000 steps | lr 0.0045 | ms/batch 43.83 | loss  2.98 | ppl    19.63\n",
      "| 45800/1000000 steps | lr 0.0045 | ms/batch 43.20 | loss  2.97 | ppl    19.43\n",
      "| 46000/1000000 steps | lr 0.0045 | ms/batch 45.78 | loss  2.95 | ppl    19.10\n",
      "| 46200/1000000 steps | lr 0.0045 | ms/batch 43.38 | loss  2.97 | ppl    19.42\n",
      "| 46400/1000000 steps | lr 0.0045 | ms/batch 42.20 | loss  2.95 | ppl    19.20\n",
      "| 46600/1000000 steps | lr 0.0045 | ms/batch 43.13 | loss  2.99 | ppl    19.86\n",
      "| 46800/1000000 steps | lr 0.0045 | ms/batch 44.06 | loss  2.96 | ppl    19.23\n",
      "| 47000/1000000 steps | lr 0.0045 | ms/batch 42.24 | loss  2.97 | ppl    19.48\n",
      "| 47200/1000000 steps | lr 0.0045 | ms/batch 42.40 | loss  2.93 | ppl    18.78\n",
      "| 47400/1000000 steps | lr 0.0045 | ms/batch 42.29 | loss  2.95 | ppl    19.18\n",
      "| 47600/1000000 steps | lr 0.0045 | ms/batch 43.79 | loss  2.95 | ppl    19.05\n",
      "| 47800/1000000 steps | lr 0.0045 | ms/batch 42.97 | loss  2.93 | ppl    18.65\n",
      "| 48000/1000000 steps | lr 0.0045 | ms/batch 42.58 | loss  2.95 | ppl    19.05\n",
      "| 48200/1000000 steps | lr 0.0044 | ms/batch 43.71 | loss  2.96 | ppl    19.34\n",
      "| 48400/1000000 steps | lr 0.0044 | ms/batch 41.92 | loss  2.92 | ppl    18.61\n",
      "| 48600/1000000 steps | lr 0.0044 | ms/batch 42.14 | loss  2.94 | ppl    18.87\n",
      "| 48800/1000000 steps | lr 0.0044 | ms/batch 42.45 | loss  2.93 | ppl    18.73\n",
      "| 49000/1000000 steps | lr 0.0044 | ms/batch 43.72 | loss  2.93 | ppl    18.81\n",
      "| 49200/1000000 steps | lr 0.0044 | ms/batch 42.21 | loss  2.93 | ppl    18.75\n",
      "| 49400/1000000 steps | lr 0.0044 | ms/batch 44.03 | loss  2.96 | ppl    19.29\n",
      "| 49600/1000000 steps | lr 0.0044 | ms/batch 41.23 | loss  2.90 | ppl    18.09\n",
      "| 49800/1000000 steps | lr 0.0044 | ms/batch 42.89 | loss  2.92 | ppl    18.48\n",
      "| 50000/1000000 steps | lr 0.0044 | ms/batch 42.99 | loss  2.92 | ppl    18.51\n",
      "| 50200/1000000 steps | lr 0.0044 | ms/batch 42.44 | loss  2.92 | ppl    18.51\n",
      "| 50400/1000000 steps | lr 0.0044 | ms/batch 43.06 | loss  2.95 | ppl    19.11\n",
      "| 50600/1000000 steps | lr 0.0044 | ms/batch 42.38 | loss  2.89 | ppl    17.92\n",
      "| 50800/1000000 steps | lr 0.0044 | ms/batch 42.64 | loss  2.90 | ppl    18.23\n",
      "| 51000/1000000 steps | lr 0.0044 | ms/batch 43.33 | loss  2.91 | ppl    18.35\n",
      "| 51200/1000000 steps | lr 0.0044 | ms/batch 42.64 | loss  2.87 | ppl    17.72\n",
      "| 51400/1000000 steps | lr 0.0044 | ms/batch 42.51 | loss  2.92 | ppl    18.48\n",
      "| 51600/1000000 steps | lr 0.0044 | ms/batch 41.12 | loss  2.88 | ppl    17.86\n",
      "| 51800/1000000 steps | lr 0.0044 | ms/batch 43.95 | loss  2.89 | ppl    17.99\n",
      "| 52000/1000000 steps | lr 0.0044 | ms/batch 44.70 | loss  2.88 | ppl    17.89\n",
      "| 52200/1000000 steps | lr 0.0044 | ms/batch 44.01 | loss  2.89 | ppl    18.00\n",
      "| 52400/1000000 steps | lr 0.0044 | ms/batch 44.79 | loss  2.91 | ppl    18.28\n",
      "| 52600/1000000 steps | lr 0.0044 | ms/batch 44.45 | loss  2.90 | ppl    18.17\n",
      "| 52800/1000000 steps | lr 0.0044 | ms/batch 45.33 | loss  2.87 | ppl    17.66\n",
      "| 53000/1000000 steps | lr 0.0044 | ms/batch 44.55 | loss  2.86 | ppl    17.51\n",
      "| 53200/1000000 steps | lr 0.0044 | ms/batch 44.24 | loss  2.88 | ppl    17.81\n",
      "| 53400/1000000 steps | lr 0.0044 | ms/batch 45.75 | loss  2.88 | ppl    17.90\n",
      "| 53600/1000000 steps | lr 0.0044 | ms/batch 44.91 | loss  2.87 | ppl    17.59\n",
      "| 53800/1000000 steps | lr 0.0044 | ms/batch 43.64 | loss  2.85 | ppl    17.25\n",
      "| 54000/1000000 steps | lr 0.0044 | ms/batch 45.46 | loss  2.88 | ppl    17.84\n",
      "| 54200/1000000 steps | lr 0.0044 | ms/batch 45.08 | loss  2.89 | ppl    17.95\n",
      "| 54400/1000000 steps | lr 0.0044 | ms/batch 45.34 | loss  2.87 | ppl    17.61\n",
      "| 54600/1000000 steps | lr 0.0044 | ms/batch 44.62 | loss  2.84 | ppl    17.14\n",
      "| 54800/1000000 steps | lr 0.0044 | ms/batch 44.74 | loss  2.87 | ppl    17.69\n",
      "| 55000/1000000 steps | lr 0.0044 | ms/batch 42.00 | loss  2.83 | ppl    16.96\n",
      "| 55200/1000000 steps | lr 0.0044 | ms/batch 41.70 | loss  2.86 | ppl    17.45\n",
      "| 55400/1000000 steps | lr 0.0044 | ms/batch 43.35 | loss  2.86 | ppl    17.43\n",
      "| 55600/1000000 steps | lr 0.0044 | ms/batch 42.18 | loss  2.84 | ppl    17.08\n",
      "| 55800/1000000 steps | lr 0.0044 | ms/batch 42.70 | loss  2.85 | ppl    17.37\n",
      "| 56000/1000000 steps | lr 0.0044 | ms/batch 43.03 | loss  2.84 | ppl    17.10\n",
      "| 56200/1000000 steps | lr 0.0043 | ms/batch 43.43 | loss  2.83 | ppl    17.03\n",
      "| 56400/1000000 steps | lr 0.0043 | ms/batch 43.95 | loss  2.86 | ppl    17.51\n",
      "| 56600/1000000 steps | lr 0.0043 | ms/batch 40.81 | loss  2.82 | ppl    16.74\n",
      "| 56800/1000000 steps | lr 0.0043 | ms/batch 44.20 | loss  2.87 | ppl    17.68\n",
      "| 57000/1000000 steps | lr 0.0043 | ms/batch 42.89 | loss  2.82 | ppl    16.76\n",
      "| 57200/1000000 steps | lr 0.0043 | ms/batch 41.00 | loss  2.81 | ppl    16.67\n",
      "| 57400/1000000 steps | lr 0.0043 | ms/batch 42.65 | loss  2.83 | ppl    16.97\n",
      "| 57600/1000000 steps | lr 0.0043 | ms/batch 44.69 | loss  2.82 | ppl    16.77\n",
      "| 57800/1000000 steps | lr 0.0043 | ms/batch 45.15 | loss  2.86 | ppl    17.45\n",
      "| 58000/1000000 steps | lr 0.0043 | ms/batch 43.02 | loss  2.81 | ppl    16.67\n",
      "| 58200/1000000 steps | lr 0.0043 | ms/batch 41.83 | loss  2.79 | ppl    16.33\n",
      "| 58400/1000000 steps | lr 0.0043 | ms/batch 42.96 | loss  2.81 | ppl    16.54\n",
      "| 58600/1000000 steps | lr 0.0043 | ms/batch 42.14 | loss  2.82 | ppl    16.73\n",
      "| 58800/1000000 steps | lr 0.0043 | ms/batch 43.04 | loss  2.83 | ppl    16.89\n",
      "| 59000/1000000 steps | lr 0.0043 | ms/batch 41.61 | loss  2.79 | ppl    16.28\n",
      "| 59200/1000000 steps | lr 0.0043 | ms/batch 43.76 | loss  2.82 | ppl    16.85\n",
      "| 59400/1000000 steps | lr 0.0043 | ms/batch 42.52 | loss  2.78 | ppl    16.16\n",
      "| 59600/1000000 steps | lr 0.0043 | ms/batch 41.61 | loss  2.79 | ppl    16.32\n",
      "| 59800/1000000 steps | lr 0.0043 | ms/batch 43.91 | loss  2.83 | ppl    16.90\n",
      "| 60000/1000000 steps | lr 0.0043 | ms/batch 41.61 | loss  2.81 | ppl    16.68\n",
      "| 60200/1000000 steps | lr 0.0043 | ms/batch 42.45 | loss  2.79 | ppl    16.25\n",
      "| 60400/1000000 steps | lr 0.0043 | ms/batch 43.65 | loss  2.82 | ppl    16.74\n",
      "| 60600/1000000 steps | lr 0.0043 | ms/batch 43.31 | loss  2.78 | ppl    16.14\n",
      "| 60800/1000000 steps | lr 0.0043 | ms/batch 41.33 | loss  2.78 | ppl    16.06\n",
      "| 61000/1000000 steps | lr 0.0043 | ms/batch 44.05 | loss  2.80 | ppl    16.41\n",
      "| 61200/1000000 steps | lr 0.0043 | ms/batch 42.98 | loss  2.79 | ppl    16.24\n",
      "| 61400/1000000 steps | lr 0.0043 | ms/batch 42.25 | loss  2.77 | ppl    15.92\n",
      "| 61600/1000000 steps | lr 0.0043 | ms/batch 42.79 | loss  2.79 | ppl    16.24\n",
      "| 61800/1000000 steps | lr 0.0043 | ms/batch 43.36 | loss  2.81 | ppl    16.62\n",
      "| 62000/1000000 steps | lr 0.0043 | ms/batch 42.40 | loss  2.75 | ppl    15.68\n",
      "| 62200/1000000 steps | lr 0.0043 | ms/batch 41.61 | loss  2.77 | ppl    15.92\n",
      "| 62400/1000000 steps | lr 0.0043 | ms/batch 43.13 | loss  2.78 | ppl    16.05\n",
      "| 62600/1000000 steps | lr 0.0043 | ms/batch 42.92 | loss  2.77 | ppl    15.92\n",
      "| 62800/1000000 steps | lr 0.0043 | ms/batch 43.20 | loss  2.75 | ppl    15.68\n",
      "| 63000/1000000 steps | lr 0.0043 | ms/batch 41.46 | loss  2.79 | ppl    16.29\n",
      "| 63200/1000000 steps | lr 0.0043 | ms/batch 43.44 | loss  2.77 | ppl    15.96\n",
      "| 63400/1000000 steps | lr 0.0043 | ms/batch 41.50 | loss  2.77 | ppl    15.92\n",
      "| 63600/1000000 steps | lr 0.0043 | ms/batch 42.55 | loss  2.76 | ppl    15.81\n",
      "| 63800/1000000 steps | lr 0.0043 | ms/batch 43.25 | loss  2.75 | ppl    15.62\n",
      "| 64000/1000000 steps | lr 0.0043 | ms/batch 42.86 | loss  2.76 | ppl    15.80\n",
      "| 64200/1000000 steps | lr 0.0043 | ms/batch 42.34 | loss  2.71 | ppl    15.02\n",
      "| 64400/1000000 steps | lr 0.0043 | ms/batch 43.03 | loss  2.78 | ppl    16.08\n",
      "| 64600/1000000 steps | lr 0.0043 | ms/batch 43.18 | loss  2.77 | ppl    15.90\n",
      "| 64800/1000000 steps | lr 0.0043 | ms/batch 42.71 | loss  2.76 | ppl    15.80\n",
      "| 65000/1000000 steps | lr 0.0043 | ms/batch 41.86 | loss  2.76 | ppl    15.74\n",
      "| 65200/1000000 steps | lr 0.0043 | ms/batch 42.32 | loss  2.72 | ppl    15.19\n",
      "| 65400/1000000 steps | lr 0.0043 | ms/batch 44.49 | loss  2.74 | ppl    15.52\n",
      "| 65600/1000000 steps | lr 0.0043 | ms/batch 45.38 | loss  2.75 | ppl    15.60\n",
      "| 65800/1000000 steps | lr 0.0043 | ms/batch 44.69 | loss  2.72 | ppl    15.20\n",
      "| 66000/1000000 steps | lr 0.0043 | ms/batch 44.81 | loss  2.75 | ppl    15.68\n",
      "| 66200/1000000 steps | lr 0.0043 | ms/batch 46.01 | loss  2.74 | ppl    15.54\n",
      "| 66400/1000000 steps | lr 0.0043 | ms/batch 43.63 | loss  2.72 | ppl    15.13\n",
      "| 66600/1000000 steps | lr 0.0043 | ms/batch 42.17 | loss  2.73 | ppl    15.38\n",
      "| 66800/1000000 steps | lr 0.0043 | ms/batch 42.58 | loss  2.72 | ppl    15.18\n",
      "| 67000/1000000 steps | lr 0.0043 | ms/batch 44.18 | loss  2.74 | ppl    15.43\n",
      "| 67200/1000000 steps | lr 0.0043 | ms/batch 43.50 | loss  2.74 | ppl    15.52\n",
      "| 67400/1000000 steps | lr 0.0043 | ms/batch 44.98 | loss  2.71 | ppl    15.02\n",
      "| 67600/1000000 steps | lr 0.0043 | ms/batch 44.33 | loss  2.72 | ppl    15.20\n",
      "| 67800/1000000 steps | lr 0.0043 | ms/batch 44.06 | loss  2.73 | ppl    15.32\n",
      "| 68000/1000000 steps | lr 0.0043 | ms/batch 44.93 | loss  2.73 | ppl    15.37\n",
      "| 68200/1000000 steps | lr 0.0042 | ms/batch 44.73 | loss  2.71 | ppl    14.99\n",
      "| 68400/1000000 steps | lr 0.0042 | ms/batch 45.17 | loss  2.72 | ppl    15.23\n",
      "| 68600/1000000 steps | lr 0.0042 | ms/batch 44.60 | loss  2.71 | ppl    15.03\n",
      "| 68800/1000000 steps | lr 0.0042 | ms/batch 44.23 | loss  2.71 | ppl    15.05\n",
      "| 69000/1000000 steps | lr 0.0042 | ms/batch 44.66 | loss  2.70 | ppl    14.93\n",
      "| 69200/1000000 steps | lr 0.0042 | ms/batch 43.72 | loss  2.69 | ppl    14.75\n",
      "| 69400/1000000 steps | lr 0.0042 | ms/batch 43.17 | loss  2.71 | ppl    15.10\n",
      "| 69600/1000000 steps | lr 0.0042 | ms/batch 43.11 | loss  2.69 | ppl    14.76\n",
      "| 69800/1000000 steps | lr 0.0042 | ms/batch 43.35 | loss  2.71 | ppl    15.04\n",
      "| 70000/1000000 steps | lr 0.0042 | ms/batch 42.67 | loss  2.70 | ppl    14.82\n",
      "| 70200/1000000 steps | lr 0.0042 | ms/batch 42.25 | loss  2.72 | ppl    15.18\n",
      "| 70400/1000000 steps | lr 0.0042 | ms/batch 41.94 | loss  2.68 | ppl    14.57\n",
      "| 70600/1000000 steps | lr 0.0042 | ms/batch 43.38 | loss  2.70 | ppl    14.88\n",
      "| 70800/1000000 steps | lr 0.0042 | ms/batch 42.70 | loss  2.68 | ppl    14.56\n",
      "| 71000/1000000 steps | lr 0.0042 | ms/batch 45.56 | loss  2.70 | ppl    14.91\n",
      "| 71200/1000000 steps | lr 0.0042 | ms/batch 43.16 | loss  2.67 | ppl    14.41\n",
      "| 71400/1000000 steps | lr 0.0042 | ms/batch 42.53 | loss  2.70 | ppl    14.94\n",
      "| 71600/1000000 steps | lr 0.0042 | ms/batch 42.70 | loss  2.67 | ppl    14.43\n",
      "| 71800/1000000 steps | lr 0.0042 | ms/batch 41.13 | loss  2.69 | ppl    14.66\n"
     ]
    }
   ],
   "source": [
    "def train(steps=10000, log_interval=200, learning_interval=4000, eval_interval=1000):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    step = 1\n",
    "    for batch in train_iterator:\n",
    "        loss = train_step(batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} steps | '\n",
    "                  'lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    step, steps, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        if step % eval_interval == 0:\n",
    "            print(\"Evaluating model\")\n",
    "            evaluate()\n",
    "            model.train()\n",
    "        \n",
    "        if step % learning_interval == 0:\n",
    "            scheduler.step()\n",
    "        \n",
    "        step += 1\n",
    "        if step >= steps:\n",
    "            print(\"Finished training\")\n",
    "            return\n",
    "\n",
    "train(steps=1000000,eval_interval=1000000,log_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((model, optimizer, scheduler), \"./saved_model.pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(SRC_TEXT.vocab.itos)\n",
    "tgt_vocab_size = len(TGT_TEXT.vocab.itos)\n",
    "\n",
    "# model = TransformerModel(src_vocab_size, tgt_vocab_size, dropout=0.2).to(device) \n",
    "# model.load_state_dict(torch.load(\"./saved_model.pytorch\"))\n",
    "(model, optimizer, scheduler) = torch.load(\"./saved_model.pytorch\")\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EVALUATION |     0/   59 batches |\n",
      "| EVALUATION |    20/   59 batches |\n",
      "| EVALUATION |    40/   59 batches |\n",
      "| EVALUATION | BLEU:  0.32 |\n"
     ]
    }
   ],
   "source": [
    "evaluate(beam_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> substitute args for self . _ _ args . <eos>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([SRC_TEXT.vocab.itos[i] for i in [ 2,21,83,13,10, 4, 5, 5,83, 4, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> self . _'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([TGT_TEXT.vocab.itos[i] for i in [ 2,12,5,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC ids shape: tensor([[  2],\n",
      "        [ 15],\n",
      "        [533],\n",
      "        [ 11],\n",
      "        [ 53],\n",
      "        [  6],\n",
      "        [  3]], device='cuda:0')\n",
      "if if if = None , name , name , "
     ]
    }
   ],
   "source": [
    "def translate(s):\n",
    "    src_ids = SRC_TEXT.numericalize([[\"<sos>\"] + SRC_TEXT.preprocess(s) + [\"<eos>\"]], device=device)\n",
    "#     src_ids = torch.tensor([ [2],[21],[83],[13],[10], [4], [5], [5],[83], [4], [3]], device=device)\n",
    "    print(\"SRC ids shape:\",src_ids)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sos_id = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "#         decoder_input = torch.zeros((1, 1), dtype=torch.long, device=device).fill_(sos_id)\n",
    "        decoder_input = torch.tensor(np.array([ [2]]), device=device)\n",
    "#         print(\"Decoder input shape:\", decoder_input.shape)\n",
    "        \n",
    "        for i in range(10):\n",
    "#             print(\"Decoder input\", decoder_input)\n",
    "            output = model(src_ids, decoder_input)\n",
    "#             print(model.tgt_mask)\n",
    "#             print(\"output:\", output)\n",
    "#             print(\"predicted ids:\", output.argmax(dim=-1))\n",
    "            last_pred = output[-1:].argmax(dim=2)\n",
    "#             decoder_input[i+1][0] = last_pred\n",
    "#             print(\"last pred:\", TGT_TEXT.vocab.itos[last_pred.cpu().numpy()[0][0]], last_pred.cpu().numpy()[0][0])\n",
    "            print(TGT_TEXT.vocab.itos[last_pred.cpu().numpy()[0][0]],'', end = '')\n",
    "            \n",
    "            decoder_input = torch.cat((decoder_input, last_pred))\n",
    "#             print(\"Decoder input\", decoder_input)\n",
    "#             break\n",
    "\n",
    "translate(\"if PY3 is true ,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([torch.tensor([1.0]),torch.tensor([2.0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moses Multi-BLEU perl script returns 0.0 for any sentence less than 4 tokens long.\n",
    "It will be best to use a function by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_moses_multi_bleu([\"this is a test\"], [\"this is a for\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
