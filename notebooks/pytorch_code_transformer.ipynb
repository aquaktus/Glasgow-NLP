{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import tqdm\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import operator\n",
    "\n",
    "from base_transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.set_device(0) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 13 12:00:12 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.26       Driver Version: 430.26       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 41%   36C    P8     5W / 280W |     10MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exists\n",
      "--2019-11-13 12:00:12--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1382085 (1.3M) [text/plain]\n",
      "Saving to: './datasets/all.desc'\n",
      "\n",
      "./datasets/all.desc 100%[===================>]   1.32M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2019-11-13 12:00:12 (15.4 MB/s) - './datasets/all.desc' saved [1382085/1382085]\n",
      "\n",
      "--2019-11-13 12:00:13--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 906732 (885K) [text/plain]\n",
      "Saving to: './datasets/all.code'\n",
      "\n",
      "./datasets/all.code 100%[===================>] 885.48K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2019-11-13 12:00:13 (11.6 MB/s) - './datasets/all.code' saved [906732/906732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"./datasets\")\n",
    "except FileExistsError:\n",
    "    print(\"Directories already exists\")\n",
    "\n",
    "# getting descriptions\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno -O ./datasets/all.desc\n",
    "\n",
    "# getting code\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code -O ./datasets/all.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a token text encoder\n",
    "An encoder will take a file and a splitting function and return an object able to encode and decode a string. It will also be able to save a vocab file and retrieve from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel_to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'substitute', 'the', 'result', 'for', 'field_type', '.', ')']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (substitute the result for field_type.)\"\n",
    "\n",
    "# looks like code split need parenthesis to be matched in the same string, if not it gives an error...\n",
    "def code_split(s):\n",
    "    return [x.string for x in tokenize(BytesIO(s.encode('utf-8')).readline) if x.string != '' and x.string != \"\\n\" and not x.string.isspace()][1:]\n",
    "\n",
    "print(code_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel', '_', 'to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'subs', '_', '_', 'titute', 'the', 'result', \"'\", 'for', 'field', '_', 'type', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (subs__titute the result' for field_type.\"\n",
    "\n",
    "def string_split(s):\n",
    "    return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(_|\\W)', s))) # this will chunk all code properly by plits strings with quotes\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|_|\\W)', s))) # this keeps the strings intact\n",
    "\n",
    "print(string_split(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_array(src_fp, tgt_fp):\n",
    "    lines = []\n",
    "    with open(src_fp, \"r\") as src_file, open(tgt_fp, \"r\") as tgt_file:\n",
    "        for src, tgt in zip(src_file, tgt_file):\n",
    "            lines.append((src, tgt))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(data, max_seq_length=200, tokenizer=string_split):\n",
    "    return [(src, tgt) for src, tgt in data if len(string_split(src)) <= max_seq_length and len(string_split(tgt)) <= max_seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_dataset(samples, src_field, tgt_field):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples: [(src_string),(tgt_string)]\n",
    "        src/tgt_tokenizer: a func that takes a string and returns an array of strings\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        src_string, tgt_string = sample\n",
    "        examples.append(torchtext.data.Example.fromdict({\"src\":src_string, \"tgt\":tgt_string}, \n",
    "                                        fields={\"src\":(\"src\",src_field), \"tgt\":(\"tgt\",tgt_field)}))\n",
    "        \n",
    "    dataset = torchtext.data.Dataset(examples,fields={\"src\":src_field, \"tgt\":tgt_field})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corpus_to_array(\"datasets/all.desc\", \"datasets/all.code\")\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src length: 586\n",
      "Max tgt length: 1087\n"
     ]
    }
   ],
   "source": [
    "print(\"Max src length:\", max([len(string_split(src)) for src, tgt in data]))\n",
    "print(\"Max tgt length:\", max([len(string_split(tgt)) for src, tgt in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 18805\n",
      "Limited dataset size: 18781\n"
     ]
    }
   ],
   "source": [
    "print(\"Full dataset size:\", len(data))\n",
    "max_seq_length=200\n",
    "data = filter_corpus(data, max_seq_length=200, tokenizer=string_split)\n",
    "print(\"Limited dataset size:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    SRC_TEXT = torch.load(\"./src_vocab.vcb\")\n",
    "except:\n",
    "    SRC_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "\n",
    "try:\n",
    "    TGT_TEXT = torch.load(\"./tgt_vocab.vcb\")\n",
    "except:\n",
    "    TGT_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "\n",
    "dataset = samples_to_dataset(data, SRC_TEXT, TGT_TEXT)\n",
    "\n",
    "train_dataset, val_dataset = dataset.split([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call -> 19\n",
      "the -> 7\n",
      "method -> 16\n",
      "operator -> 764\n",
      ". -> 4\n",
      "attrgetter -> 1799\n",
      "with -> 9\n",
      "an -> 12\n",
      "argument -> 20\n",
      "_ -> 5\n",
      "func -> 142\n",
      "_ -> 5\n",
      "code -> 141\n",
      ", -> 6\n",
      "substitute -> 21\n",
      "the -> 7\n",
      "result -> 22\n",
      "for -> 13\n",
      "get -> 50\n",
      "_ -> 5\n",
      "method -> 16\n",
      "_ -> 5\n",
      "code -> 141\n",
      ". -> 4\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(SRC_TEXT, \"vocab\"):\n",
    "    print(\"creating src vocab\")\n",
    "    SRC_TEXT.build_vocab(train_dataset)\n",
    "if not hasattr(TGT_TEXT, \"vocab\"):\n",
    "    print(\"creating tgt vocab\")\n",
    "    TGT_TEXT.build_vocab(train_dataset)\n",
    "\n",
    "\n",
    "sample = dataset[2].src\n",
    "for tok, id in zip(sample, SRC_TEXT.numericalize([sample])):\n",
    "    print(\"{} -> {}\".format(tok, id.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset iterator\n",
    "This will create a finction returning a different batch. The `train_iterator` is infinitely repeating. while the validation one is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'if', 'format', 'is', 'not', 'contained', 'in', '_', 'serializers', ',', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[  2  15 108  11  39 106  38   5 416   6   3   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1]\n",
      "[  2  14  81  29  24   4 326  11   3   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_iterator = BucketIterator(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    repeat=True,\n",
    "#     shuffle=True,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "valid_iterator = BucketIterator(val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "# The iterator generates batches with padded length for sequences with similar sizes, a batch is [seq_length, batch_size]\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    idx = 0\n",
    "    print([SRC_TEXT.vocab.itos[id] for id in batch.src.cpu().numpy()[:,idx]])\n",
    "    print(batch.src.cpu().numpy()[:,idx])\n",
    "    print(batch.tgt.cpu().numpy()[:,idx])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample transformer without positional encoding, it uses the built in transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_transformer_model = nn.Transformer() # uses default hyperparameters\n",
    "src = torch.rand((10, 32, 512)) # [src_seq_length, batch_size, embedding_size]\n",
    "tgt = torch.rand((20, 32, 512)) # [tgt_seq_length, batch_size, embedding_size]\n",
    "rand_transformer_model(src, tgt).shape # [tgt_seq_length, batch_size, embedding_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(SRC_TEXT.vocab.itos)\n",
    "tgt_vocab_size = len(TGT_TEXT.vocab.itos)\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, dropout=0.2).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_batch_ids(encoder_input, max_seq_length=50):\n",
    "    batch_len = encoder_input.shape[1]\n",
    "    sos_id = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "    decoder_input = torch.zeros((1, batch_len), dtype=torch.long, device=device).fill_(sos_id)\n",
    "\n",
    "    for i in range(max_seq_length):\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        last_pred = output[-1:].argmax(dim=2)\n",
    "\n",
    "        decoder_input = torch.cat((decoder_input, last_pred))\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return True\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        beta = 4.0\n",
    "        # Add here a function for shaping a reward\n",
    "\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2689, 0.7311]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1.,2.]]).softmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f beam_decode beam_decode(model, batch_size=1, encoder_states=src_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'BadHeaderError', 'cyclevars', 'DebugNode', 'FORMAT', 'cnt', 'cyclevars', 'DebugNode', 'UTF16', 'dT', 'BadHeaderError', 'BadHeaderError', 'omittable', 'rindex', 'store', 'UTF16', 'includes', 'rindex']\n",
      "['<sos>', 'BadHeaderError', 'cyclevars', 'DebugNode', 'FORMAT', 'cnt', 'cyclevars', 'DebugNode', 'UTF16', 'dT', 'BadHeaderError', 'BadHeaderError', 'omittable', 'rindex', 'store', 'UTF16', 'includes', 'UTF16']\n",
      "['<sos>', 'BadHeaderError', 'cyclevars', 'DebugNode', 'FORMAT', 'cnt', 'cyclevars', 'DebugNode', 'UTF16', 'dT', 'BadHeaderError', 'BadHeaderError', 'omittable', 'rindex', 'store', 'UTF16', 'includes', 'unescape', 'BadHeaderError', 'moved', 'cyclevars', 'BadHeaderError', 'autocomplete']\n"
     ]
    }
   ],
   "source": [
    "def beam_decode(model, encoder_states):\n",
    "    '''\n",
    "    :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
    "    :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
    "    :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
    "    :return: decoded_batch\n",
    "    '''\n",
    "\n",
    "    beam_width = 10\n",
    "    topk = 3  # how many sentence do you want to generate\n",
    "    decoded_batch = []\n",
    "    \n",
    "    SOS_token = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "    EOS_token = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "    MAX_LENGTH = 7\n",
    "    \n",
    "    batch_size = encoder_states.shape[1]\n",
    "\n",
    "    # decoding goes sentence by sentence\n",
    "    for idx in range(batch_size):\n",
    "        encoder_input = encoder_states[:, idx].view(-1,1)\n",
    "        \n",
    "        # Start with the start of the sentence token\n",
    "        decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
    "\n",
    "        # Number of sentence to generate\n",
    "        endnodes = []\n",
    "        number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "        # starting node -  hidden vector, previous node, word id, logp, length\n",
    "        node = BeamSearchNode(decoder_input, None, SOS_token, 0, 1)\n",
    "        nodes = PriorityQueue()\n",
    "\n",
    "        # start the queue\n",
    "        nodes.put((-node.eval(), node))\n",
    "        qsize = 1\n",
    "\n",
    "        # start beam search\n",
    "        while True:\n",
    "            # give up when decoding takes too long\n",
    "            if qsize > 400: break\n",
    "\n",
    "            # fetch the best node\n",
    "            score, n = nodes.get()\n",
    "#             decoder_input = n.wordid\n",
    "            decoder_input = n.h\n",
    "\n",
    "            if n.wordid == EOS_token and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # decode for one step using decoder\n",
    "#             decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "#             print(encoder_input)\n",
    "#             print(decoder_input)\n",
    "            decoder_output = model(encoder_input, decoder_input)\n",
    "            last_token_logits = decoder_output[-1]\n",
    "            last_token_logs = last_token_logits.log_softmax(1)\n",
    "            # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "            log_prob, indexes = torch.topk(last_token_logs, beam_width)\n",
    "            nextnodes = []\n",
    "\n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[0][new_k]\n",
    "                log_p = log_prob[0][new_k].item()\n",
    "                decoder_input = torch.cat((decoder_input,decoded_t.view(1,-1)))\n",
    "                node = BeamSearchNode(decoder_input, n, decoded_t.cpu().item(), n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "\n",
    "            # put them into queue\n",
    "            for i in range(len(nextnodes)):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "                # increase qsize\n",
    "            qsize += len(nextnodes) - 1\n",
    "\n",
    "        # choose nbest paths, back trace them\n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "        utterances = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "            utterance = []\n",
    "            utterance.append(n.wordid)\n",
    "            # back trace\n",
    "            while n.prevNode != None:\n",
    "                n = n.prevNode\n",
    "                utterance.append(n.wordid)\n",
    "\n",
    "            utterance = utterance[::-1]\n",
    "            utterances.append(utterance)\n",
    "\n",
    "        decoded_batch.append(utterances)\n",
    "\n",
    "    return decoded_batch\n",
    "\n",
    "sent1 = [\"<sos>\"] + SRC_TEXT.preprocess(\"call the options.get method with string 'CULL_FREQUENCY' and integer 3 as arguments, use the string 'cull_frequency' and previous result as the arguments for the call to the params.get method, substitute the result for cull_frequency.\") + [\"<eos>\"] + [\"<pad>\"]\n",
    "# sent2 = [\"<sos>\"] + SRC_TEXT.preprocess(\"if not,\") + [\"<eos>\"]\n",
    "src_ids = SRC_TEXT.numericalize([sent1], device=device)\n",
    "# print(\"input ids:\", src_ids)\n",
    "outs = beam_decode(model, encoder_states=src_ids)\n",
    "\n",
    "for b in outs:\n",
    "    for sent in b:\n",
    "        print([TGT_TEXT.vocab.itos[id] for id in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.1062,  0.5920, -1.4175,  ...,  0.2437,  0.2524, -1.2225]],\n",
      "\n",
      "        [[ 2.4309,  0.8538, -1.8844,  ...,  0.6236, -0.1974, -1.6820]],\n",
      "\n",
      "        [[ 2.7053,  0.8010, -0.7450,  ...,  1.4385, -0.7941, -1.2559]],\n",
      "\n",
      "        [[ 1.7970,  0.0554, -0.6959,  ...,  0.8729,  0.0657, -1.1797]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "['def', 'self', 'raise', '(']\n"
     ]
    }
   ],
   "source": [
    "s = \"for every log in existing ,\"\n",
    "sent1 = [\"<sos>\"] + SRC_TEXT.preprocess(s) + [\"<eos>\"]\n",
    "src_ids = SRC_TEXT.numericalize([sent1], device=device)\n",
    "\n",
    "decode_ids = SRC_TEXT.numericalize([['<sos>', 'self', '.', 'name']], device=device)\n",
    "\n",
    "output = model(src_ids, decode_ids)\n",
    "print(output)\n",
    "print([TGT_TEXT.vocab.itos[f] for f in output.argmax(dim=-1).view(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  2],\n",
      "        [17, 17],\n",
      "        [10, 10],\n",
      "        [12, 12],\n",
      "        [ 5,  5],\n",
      "        [ 4,  4],\n",
      "        [20, 20],\n",
      "        [ 6,  6],\n",
      "        [12, 12],\n",
      "        [ 7,  8],\n",
      "        [ 3, 25],\n",
      "        [10,  8],\n",
      "        [ 9, 25],\n",
      "        [ 7,  7],\n",
      "        [ 3, 11],\n",
      "        [ 7,  3],\n",
      "        [ 3,  8],\n",
      "        [ 3, 25],\n",
      "        [ 3,  7],\n",
      "        [ 3, 11],\n",
      "        [ 3,  3]], device='cuda:0')\n",
      "['<sos>', '<sos>', 'def', 'def', '=', '=', 'self', 'self', '.', '.', '_', '_', 'name', 'name', '(', '(', 'self', 'self', ')', ',', '<eos>', '*', '=', ',', \"'\", '*', ')', ')', '<eos>', ':', ')', '<eos>', '<eos>', ',', '<eos>', '*', '<eos>', ')', '<eos>', ':', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "output = greedy_decode_batch_ids(src_ids, max_seq_length=20)\n",
    "print(output)\n",
    "print([TGT_TEXT.vocab.itos[f] for f in output.view(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND, -8.140298843383789\n",
      "FOUND, -8.911083221435547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 4.829 s\n",
       "File: /nfs/phd_by_carlos/notebooks/beam_search.py\n",
       "Function: beam_search_decode at line 44\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    44                                           def beam_search_decode(model,TGT_TEXT, batch_encoder_ids, beam_size=3, num_out=3, max_length=10, SOS_token=1,EOS_token=2, PAD_token=3):\n",
       "    45                                               '''\n",
       "    46                                               :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
       "    47                                               :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
       "    48                                               :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
       "    49                                               :return: decoded_batch\n",
       "    50                                               '''\n",
       "    51         1        690.0    690.0      0.0      model.eval()\n",
       "    52         1         21.0     21.0      0.0      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
       "    53         1          1.0      1.0      0.0      decoded_batch = []\n",
       "    54         1          6.0      6.0      0.0      batch_size = batch_encoder_ids.shape[1]\n",
       "    55         1          2.0      2.0      0.0      batch_endnodes = [[] for i in range(batch_size)]\n",
       "    56                                           #     batch_trees = [PriorityQueue() for i in range(batch_size)]\n",
       "    57         1          2.0      2.0      0.0      batch_trees = [[] for i in range(batch_size)]\n",
       "    58                                               \n",
       "    59         1          2.0      2.0      0.0      beam_sizes = [beam_size] * batch_size\n",
       "    60                                               \n",
       "    61         1        176.0    176.0      0.0      first_decoder_id = torch.LongTensor([[SOS_token]]).to(device)\n",
       "    62                                               \n",
       "    63         3          5.0      1.7      0.0      for i, decode_tree in enumerate(batch_trees):\n",
       "    64         2         91.0     45.5      0.0          root_node = BeamSearchNode(first_decoder_id, batch_encoder_ids[:,i].view(-1,1), None, SOS_token, 0, 1,i)\n",
       "    65         2          9.0      4.5      0.0          heappush(decode_tree, (-root_node.eval(), root_node))\n",
       "    66                                           #         decode_tree.put((-root_node.eval(), root_node))\n",
       "    67                                               \n",
       "    68         1          1.0      1.0      0.0      while True:\n",
       "    69                                           #         print(\"STEP\")\n",
       "    70        20       1334.0     66.7      0.0          working_nodes = []\n",
       "    71        60        814.0     13.6      0.0          for tree, sample_endnode_list, sample_beam_size in zip(batch_trees, batch_endnodes, beam_sizes):\n",
       "    72       802        924.0      1.2      0.0              for i in range(sample_beam_size):\n",
       "    73                                           #                 if tree.empty():\n",
       "    74       764        916.0      1.2      0.0                  if len(tree) == 0:\n",
       "    75         2          2.0      1.0      0.0                      break\n",
       "    76       762      86078.0    113.0      1.8                  score, node = heappop(tree)\n",
       "    77                                           #                 score, node = tree.get()\n",
       "    78                                           #                 print(node.leng)\n",
       "    79       762      18102.0     23.8      0.4                  if node.last_id == EOS_token or node.leng >= max_length:\n",
       "    80       211        314.0      1.5      0.0                      if len(sample_endnode_list) < num_out:\n",
       "    81         2        399.0    199.5      0.0                          print(\"FOUND,\",node.logp.item())\n",
       "    82         2          4.0      2.0      0.0                          sample_endnode_list.append(node)\n",
       "    83       211        249.0      1.2      0.0                      sample_beam_size -= 1\n",
       "    84                                                           else:\n",
       "    85                                           #                     print(node.batch_num, float(node.logp), TGT_TEXT.vocab.itos[node.last_id])\n",
       "    86       551        812.0      1.5      0.0                      working_nodes.append((score, node))\n",
       "    87                                                       \n",
       "    88        40         65.0      1.6      0.0              del tree[:]\n",
       "    89                                                       # empty the tree to free up memory on the GPU\n",
       "    90                                           #             while not tree.empty():\n",
       "    91                                           #             for j in range(len(tree)):\n",
       "    92                                           #                 score, node = tree.get()\n",
       "    93                                           #                 del tree[j]\n",
       "    94                                           #                 del node.decoder_output\n",
       "    95                                           #                 del node.encoder_input\n",
       "    96                                           #                 del node.last_id\n",
       "    97                                           #                 del node\n",
       "    98                                           #                 del score\n",
       "    99                                                           \n",
       "   100                                                               \n",
       "   101        20         30.0      1.5      0.0          if working_nodes == []:\n",
       "   102         1          2.0      2.0      0.0              break\n",
       "   103                                                   \n",
       "   104                                                   # making the padded input from different batches and sequence lengths\n",
       "   105        19        321.0     16.9      0.0          rough_input = [n.decoder_output for (score, n) in working_nodes]\n",
       "   106                                           #         padding_shapes = [t.shape[0] for t in rough_input]\n",
       "   107        19        169.0      8.9      0.0          max_decoder_size = max([t.shape[0] for t in rough_input])\n",
       "   108        19        604.0     31.8      0.0          padded_decoder_input = torch.zeros((max_decoder_size,len(working_nodes)), dtype=torch.long, device=device).fill_(PAD_token)\n",
       "   109                                                   \n",
       "   110                                                   # we create a correctly sized tensor with all <pad> symbols and fill with the according tokens\n",
       "   111       570        674.0      1.2      0.0          for i in range(len(working_nodes)):\n",
       "   112       551        728.0      1.3      0.0              length = rough_input[i].shape[0]\n",
       "   113       551       7259.0     13.2      0.2              padded_decoder_input[:length,i] = rough_input[i].view(-1)\n",
       "   114                                                   \n",
       "   115        19       3529.0    185.7      0.1          encoder_input = torch.cat([n.encoder_input for (score,n) in working_nodes], dim=1)\n",
       "   116                                                   \n",
       "   117        19     174260.0   9171.6      3.6          decoder_predictions = model(encoder_input, padded_decoder_input)\n",
       "   118                                           #         print(padding_shapes)\n",
       "   119                                           #         print(decoder_predictions.shape)\n",
       "   120                                                   \n",
       "   121       570       5707.0     10.0      0.1          for (score, node), logits in zip(working_nodes, decoder_predictions.transpose(0,1)):\n",
       "   122       551       1339.0      2.4      0.0              last_token_pos = node.decoder_output.shape[0] - 1\n",
       "   123       551       1967.0      3.6      0.0              last_token_logits = logits[last_token_pos] \n",
       "   124                                           #             print(last_token_logits)\n",
       "   125       551       8397.0     15.2      0.2              last_token_log_probs = last_token_logits.log_softmax(0)\n",
       "   126       551      33658.0     61.1      0.7              log_probs, indexes = torch.topk(last_token_log_probs, beam_sizes[node.batch_num])\n",
       "   127     11571      89403.0      7.7      1.9              for log_prob, idx in zip(log_probs, indexes):\n",
       "   128                                           #                 print(TGT_TEXT.vocab.itos[idx])\n",
       "   129     11020     262777.0     23.8      5.4                  new_decoder_output = torch.cat([node.decoder_output, idx.view(-1,1)])\n",
       "   130                                           #                 print([TGT_TEXT.vocab.itos[f] for f in new_decoder_output.view(-1)], -(node.logp+log_prob).item())\n",
       "   131                                           #                 print(log_prob)\n",
       "   132                                           #                 print(TGT_TEXT.vocab.itos[idx])\n",
       "   133     11020     167937.0     15.2      3.5                  new_node = BeamSearchNode(new_decoder_output, node.encoder_input, node, idx, node.logp+log_prob, node.leng+1,node.batch_num)\n",
       "   134                                           #                 batch_trees[node.batch_num].put((-(node.logp+log_prob),new_node))\n",
       "   135     11020     703525.0     63.8     14.6                  heappush(batch_trees[node.batch_num], (-(node.logp+log_prob),new_node))\n",
       "   136       551    3255613.0   5908.6     67.4              batch_trees[node.batch_num] = nsmallest(beam_sizes[node.batch_num],batch_trees[node.batch_num])\n",
       "   137                                                   \n",
       "   138                                                  \n",
       "   139         3          4.0      1.3      0.0      for i in range(len(batch_endnodes)):\n",
       "   140         2         80.0     40.0      0.0          batch_endnodes[i] = [n.decoder_output for n in sorted(batch_endnodes[i], key=lambda node: -node.logp)]\n",
       "   141         1          1.0      1.0      0.0      return batch_endnodes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f beam_search.beam_search_decode beam_search.beam_search_decode(model,TGT_TEXT, batch_encoder_ids=src_ids, SOS_token=SOS_token, EOS_token=EOS_token, PAD_token=PAD_token, beam_size=20, max_length=20, num_out=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([[ 2],\n",
      "        [17],\n",
      "        [ 6],\n",
      "        [12],\n",
      "        [ 7],\n",
      "        [ 3]], device='cuda:0')], [tensor([[ 2],\n",
      "        [14],\n",
      "        [12],\n",
      "        [ 5],\n",
      "        [20],\n",
      "        [ 4],\n",
      "        [20],\n",
      "        [ 6],\n",
      "        [12],\n",
      "        [ 7],\n",
      "        [ 3]], device='cuda:0')]]\n",
      "['<sos>', 'def', '(', 'self', ')', '<eos>']\n",
      "\n",
      "['<sos>', 'if', 'self', '.', 'name', '_', 'name', '(', 'self', ')', '<eos>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import beam_search\n",
    "import importlib\n",
    "importlib.reload(beam_search)\n",
    "\n",
    "sent1 = [\"<sos>\"] + SRC_TEXT.preprocess(\"for every log in existing\") + [\"<eos>\"] + [\"<pad>\"]\n",
    "sent2 = [\"<sos>\"] + SRC_TEXT.preprocess(\"for every log in existing ,\") + [\"<eos>\"]\n",
    "src_ids = SRC_TEXT.numericalize([sent1,sent2], device=device)\n",
    "\n",
    "SOS_token = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "EOS_token = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "PAD_token = TGT_TEXT.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "outputs = beam_search.beam_search_decode(model,TGT_TEXT,\n",
    "                              batch_encoder_ids=src_ids,\n",
    "                              SOS_token=SOS_token,\n",
    "                              EOS_token=EOS_token,\n",
    "                              PAD_token=PAD_token,\n",
    "                              beam_size=4,\n",
    "                              max_length=20,\n",
    "                              num_out=1)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "for out in outputs:\n",
    "    for sent in out:\n",
    "        print([TGT_TEXT.vocab.itos[id] for id in sent.view(-1).cpu().tolist()])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2740311596835683"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nltk_bleu(refrence, prediction):\n",
    "    \"\"\"\n",
    "    Implementation from ReCode\n",
    "    and moses multi belu script sets BLEU to 0.0 if len(toks) < 4\n",
    "    \"\"\"\n",
    "    ngram_weights = [0.25] * min(4, len(refrence))\n",
    "    return sentence_bleu([refrence], prediction, weights=ngram_weights, \n",
    "                          smoothing_function=SmoothingFunction().method3)\n",
    "\n",
    "nltk_bleu(np.array([1,2,3,4,5,6]), np.array([1,2,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(beam_size=1):\n",
    "    model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        sources = []\n",
    "        results = []\n",
    "        targets = []\n",
    "        BLEU_scores = []\n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "            encoder_inputs = batch.src\n",
    "            target = batch.tgt\n",
    "            \n",
    "            if beam_size == 1:\n",
    "                predictions = greedy_decode_batch_ids(encoder_inputs, max_seq_length=20)\n",
    "                results += predictions.transpose(0,1).cpu().tolist()\n",
    "            else:\n",
    "                predictions = beam_decode(model, encoder_inputs)\n",
    "                results += [sent[0] for sent in predictions]\n",
    "            \n",
    "            sources += encoder_inputs.transpose(0,1).cpu().tolist()\n",
    "            targets += target.transpose(0,1).cpu().tolist()\n",
    "            if i % 2 == 0:\n",
    "                print(\"| EVALUATION | {:5d}/{:5d} batches |\".format(i, len(valid_iterator)))\n",
    "        \n",
    "        for r_ids, target in zip(results, targets):\n",
    "            eos_id = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "            eos_index = r_ids.index(eos_id) if eos_id in r_ids else None\n",
    "            cut_ids = r_ids[:eos_index]\n",
    "            filtered_ids = [id for id in cut_ids if id not in [0,1,2,3]]\n",
    "            filtered_target_ids = [id for id in target if id not in [0,1,2,3]]\n",
    "            BLEU_scores.append(nltk_bleu(filtered_target_ids, filtered_ids))\n",
    "        \n",
    "        with open(\"out.txt\", \"w\") as out_fp:\n",
    "            for source, result, target, BLEU in zip(sources, results, targets, BLEU_scores):\n",
    "                eos_id = TGT_TEXT.vocab.stoi[\"<eos>\"]\n",
    "                eos_index = result.index(eos_id) if eos_id in result else None\n",
    "                cut_ids = result[:eos_index]\n",
    "                filtered_ids = [id for id in cut_ids if id not in [0,1,2,3]]\n",
    "                filtered_target_ids = [id for id in target if id not in [0,1,2,3]]\n",
    "                filtered_source_ids = [id for id in source if id not in [0,1,2,3]]\n",
    "                \n",
    "                out_fp.write(\"SRC  :\" + \" \".join([SRC_TEXT.vocab.itos[id] for id in filtered_source_ids]) + \"\\n\")\n",
    "                out_fp.write(\"TGT  :\" + \" \".join([TGT_TEXT.vocab.itos[id] for id in filtered_target_ids]) + \"\\n\")\n",
    "                out_fp.write(\"PRED :\" + \" \".join([TGT_TEXT.vocab.itos[id] for id in filtered_ids]) + \"\\n\")\n",
    "                out_fp.write(\"BLEU :\" + str(BLEU) + \"\\n\")\n",
    "                out_fp.write(\"\\n\")\n",
    "            out_fp.write(\"\\n\\n| EVALUATION | BLEU: {:5.2f} |\\n\".format(np.average(BLEU_scores)))\n",
    "                \n",
    "        print(\"| EVALUATION | BLEU: {:5.2f} |\".format(np.average(BLEU_scores)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    tgt_vocab_size = len(TGT_TEXT.vocab.itos)\n",
    "    encoder_input = batch.src\n",
    "    decoder_input = batch.tgt[:-1]\n",
    "    targets = batch.tgt[1:]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(encoder_input, decoder_input)\n",
    "\n",
    "    loss = criterion(output.view(-1, tgt_vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    elapsed = time.time() - start_time\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TGT_TEXT.vocab.stoi['<pad>'])\n",
    "lr = 0.005 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   200/1000000 steps | lr 0.0050 | ms/batch 42.96 | loss  7.11 | ppl  1225.49\n",
      "|   400/1000000 steps | lr 0.0050 | ms/batch 40.49 | loss  5.84 | ppl   344.99\n",
      "|   600/1000000 steps | lr 0.0050 | ms/batch 42.05 | loss  5.66 | ppl   287.82\n",
      "|   800/1000000 steps | lr 0.0050 | ms/batch 42.22 | loss  5.53 | ppl   252.22\n",
      "|  1000/1000000 steps | lr 0.0050 | ms/batch 41.02 | loss  5.42 | ppl   225.42\n",
      "|  1200/1000000 steps | lr 0.0050 | ms/batch 42.30 | loss  5.34 | ppl   209.01\n",
      "|  1400/1000000 steps | lr 0.0050 | ms/batch 41.45 | loss  5.25 | ppl   189.83\n",
      "|  1600/1000000 steps | lr 0.0050 | ms/batch 41.39 | loss  5.20 | ppl   181.06\n",
      "|  1800/1000000 steps | lr 0.0050 | ms/batch 42.85 | loss  5.12 | ppl   167.92\n",
      "|  2000/1000000 steps | lr 0.0050 | ms/batch 41.89 | loss  5.03 | ppl   153.41\n",
      "|  2200/1000000 steps | lr 0.0050 | ms/batch 40.00 | loss  4.99 | ppl   147.23\n",
      "|  2400/1000000 steps | lr 0.0050 | ms/batch 43.35 | loss  4.94 | ppl   139.81\n",
      "|  2600/1000000 steps | lr 0.0050 | ms/batch 42.47 | loss  4.90 | ppl   133.78\n",
      "|  2800/1000000 steps | lr 0.0050 | ms/batch 42.71 | loss  4.86 | ppl   128.55\n",
      "|  3000/1000000 steps | lr 0.0050 | ms/batch 41.40 | loss  4.81 | ppl   122.45\n",
      "|  3200/1000000 steps | lr 0.0050 | ms/batch 42.64 | loss  4.76 | ppl   116.68\n",
      "|  3400/1000000 steps | lr 0.0050 | ms/batch 42.61 | loss  4.74 | ppl   114.42\n",
      "|  3600/1000000 steps | lr 0.0050 | ms/batch 42.33 | loss  4.67 | ppl   106.77\n",
      "|  3800/1000000 steps | lr 0.0050 | ms/batch 41.42 | loss  4.61 | ppl   100.53\n",
      "|  4000/1000000 steps | lr 0.0050 | ms/batch 43.25 | loss  4.63 | ppl   102.90\n",
      "|  4200/1000000 steps | lr 0.0050 | ms/batch 42.69 | loss  4.57 | ppl    96.77\n",
      "|  4400/1000000 steps | lr 0.0050 | ms/batch 41.77 | loss  4.55 | ppl    94.43\n",
      "|  4600/1000000 steps | lr 0.0050 | ms/batch 42.35 | loss  4.51 | ppl    90.54\n",
      "|  4800/1000000 steps | lr 0.0050 | ms/batch 42.34 | loss  4.49 | ppl    89.42\n",
      "|  5000/1000000 steps | lr 0.0050 | ms/batch 42.63 | loss  4.49 | ppl    89.29\n",
      "|  5200/1000000 steps | lr 0.0050 | ms/batch 43.22 | loss  4.43 | ppl    84.04\n",
      "|  5400/1000000 steps | lr 0.0050 | ms/batch 42.18 | loss  4.41 | ppl    82.20\n",
      "|  5600/1000000 steps | lr 0.0050 | ms/batch 42.71 | loss  4.39 | ppl    80.48\n",
      "|  5800/1000000 steps | lr 0.0050 | ms/batch 41.48 | loss  4.34 | ppl    77.09\n",
      "|  6000/1000000 steps | lr 0.0050 | ms/batch 41.27 | loss  4.34 | ppl    76.82\n",
      "|  6200/1000000 steps | lr 0.0050 | ms/batch 42.44 | loss  4.31 | ppl    74.28\n",
      "|  6400/1000000 steps | lr 0.0050 | ms/batch 42.73 | loss  4.29 | ppl    73.06\n",
      "|  6600/1000000 steps | lr 0.0050 | ms/batch 42.62 | loss  4.30 | ppl    73.96\n",
      "|  6800/1000000 steps | lr 0.0050 | ms/batch 41.10 | loss  4.22 | ppl    67.83\n",
      "|  7000/1000000 steps | lr 0.0050 | ms/batch 43.01 | loss  4.23 | ppl    68.53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-de448cca1a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-de448cca1a5b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(steps, log_interval, learning_interval, eval_interval)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3990ec8c4976>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(steps=10000, log_interval=200, learning_interval=4000, eval_interval=1000):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    step = 1\n",
    "    for batch in train_iterator:\n",
    "        loss = train_step(batch)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} steps | '\n",
    "                  'lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    step, steps, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        if step % eval_interval == 0:\n",
    "            print(\"Evaluating model\")\n",
    "            evaluate()\n",
    "            model.train()\n",
    "        \n",
    "        if step % learning_interval == 0:\n",
    "            scheduler.step()\n",
    "        \n",
    "        step += 1\n",
    "        if step >= steps:\n",
    "            print(\"Finished training\")\n",
    "            return\n",
    "\n",
    "train(steps=1000000,eval_interval=8000,log_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./saved_model.pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = len(SRC_TEXT.vocab.itos)\n",
    "tgt_vocab_size = len(TGT_TEXT.vocab.itos)\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, dropout=0.2).to(device) \n",
    "model.load_state_dict(torch.load(\"./saved_model.pytorch\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| EVALUATION |     0/   59 batches |\n",
      "| EVALUATION |     2/   59 batches |\n",
      "| EVALUATION |     4/   59 batches |\n",
      "| EVALUATION |     6/   59 batches |\n",
      "| EVALUATION |     8/   59 batches |\n",
      "| EVALUATION |    10/   59 batches |\n",
      "| EVALUATION |    12/   59 batches |\n",
      "| EVALUATION |    14/   59 batches |\n",
      "| EVALUATION |    16/   59 batches |\n",
      "| EVALUATION |    18/   59 batches |\n",
      "| EVALUATION |    20/   59 batches |\n",
      "| EVALUATION |    22/   59 batches |\n",
      "| EVALUATION |    24/   59 batches |\n",
      "| EVALUATION |    26/   59 batches |\n",
      "| EVALUATION |    28/   59 batches |\n",
      "| EVALUATION |    30/   59 batches |\n",
      "| EVALUATION |    32/   59 batches |\n",
      "| EVALUATION |    34/   59 batches |\n",
      "| EVALUATION |    36/   59 batches |\n",
      "| EVALUATION |    38/   59 batches |\n",
      "| EVALUATION |    40/   59 batches |\n",
      "| EVALUATION |    42/   59 batches |\n",
      "| EVALUATION |    44/   59 batches |\n",
      "| EVALUATION |    46/   59 batches |\n",
      "| EVALUATION |    48/   59 batches |\n",
      "| EVALUATION |    50/   59 batches |\n",
      "| EVALUATION |    52/   59 batches |\n",
      "| EVALUATION |    54/   59 batches |\n",
      "| EVALUATION |    56/   59 batches |\n",
      "| EVALUATION |    58/   59 batches |\n",
      "| EVALUATION | BLEU:  0.16 |\n"
     ]
    }
   ],
   "source": [
    "evaluate(beam_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> substitute args for self . _ _ args . <eos>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([SRC_TEXT.vocab.itos[i] for i in [ 2,21,83,13,10, 4, 5, 5,83, 4, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> self . _'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([TGT_TEXT.vocab.itos[i] for i in [ 2,12,5,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC ids shape: tensor([[  2],\n",
      "        [ 15],\n",
      "        [533],\n",
      "        [ 11],\n",
      "        [ 53],\n",
      "        [  6],\n",
      "        [  3]], device='cuda:0')\n",
      "if if if = None , name , name , "
     ]
    }
   ],
   "source": [
    "def translate(s):\n",
    "    src_ids = SRC_TEXT.numericalize([[\"<sos>\"] + SRC_TEXT.preprocess(s) + [\"<eos>\"]], device=device)\n",
    "#     src_ids = torch.tensor([ [2],[21],[83],[13],[10], [4], [5], [5],[83], [4], [3]], device=device)\n",
    "    print(\"SRC ids shape:\",src_ids)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sos_id = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "#         decoder_input = torch.zeros((1, 1), dtype=torch.long, device=device).fill_(sos_id)\n",
    "        decoder_input = torch.tensor(np.array([ [2]]), device=device)\n",
    "#         print(\"Decoder input shape:\", decoder_input.shape)\n",
    "        \n",
    "        for i in range(10):\n",
    "#             print(\"Decoder input\", decoder_input)\n",
    "            output = model(src_ids, decoder_input)\n",
    "#             print(model.tgt_mask)\n",
    "#             print(\"output:\", output)\n",
    "#             print(\"predicted ids:\", output.argmax(dim=-1))\n",
    "            last_pred = output[-1:].argmax(dim=2)\n",
    "#             decoder_input[i+1][0] = last_pred\n",
    "#             print(\"last pred:\", TGT_TEXT.vocab.itos[last_pred.cpu().numpy()[0][0]], last_pred.cpu().numpy()[0][0])\n",
    "            print(TGT_TEXT.vocab.itos[last_pred.cpu().numpy()[0][0]],'', end = '')\n",
    "            \n",
    "            decoder_input = torch.cat((decoder_input, last_pred))\n",
    "#             print(\"Decoder input\", decoder_input)\n",
    "#             break\n",
    "\n",
    "translate(\"if PY3 is true ,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([torch.tensor([1.0]),torch.tensor([2.0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moses Multi-BLEU perl script returns 0.0 for any sentence less than 4 tokens long.\n",
    "It will be best to use a function by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_moses_multi_bleu([\"this is a test\"], [\"this is a for\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
