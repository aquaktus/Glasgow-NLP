{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchtext\n",
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "import linecache\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 25 15:12:20 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.40       Driver Version: 430.40       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 31%   44C    P2    74W / 250W |   9479MiB / 11019MiB |     11%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 27%   41C    P2    76W / 250W |   8727MiB / 11019MiB |     11%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:60:00.0 Off |                  N/A |\n",
      "| 28%   42C    P2    70W / 250W |  10221MiB / 11019MiB |      8%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:61:00.0 Off |                  N/A |\n",
      "| 36%   47C    P2    61W / 250W |   2241MiB / 11019MiB |     10%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 29%   43C    P2    52W / 250W |   3035MiB / 11019MiB |      6%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 31%   44C    P2    56W / 250W |  10279MiB / 11019MiB |     10%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:DA:00.0 Off |                  N/A |\n",
      "| 27%   39C    P2    68W / 250W |   2609MiB / 11019MiB |     10%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:DB:00.0 Off |                  N/A |\n",
      "| 27%   41C    P2    64W / 250W |   7473MiB / 11019MiB |     10%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exists\n",
      "--2019-10-25 15:12:21--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1382085 (1.3M) [text/plain]\n",
      "Saving to: './datasets/all.desc'\n",
      "\n",
      "./datasets/all.desc 100%[===================>]   1.32M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2019-10-25 15:12:21 (17.9 MB/s) - './datasets/all.desc' saved [1382085/1382085]\n",
      "\n",
      "--2019-10-25 15:12:21--  https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 906732 (885K) [text/plain]\n",
      "Saving to: './datasets/all.code'\n",
      "\n",
      "./datasets/all.code 100%[===================>] 885.48K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2019-10-25 15:12:22 (13.5 MB/s) - './datasets/all.code' saved [906732/906732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"./datasets\")\n",
    "except FileExistsError:\n",
    "    print(\"Directories already exists\")\n",
    "\n",
    "# getting descriptions\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.anno -O ./datasets/all.desc\n",
    "\n",
    "# getting code\n",
    "!wget https://raw.githubusercontent.com/odashi/ase15-django-dataset/master/django/all.code -O ./datasets/all.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a token text encoder\n",
    "An encoder will take a file and a splitting function and return an object able to encode and decode a string. It will also be able to save a vocab file and retrieve from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel_to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'substitute', 'the', 'result', 'for', 'field_type', '.', ')']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (substitute the result for field_type.)\"\n",
    "\n",
    "# looks like code split need parenthesis to be matched in the same string, if not it gives an error...\n",
    "def code_split(s):\n",
    "    return [x.string for x in tokenize(BytesIO(s.encode('utf-8')).readline) if x.string != '' and x.string != \"\\n\" and not x.string.isspace()][1:]\n",
    "\n",
    "print(code_split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['append', 'rel', '_', 'to', 'to', 'string', \"'\", 'ForeignKey', ',', '(', 'subs', '_', '_', 'titute', 'the', 'result', \"'\", 'for', 'field', '_', 'type', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \" append rel_to to string 'ForeignKey, (subs__titute the result' for field_type.\"\n",
    "\n",
    "def string_split(s):\n",
    "    return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(_|\\W)', s))) # this will chunk all code properly by plits strings with quotes\n",
    "#     return list(filter(lambda x: x != '' and x != \"\\n\" and not x.isspace(), re.split('(\\\\\\'.*?\\\\\\'|\\\\\\\".*?\\\\\\\"|_|\\W)', s))) # this keeps the strings intact\n",
    "\n",
    "print(string_split(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_array(src_fp, tgt_fp):\n",
    "    lines = []\n",
    "    with open(src_fp, \"r\") as src_file, open(tgt_fp, \"r\") as tgt_file:\n",
    "        for src, tgt in zip(src_file, tgt_file):\n",
    "            lines.append((src, tgt))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(data, max_seq_length=200, tokenizer=string_split):\n",
    "    return [(src, tgt) for src, tgt in data if len(string_split(src)) <= max_seq_length and len(string_split(tgt)) <= max_seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_dataset(samples, src_field, tgt_field):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples: [(src_string),(tgt_string)]\n",
    "        src/tgt_tokenizer: a func that takes a string and returns an array of strings\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        src_string, tgt_string = sample\n",
    "        examples.append(torchtext.data.Example.fromdict({\"src\":src_string, \"tgt\":tgt_string}, \n",
    "                                        fields={\"src\":(\"src\",src_field), \"tgt\":(\"tgt\",tgt_field)}))\n",
    "        \n",
    "    dataset = torchtext.data.Dataset(examples,fields={\"src\":src_field, \"tgt\":tgt_field})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corpus_to_array(\"datasets/all.desc\", \"datasets/all.code\")\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src length: 586\n",
      "Max tgt length: 1087\n"
     ]
    }
   ],
   "source": [
    "print(\"Max src length:\", max([len(string_split(src)) for src, tgt in data]))\n",
    "print(\"Max tgt length:\", max([len(string_split(tgt)) for src, tgt in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 18805\n",
      "Limited dataset size: 18781\n"
     ]
    }
   ],
   "source": [
    "print(\"Full dataset size:\", len(data))\n",
    "max_seq_length=200\n",
    "data = filter_corpus(data, max_seq_length=200, tokenizer=string_split)\n",
    "print(\"Limited dataset size:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "TGT_TEXT = Field(sequential=True, tokenize=string_split, init_token='<sos>',eos_token='<eos>')\n",
    "\n",
    "dataset = samples_to_dataset(data, SRC_TEXT, TGT_TEXT)\n",
    "\n",
    "train_dataset, val_dataset = dataset.split([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(3) # choose GPU from nvidia-smi \n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call -> 19\n",
      "the -> 7\n",
      "method -> 16\n",
      "self -> 10\n",
      ". -> 4\n",
      "_ -> 5\n",
      "get -> 50\n",
      "_ -> 5\n",
      "size -> 264\n",
      "_ -> 5\n",
      "from -> 32\n",
      "_ -> 5\n",
      "underlying -> 2981\n",
      "_ -> 5\n",
      "file -> 73\n",
      ", -> 6\n",
      "substitute -> 22\n",
      "the -> 7\n",
      "result -> 21\n",
      "for -> 14\n",
      "self -> 10\n",
      ". -> 4\n",
      "_ -> 5\n",
      "size -> 264\n",
      ". -> 4\n"
     ]
    }
   ],
   "source": [
    "SRC_TEXT.build_vocab(train_dataset)\n",
    "TGT_TEXT.build_vocab(train_dataset)\n",
    "\n",
    "\n",
    "sample = dataset[0].src\n",
    "for tok, id in zip(sample, SRC_TEXT.numericalize([sample])):\n",
    "    print(\"{} -> {}\".format(tok, id.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'otherwise', 'if', 'self', '.', '_', 'delegate', '_', 'text', 'is', 'true', ',', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_dataset, val_dataset),\n",
    "    batch_size = batch_size,\n",
    "    sort_key = lambda x: len(x.src)+len(x.tgt),\n",
    "    device = device)\n",
    "\n",
    "# The iterator generates batches with padded length for sequences with similar sizes, a batch is [seq_length, batch_size]\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    print([SRC_TEXT.vocab.itos[id] for id in batch.src.cpu().numpy()[:,4]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample transformer without positional encoding, it uses the built in transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_transformer_model = nn.Transformer() # uses default hyperparameters\n",
    "src = torch.rand((10, 32, 512)) # [src_seq_length, batch_size, embedding_size]\n",
    "tgt = torch.rand((20, 32, 512)) # [tgt_seq_length, batch_size, embedding_size]\n",
    "rand_transformer_model(src, tgt).shape # [tgt_seq_length, batch_size, embedding_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_size=128, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, dropout)\n",
    "        self.src_encoder = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.tgt_encoder = nn.Embedding(tgt_vocab_size, embedding_size)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model=embedding_size, nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024)\n",
    "        self.decoder = nn.Linear(embedding_size, tgt_vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.src_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        self.tgt_mask = self._generate_square_subsequent_mask(len(tgt)).to(device)\n",
    "\n",
    "        src = self.src_encoder(src) * math.sqrt(self.embedding_size)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        tgt = self.tgt_encoder(tgt) * math.sqrt(self.embedding_size)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        output = self.transformer(src, tgt, tgt_mask=self.tgt_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 8, 3894])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = len(SRC_TEXT.vocab.stoi)\n",
    "tgt_vocab_size = len(TGT_TEXT.vocab.stoi)\n",
    "\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size).to(device) \n",
    "src = torch.randint(0, src_vocab_size, (10,8), device=device) # [src_seq_length, batch_size]\n",
    "tgt = torch.randint(0, tgt_vocab_size, (20,8), device=device) # [src_seq_length, batch_size]\n",
    "model(src, tgt).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    tgt_vocab_size = len(TGT_TEXT.vocab.stoi)\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        encoder_input = batch.src\n",
    "        decoder_input = batch.tgt[:-1]\n",
    "        targets = batch.tgt[1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        loss = criterion(output.view(-1, tgt_vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, i, len(train_iterator), scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3],\n",
       "        [3, 3],\n",
       "        [3, 3],\n",
       "        [3, 3]], dtype=torch.int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 4), dtype=torch.int32).fill_(3).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, valid_iterator):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    tgt_vocab_size = len(TGT_TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        results = []\n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "            encoder_input = batch.src\n",
    "            sos_id = TGT_TEXT.vocab.stoi[\"<sos>\"]\n",
    "            decoder_input = torch.zeros((1, batch.tgt.shape[1]), dtype=torch.long, device=device).fill_(sos_id)\n",
    "            targets = batch.tgt[1:]\n",
    "            \n",
    "#             for i in range(max_seq_length):\n",
    "            for i in range(5):\n",
    "                output = eval_model(encoder_input, decoder_input)\n",
    "                last_pred = output[-1:].argmax(dim=2)\n",
    "                # top k sampling\n",
    "                # indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "                # F.softmax()\n",
    "                decoder_input = torch.cat((decoder_input, last_pred))\n",
    "#             print(decoder_input.shape)\n",
    "            for i in range(decoder_input.shape[1]):\n",
    "                src_sent = \" \".join([SRC_TEXT.vocab.itos[id] for id in batch.src.transpose(0,1)[i]])\n",
    "                predicted_sent = \" \".join([TGT_TEXT.vocab.itos[id] for id in decoder_input.transpose(0,1)[i]])\n",
    "                ref_sent = \" \".join([TGT_TEXT.vocab.itos[id] for id in batch.tgt.transpose(0,1)[i]])\n",
    "                BLEU = get_moses_multi_bleu([predicted_sent], [ref_sent])\n",
    "                result = (src_sent, ref_sent, predicted_sent, BLEU)\n",
    "                print(result)\n",
    "                results.append(result)\n",
    "                \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> return nothing . <eos>', '<sos> return <eos> <pad>', '<sos> u003C Y2 u003C Y2 u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to fetch multi-bleu.perl script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', None)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> return result <eos> <pad>', '<sos> return result <eos>', '<sos> G CRITICALS mass G CRITICALS', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> return nothing . <eos>', '<sos> return <eos> <pad>', '<sos> u003C Y2 u003C Y2 u003C', 0.0)\n",
      "('<sos> return . <eos> <pad>', '<sos> return <eos> <pad>', '<sos> u003C hlen CRITICALS CRITICALS CRITICALS', 0.0)\n",
      "('<sos> try <eos> <pad> <pad>', '<sos> try : <eos>', '<sos> mass errorlist mass G CRITICALS', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> return nothing . <eos>', '<sos> return <eos> <pad>', '<sos> u003C Y2 u003C Y2 u003C', 0.0)\n",
      "('<sos> return nothing . <eos>', '<sos> return <eos> <pad>', '<sos> u003C Y2 u003C Y2 u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to fetch multi-bleu.perl script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<sos> return nothing . <eos>', '<sos> return <eos> <pad>', '<sos> u003C Y2 u003C Y2 u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', None)\n",
      "('<sos> return nothing . <eos>', '<sos> return <eos> <pad>', '<sos> u003C Y2 u003C Y2 u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> do nothing . <eos>', '<sos> pass <eos> <pad>', '<sos> u003C u003C u003C u003C u003C', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n",
      "('<sos> try , <eos> <pad>', '<sos> try : <eos>', '<sos> mass dirs RegexValidator dirs errorlist', 0.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1fcdd7acf568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-24d7231612fe>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(eval_model, valid_iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#             for i in range(max_seq_length):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mlast_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;31m# top k sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-967698e6996b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[1;32m    120\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             output = self.layers[i](output, src_mask=mask,\n\u001b[0;32m--> 176\u001b[0;31m                                     src_key_padding_mask=src_key_padding_mask)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"activation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_ = evaluate(model, valid_iterator)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([0.1,0.0,0.9])\n",
    "top_k = top_k_top_p_filtering(logits, top_k=2)\n",
    "probabilities = F.softmax(top_k, dim=-1)\n",
    "torch.multinomial(probabilities, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  411 batches | lr 5.00 | ms/batch 42.83 | loss  3.76 | ppl    43.03\n",
      "| epoch   1 |   400/  411 batches | lr 5.00 | ms/batch 45.00 | loss  2.80 | ppl    16.43\n",
      "######## Finished Epoch 1 #########\n",
      "| epoch   2 |   200/  411 batches | lr 4.75 | ms/batch 41.09 | loss  2.36 | ppl    10.57\n",
      "| epoch   2 |   400/  411 batches | lr 4.75 | ms/batch 40.29 | loss  2.35 | ppl    10.52\n",
      "######## Finished Epoch 2 #########\n",
      "| epoch   3 |   200/  411 batches | lr 4.51 | ms/batch 41.47 | loss  2.33 | ppl    10.25\n",
      "| epoch   3 |   400/  411 batches | lr 4.51 | ms/batch 42.07 | loss  2.22 | ppl     9.16\n",
      "######## Finished Epoch 3 #########\n",
      "| epoch   4 |   200/  411 batches | lr 4.29 | ms/batch 40.98 | loss  2.26 | ppl     9.54\n",
      "| epoch   4 |   400/  411 batches | lr 4.29 | ms/batch 42.03 | loss  2.16 | ppl     8.64\n",
      "######## Finished Epoch 4 #########\n",
      "| epoch   5 |   200/  411 batches | lr 4.07 | ms/batch 39.18 | loss  2.20 | ppl     9.01\n",
      "| epoch   5 |   400/  411 batches | lr 4.07 | ms/batch 46.91 | loss  2.17 | ppl     8.75\n",
      "######## Finished Epoch 5 #########\n",
      "| epoch   6 |   200/  411 batches | lr 3.87 | ms/batch 45.21 | loss  2.11 | ppl     8.24\n",
      "| epoch   6 |   400/  411 batches | lr 3.87 | ms/batch 39.61 | loss  2.18 | ppl     8.84\n",
      "######## Finished Epoch 6 #########\n",
      "| epoch   7 |   200/  411 batches | lr 3.68 | ms/batch 39.35 | loss  2.16 | ppl     8.68\n",
      "| epoch   7 |   400/  411 batches | lr 3.68 | ms/batch 41.44 | loss  2.11 | ppl     8.28\n",
      "######## Finished Epoch 7 #########\n",
      "| epoch   8 |   200/  411 batches | lr 3.49 | ms/batch 39.53 | loss  2.19 | ppl     8.93\n",
      "| epoch   8 |   400/  411 batches | lr 3.49 | ms/batch 39.97 | loss  2.08 | ppl     7.99\n",
      "######## Finished Epoch 8 #########\n",
      "| epoch   9 |   200/  411 batches | lr 3.32 | ms/batch 41.32 | loss  2.16 | ppl     8.69\n",
      "| epoch   9 |   400/  411 batches | lr 3.32 | ms/batch 42.40 | loss  2.12 | ppl     8.36\n",
      "######## Finished Epoch 9 #########\n",
      "| epoch  10 |   200/  411 batches | lr 3.15 | ms/batch 39.14 | loss  2.12 | ppl     8.33\n",
      "| epoch  10 |   400/  411 batches | lr 3.15 | ms/batch 40.47 | loss  2.16 | ppl     8.64\n",
      "######## Finished Epoch 10 #########\n",
      "| epoch  11 |   200/  411 batches | lr 2.99 | ms/batch 40.45 | loss  2.10 | ppl     8.18\n",
      "| epoch  11 |   400/  411 batches | lr 2.99 | ms/batch 42.32 | loss  2.12 | ppl     8.37\n",
      "######## Finished Epoch 11 #########\n",
      "| epoch  12 |   200/  411 batches | lr 2.84 | ms/batch 39.21 | loss  2.14 | ppl     8.52\n",
      "| epoch  12 |   400/  411 batches | lr 2.84 | ms/batch 38.83 | loss  2.08 | ppl     7.98\n",
      "######## Finished Epoch 12 #########\n",
      "| epoch  13 |   200/  411 batches | lr 2.70 | ms/batch 40.48 | loss  2.11 | ppl     8.25\n",
      "| epoch  13 |   400/  411 batches | lr 2.70 | ms/batch 42.86 | loss  2.14 | ppl     8.53\n",
      "######## Finished Epoch 13 #########\n",
      "| epoch  14 |   200/  411 batches | lr 2.57 | ms/batch 40.69 | loss  2.11 | ppl     8.28\n",
      "| epoch  14 |   400/  411 batches | lr 2.57 | ms/batch 41.45 | loss  2.05 | ppl     7.79\n",
      "######## Finished Epoch 14 #########\n",
      "| epoch  15 |   200/  411 batches | lr 2.44 | ms/batch 43.52 | loss  2.11 | ppl     8.22\n",
      "| epoch  15 |   400/  411 batches | lr 2.44 | ms/batch 41.52 | loss  2.03 | ppl     7.64\n",
      "######## Finished Epoch 15 #########\n",
      "| epoch  16 |   200/  411 batches | lr 2.32 | ms/batch 39.09 | loss  2.10 | ppl     8.20\n",
      "| epoch  16 |   400/  411 batches | lr 2.32 | ms/batch 43.82 | loss  2.09 | ppl     8.10\n",
      "######## Finished Epoch 16 #########\n",
      "| epoch  17 |   200/  411 batches | lr 2.20 | ms/batch 41.29 | loss  2.10 | ppl     8.15\n",
      "| epoch  17 |   400/  411 batches | lr 2.20 | ms/batch 39.77 | loss  2.09 | ppl     8.10\n",
      "######## Finished Epoch 17 #########\n",
      "| epoch  18 |   200/  411 batches | lr 2.09 | ms/batch 39.79 | loss  2.07 | ppl     7.90\n",
      "| epoch  18 |   400/  411 batches | lr 2.09 | ms/batch 36.52 | loss  2.15 | ppl     8.59\n",
      "######## Finished Epoch 18 #########\n",
      "| epoch  19 |   200/  411 batches | lr 1.99 | ms/batch 39.14 | loss  2.11 | ppl     8.27\n",
      "| epoch  19 |   400/  411 batches | lr 1.99 | ms/batch 39.29 | loss  2.09 | ppl     8.12\n",
      "######## Finished Epoch 19 #########\n",
      "| epoch  20 |   200/  411 batches | lr 1.89 | ms/batch 40.84 | loss  2.07 | ppl     7.92\n",
      "| epoch  20 |   400/  411 batches | lr 1.89 | ms/batch 40.85 | loss  2.11 | ppl     8.21\n",
      "######## Finished Epoch 20 #########\n",
      "| epoch  21 |   200/  411 batches | lr 1.79 | ms/batch 42.84 | loss  2.05 | ppl     7.76\n",
      "| epoch  21 |   400/  411 batches | lr 1.79 | ms/batch 38.04 | loss  2.08 | ppl     8.00\n",
      "######## Finished Epoch 21 #########\n",
      "| epoch  22 |   200/  411 batches | lr 1.70 | ms/batch 38.72 | loss  2.13 | ppl     8.39\n",
      "| epoch  22 |   400/  411 batches | lr 1.70 | ms/batch 41.21 | loss  2.07 | ppl     7.91\n",
      "######## Finished Epoch 22 #########\n",
      "| epoch  23 |   200/  411 batches | lr 1.62 | ms/batch 40.46 | loss  2.14 | ppl     8.53\n",
      "| epoch  23 |   400/  411 batches | lr 1.62 | ms/batch 42.55 | loss  2.05 | ppl     7.79\n",
      "######## Finished Epoch 23 #########\n",
      "| epoch  24 |   200/  411 batches | lr 1.54 | ms/batch 40.93 | loss  2.09 | ppl     8.07\n",
      "| epoch  24 |   400/  411 batches | lr 1.54 | ms/batch 38.71 | loss  2.07 | ppl     7.93\n",
      "######## Finished Epoch 24 #########\n",
      "| epoch  25 |   200/  411 batches | lr 1.46 | ms/batch 42.83 | loss  2.14 | ppl     8.49\n",
      "| epoch  25 |   400/  411 batches | lr 1.46 | ms/batch 43.04 | loss  2.00 | ppl     7.36\n",
      "######## Finished Epoch 25 #########\n",
      "| epoch  26 |   200/  411 batches | lr 1.39 | ms/batch 40.61 | loss  2.17 | ppl     8.78\n",
      "| epoch  26 |   400/  411 batches | lr 1.39 | ms/batch 39.61 | loss  2.03 | ppl     7.63\n",
      "######## Finished Epoch 26 #########\n",
      "| epoch  27 |   200/  411 batches | lr 1.32 | ms/batch 40.33 | loss  2.08 | ppl     7.98\n",
      "| epoch  27 |   400/  411 batches | lr 1.32 | ms/batch 37.87 | loss  2.11 | ppl     8.21\n",
      "######## Finished Epoch 27 #########\n",
      "| epoch  28 |   200/  411 batches | lr 1.25 | ms/batch 40.02 | loss  2.10 | ppl     8.16\n",
      "| epoch  28 |   400/  411 batches | lr 1.25 | ms/batch 37.78 | loss  2.13 | ppl     8.41\n",
      "######## Finished Epoch 28 #########\n",
      "| epoch  29 |   200/  411 batches | lr 1.19 | ms/batch 40.57 | loss  2.07 | ppl     7.93\n",
      "| epoch  29 |   400/  411 batches | lr 1.19 | ms/batch 39.70 | loss  2.14 | ppl     8.51\n",
      "######## Finished Epoch 29 #########\n",
      "| epoch  30 |   200/  411 batches | lr 1.13 | ms/batch 41.25 | loss  2.09 | ppl     8.05\n",
      "| epoch  30 |   400/  411 batches | lr 1.13 | ms/batch 41.27 | loss  2.05 | ppl     7.78\n",
      "######## Finished Epoch 30 #########\n",
      "| epoch  31 |   200/  411 batches | lr 1.07 | ms/batch 39.50 | loss  2.06 | ppl     7.84\n",
      "| epoch  31 |   400/  411 batches | lr 1.07 | ms/batch 40.00 | loss  2.13 | ppl     8.41\n",
      "######## Finished Epoch 31 #########\n",
      "| epoch  32 |   200/  411 batches | lr 1.02 | ms/batch 41.46 | loss  2.05 | ppl     7.79\n",
      "| epoch  32 |   400/  411 batches | lr 1.02 | ms/batch 41.15 | loss  2.14 | ppl     8.51\n",
      "######## Finished Epoch 32 #########\n",
      "| epoch  33 |   200/  411 batches | lr 0.97 | ms/batch 45.31 | loss  2.03 | ppl     7.60\n",
      "| epoch  33 |   400/  411 batches | lr 0.97 | ms/batch 40.27 | loss  2.07 | ppl     7.96\n",
      "######## Finished Epoch 33 #########\n",
      "| epoch  34 |   200/  411 batches | lr 0.92 | ms/batch 40.93 | loss  2.07 | ppl     7.90\n",
      "| epoch  34 |   400/  411 batches | lr 0.92 | ms/batch 40.15 | loss  2.10 | ppl     8.14\n",
      "######## Finished Epoch 34 #########\n",
      "| epoch  35 |   200/  411 batches | lr 0.87 | ms/batch 40.83 | loss  2.16 | ppl     8.70\n",
      "| epoch  35 |   400/  411 batches | lr 0.87 | ms/batch 42.11 | loss  2.05 | ppl     7.74\n",
      "######## Finished Epoch 35 #########\n",
      "| epoch  36 |   200/  411 batches | lr 0.83 | ms/batch 37.57 | loss  2.10 | ppl     8.20\n",
      "| epoch  36 |   400/  411 batches | lr 0.83 | ms/batch 41.36 | loss  2.08 | ppl     8.01\n",
      "######## Finished Epoch 36 #########\n",
      "| epoch  37 |   200/  411 batches | lr 0.79 | ms/batch 41.74 | loss  2.10 | ppl     8.17\n",
      "| epoch  37 |   400/  411 batches | lr 0.79 | ms/batch 40.23 | loss  2.07 | ppl     7.94\n",
      "######## Finished Epoch 37 #########\n",
      "| epoch  38 |   200/  411 batches | lr 0.75 | ms/batch 38.29 | loss  2.07 | ppl     7.95\n",
      "| epoch  38 |   400/  411 batches | lr 0.75 | ms/batch 42.07 | loss  2.07 | ppl     7.91\n",
      "######## Finished Epoch 38 #########\n",
      "| epoch  39 |   200/  411 batches | lr 0.71 | ms/batch 39.50 | loss  2.12 | ppl     8.29\n",
      "| epoch  39 |   400/  411 batches | lr 0.71 | ms/batch 40.11 | loss  2.06 | ppl     7.88\n",
      "######## Finished Epoch 39 #########\n",
      "| epoch  40 |   200/  411 batches | lr 0.68 | ms/batch 40.74 | loss  2.05 | ppl     7.78\n",
      "| epoch  40 |   400/  411 batches | lr 0.68 | ms/batch 42.54 | loss  2.11 | ppl     8.22\n",
      "######## Finished Epoch 40 #########\n",
      "| epoch  41 |   200/  411 batches | lr 0.64 | ms/batch 45.46 | loss  2.09 | ppl     8.10\n",
      "| epoch  41 |   400/  411 batches | lr 0.64 | ms/batch 40.16 | loss  2.12 | ppl     8.31\n",
      "######## Finished Epoch 41 #########\n",
      "| epoch  42 |   200/  411 batches | lr 0.61 | ms/batch 40.09 | loss  2.10 | ppl     8.19\n",
      "| epoch  42 |   400/  411 batches | lr 0.61 | ms/batch 41.78 | loss  2.08 | ppl     8.04\n",
      "######## Finished Epoch 42 #########\n",
      "| epoch  43 |   200/  411 batches | lr 0.58 | ms/batch 38.52 | loss  2.07 | ppl     7.90\n",
      "| epoch  43 |   400/  411 batches | lr 0.58 | ms/batch 41.52 | loss  2.07 | ppl     7.93\n",
      "######## Finished Epoch 43 #########\n",
      "| epoch  44 |   200/  411 batches | lr 0.55 | ms/batch 43.62 | loss  2.06 | ppl     7.86\n",
      "| epoch  44 |   400/  411 batches | lr 0.55 | ms/batch 39.74 | loss  2.05 | ppl     7.76\n",
      "######## Finished Epoch 44 #########\n",
      "| epoch  45 |   200/  411 batches | lr 0.52 | ms/batch 41.74 | loss  2.12 | ppl     8.33\n",
      "| epoch  45 |   400/  411 batches | lr 0.52 | ms/batch 40.35 | loss  2.08 | ppl     8.03\n",
      "######## Finished Epoch 45 #########\n",
      "| epoch  46 |   200/  411 batches | lr 0.50 | ms/batch 41.97 | loss  2.06 | ppl     7.84\n",
      "| epoch  46 |   400/  411 batches | lr 0.50 | ms/batch 37.71 | loss  2.12 | ppl     8.35\n",
      "######## Finished Epoch 46 #########\n",
      "| epoch  47 |   200/  411 batches | lr 0.47 | ms/batch 41.53 | loss  2.09 | ppl     8.11\n",
      "| epoch  47 |   400/  411 batches | lr 0.47 | ms/batch 39.07 | loss  2.10 | ppl     8.21\n",
      "######## Finished Epoch 47 #########\n",
      "| epoch  48 |   200/  411 batches | lr 0.45 | ms/batch 41.61 | loss  2.10 | ppl     8.13\n",
      "| epoch  48 |   400/  411 batches | lr 0.45 | ms/batch 39.57 | loss  2.13 | ppl     8.41\n",
      "######## Finished Epoch 48 #########\n",
      "| epoch  49 |   200/  411 batches | lr 0.43 | ms/batch 42.72 | loss  2.05 | ppl     7.80\n",
      "| epoch  49 |   400/  411 batches | lr 0.43 | ms/batch 42.28 | loss  2.10 | ppl     8.13\n",
      "######## Finished Epoch 49 #########\n",
      "| epoch  50 |   200/  411 batches | lr 0.40 | ms/batch 41.24 | loss  2.07 | ppl     7.95\n",
      "| epoch  50 |   400/  411 batches | lr 0.40 | ms/batch 42.36 | loss  2.13 | ppl     8.41\n",
      "######## Finished Epoch 50 #########\n",
      "| epoch  51 |   200/  411 batches | lr 0.38 | ms/batch 43.04 | loss  2.08 | ppl     8.03\n",
      "| epoch  51 |   400/  411 batches | lr 0.38 | ms/batch 39.45 | loss  2.06 | ppl     7.85\n",
      "######## Finished Epoch 51 #########\n",
      "| epoch  52 |   200/  411 batches | lr 0.37 | ms/batch 41.27 | loss  2.10 | ppl     8.13\n",
      "| epoch  52 |   400/  411 batches | lr 0.37 | ms/batch 42.66 | loss  2.06 | ppl     7.84\n",
      "######## Finished Epoch 52 #########\n",
      "| epoch  53 |   200/  411 batches | lr 0.35 | ms/batch 41.20 | loss  2.05 | ppl     7.78\n",
      "| epoch  53 |   400/  411 batches | lr 0.35 | ms/batch 41.25 | loss  2.10 | ppl     8.18\n",
      "######## Finished Epoch 53 #########\n",
      "| epoch  54 |   200/  411 batches | lr 0.33 | ms/batch 39.25 | loss  2.12 | ppl     8.31\n",
      "| epoch  54 |   400/  411 batches | lr 0.33 | ms/batch 40.05 | loss  2.04 | ppl     7.67\n",
      "######## Finished Epoch 54 #########\n",
      "| epoch  55 |   200/  411 batches | lr 0.31 | ms/batch 40.94 | loss  2.11 | ppl     8.25\n",
      "| epoch  55 |   400/  411 batches | lr 0.31 | ms/batch 39.94 | loss  2.08 | ppl     8.04\n",
      "######## Finished Epoch 55 #########\n",
      "| epoch  56 |   200/  411 batches | lr 0.30 | ms/batch 42.67 | loss  2.11 | ppl     8.24\n",
      "| epoch  56 |   400/  411 batches | lr 0.30 | ms/batch 38.23 | loss  2.09 | ppl     8.11\n",
      "######## Finished Epoch 56 #########\n",
      "| epoch  57 |   200/  411 batches | lr 0.28 | ms/batch 40.55 | loss  2.09 | ppl     8.06\n",
      "| epoch  57 |   400/  411 batches | lr 0.28 | ms/batch 40.34 | loss  2.11 | ppl     8.24\n",
      "######## Finished Epoch 57 #########\n",
      "| epoch  58 |   200/  411 batches | lr 0.27 | ms/batch 38.42 | loss  2.14 | ppl     8.51\n",
      "| epoch  58 |   400/  411 batches | lr 0.27 | ms/batch 39.41 | loss  2.06 | ppl     7.81\n",
      "######## Finished Epoch 58 #########\n",
      "| epoch  59 |   200/  411 batches | lr 0.26 | ms/batch 40.61 | loss  2.06 | ppl     7.84\n",
      "| epoch  59 |   400/  411 batches | lr 0.26 | ms/batch 40.20 | loss  2.09 | ppl     8.08\n",
      "######## Finished Epoch 59 #########\n",
      "| epoch  60 |   200/  411 batches | lr 0.24 | ms/batch 41.18 | loss  2.02 | ppl     7.55\n",
      "| epoch  60 |   400/  411 batches | lr 0.24 | ms/batch 40.25 | loss  2.09 | ppl     8.12\n",
      "######## Finished Epoch 60 #########\n",
      "| epoch  61 |   200/  411 batches | lr 0.23 | ms/batch 41.99 | loss  2.12 | ppl     8.33\n",
      "| epoch  61 |   400/  411 batches | lr 0.23 | ms/batch 40.48 | loss  2.07 | ppl     7.93\n",
      "######## Finished Epoch 61 #########\n",
      "| epoch  62 |   200/  411 batches | lr 0.22 | ms/batch 40.37 | loss  2.08 | ppl     7.98\n",
      "| epoch  62 |   400/  411 batches | lr 0.22 | ms/batch 41.81 | loss  2.08 | ppl     8.00\n",
      "######## Finished Epoch 62 #########\n",
      "| epoch  63 |   200/  411 batches | lr 0.21 | ms/batch 39.70 | loss  2.11 | ppl     8.23\n",
      "| epoch  63 |   400/  411 batches | lr 0.21 | ms/batch 39.30 | loss  2.07 | ppl     7.90\n",
      "######## Finished Epoch 63 #########\n",
      "| epoch  64 |   200/  411 batches | lr 0.20 | ms/batch 40.22 | loss  2.13 | ppl     8.42\n",
      "| epoch  64 |   400/  411 batches | lr 0.20 | ms/batch 37.67 | loss  2.01 | ppl     7.43\n",
      "######## Finished Epoch 64 #########\n",
      "| epoch  65 |   200/  411 batches | lr 0.19 | ms/batch 39.63 | loss  2.14 | ppl     8.46\n",
      "| epoch  65 |   400/  411 batches | lr 0.19 | ms/batch 40.21 | loss  2.04 | ppl     7.66\n",
      "######## Finished Epoch 65 #########\n",
      "| epoch  66 |   200/  411 batches | lr 0.18 | ms/batch 38.97 | loss  2.09 | ppl     8.12\n",
      "| epoch  66 |   400/  411 batches | lr 0.18 | ms/batch 40.67 | loss  2.08 | ppl     7.97\n",
      "######## Finished Epoch 66 #########\n",
      "| epoch  67 |   200/  411 batches | lr 0.17 | ms/batch 40.44 | loss  2.11 | ppl     8.23\n",
      "| epoch  67 |   400/  411 batches | lr 0.17 | ms/batch 41.51 | loss  2.06 | ppl     7.84\n",
      "######## Finished Epoch 67 #########\n",
      "| epoch  68 |   200/  411 batches | lr 0.16 | ms/batch 40.62 | loss  2.08 | ppl     7.99\n",
      "| epoch  68 |   400/  411 batches | lr 0.16 | ms/batch 40.78 | loss  2.10 | ppl     8.16\n",
      "######## Finished Epoch 68 #########\n",
      "| epoch  69 |   200/  411 batches | lr 0.15 | ms/batch 39.60 | loss  2.11 | ppl     8.23\n",
      "| epoch  69 |   400/  411 batches | lr 0.15 | ms/batch 39.21 | loss  2.08 | ppl     7.99\n",
      "######## Finished Epoch 69 #########\n",
      "| epoch  70 |   200/  411 batches | lr 0.15 | ms/batch 40.42 | loss  2.12 | ppl     8.33\n",
      "| epoch  70 |   400/  411 batches | lr 0.15 | ms/batch 40.63 | loss  2.03 | ppl     7.62\n",
      "######## Finished Epoch 70 #########\n",
      "| epoch  71 |   200/  411 batches | lr 0.14 | ms/batch 40.14 | loss  2.05 | ppl     7.80\n",
      "| epoch  71 |   400/  411 batches | lr 0.14 | ms/batch 39.99 | loss  2.10 | ppl     8.15\n",
      "######## Finished Epoch 71 #########\n",
      "| epoch  72 |   200/  411 batches | lr 0.13 | ms/batch 42.23 | loss  2.16 | ppl     8.67\n",
      "| epoch  72 |   400/  411 batches | lr 0.13 | ms/batch 38.58 | loss  2.03 | ppl     7.64\n",
      "######## Finished Epoch 72 #########\n",
      "| epoch  73 |   200/  411 batches | lr 0.12 | ms/batch 41.86 | loss  2.07 | ppl     7.95\n",
      "| epoch  73 |   400/  411 batches | lr 0.12 | ms/batch 40.15 | loss  2.07 | ppl     7.94\n",
      "######## Finished Epoch 73 #########\n",
      "| epoch  74 |   200/  411 batches | lr 0.12 | ms/batch 41.28 | loss  2.06 | ppl     7.81\n",
      "| epoch  74 |   400/  411 batches | lr 0.12 | ms/batch 41.42 | loss  2.09 | ppl     8.07\n",
      "######## Finished Epoch 74 #########\n",
      "| epoch  75 |   200/  411 batches | lr 0.11 | ms/batch 41.50 | loss  2.03 | ppl     7.63\n",
      "| epoch  75 |   400/  411 batches | lr 0.11 | ms/batch 43.71 | loss  2.12 | ppl     8.31\n",
      "######## Finished Epoch 75 #########\n",
      "| epoch  76 |   200/  411 batches | lr 0.11 | ms/batch 42.47 | loss  2.05 | ppl     7.78\n",
      "| epoch  76 |   400/  411 batches | lr 0.11 | ms/batch 39.18 | loss  2.13 | ppl     8.44\n",
      "######## Finished Epoch 76 #########\n",
      "| epoch  77 |   200/  411 batches | lr 0.10 | ms/batch 37.65 | loss  2.07 | ppl     7.96\n",
      "| epoch  77 |   400/  411 batches | lr 0.10 | ms/batch 39.60 | loss  2.07 | ppl     7.90\n",
      "######## Finished Epoch 77 #########\n",
      "| epoch  78 |   200/  411 batches | lr 0.10 | ms/batch 40.10 | loss  2.09 | ppl     8.09\n",
      "| epoch  78 |   400/  411 batches | lr 0.10 | ms/batch 39.02 | loss  2.08 | ppl     7.98\n",
      "######## Finished Epoch 78 #########\n",
      "| epoch  79 |   200/  411 batches | lr 0.09 | ms/batch 40.01 | loss  2.08 | ppl     7.98\n",
      "| epoch  79 |   400/  411 batches | lr 0.09 | ms/batch 38.05 | loss  2.08 | ppl     8.02\n",
      "######## Finished Epoch 79 #########\n",
      "| epoch  80 |   200/  411 batches | lr 0.09 | ms/batch 41.58 | loss  2.08 | ppl     8.00\n",
      "| epoch  80 |   400/  411 batches | lr 0.09 | ms/batch 38.72 | loss  2.05 | ppl     7.75\n",
      "######## Finished Epoch 80 #########\n",
      "| epoch  81 |   200/  411 batches | lr 0.08 | ms/batch 39.30 | loss  2.13 | ppl     8.40\n",
      "| epoch  81 |   400/  411 batches | lr 0.08 | ms/batch 41.25 | loss  2.00 | ppl     7.41\n",
      "######## Finished Epoch 81 #########\n",
      "| epoch  82 |   200/  411 batches | lr 0.08 | ms/batch 40.78 | loss  2.09 | ppl     8.05\n",
      "| epoch  82 |   400/  411 batches | lr 0.08 | ms/batch 39.73 | loss  2.08 | ppl     7.97\n",
      "######## Finished Epoch 82 #########\n",
      "| epoch  83 |   200/  411 batches | lr 0.07 | ms/batch 42.51 | loss  2.10 | ppl     8.20\n",
      "| epoch  83 |   400/  411 batches | lr 0.07 | ms/batch 40.28 | loss  2.09 | ppl     8.05\n",
      "######## Finished Epoch 83 #########\n",
      "| epoch  84 |   200/  411 batches | lr 0.07 | ms/batch 42.03 | loss  2.14 | ppl     8.52\n",
      "| epoch  84 |   400/  411 batches | lr 0.07 | ms/batch 42.76 | loss  2.03 | ppl     7.61\n",
      "######## Finished Epoch 84 #########\n",
      "| epoch  85 |   200/  411 batches | lr 0.07 | ms/batch 40.83 | loss  2.11 | ppl     8.26\n",
      "| epoch  85 |   400/  411 batches | lr 0.07 | ms/batch 40.25 | loss  2.07 | ppl     7.90\n",
      "######## Finished Epoch 85 #########\n",
      "| epoch  86 |   200/  411 batches | lr 0.06 | ms/batch 41.00 | loss  2.11 | ppl     8.26\n",
      "| epoch  86 |   400/  411 batches | lr 0.06 | ms/batch 40.43 | loss  2.09 | ppl     8.12\n",
      "######## Finished Epoch 86 #########\n",
      "| epoch  87 |   200/  411 batches | lr 0.06 | ms/batch 42.23 | loss  2.09 | ppl     8.08\n",
      "| epoch  87 |   400/  411 batches | lr 0.06 | ms/batch 41.31 | loss  2.10 | ppl     8.17\n",
      "######## Finished Epoch 87 #########\n",
      "| epoch  88 |   200/  411 batches | lr 0.06 | ms/batch 42.78 | loss  2.12 | ppl     8.35\n",
      "| epoch  88 |   400/  411 batches | lr 0.06 | ms/batch 39.53 | loss  2.02 | ppl     7.58\n",
      "######## Finished Epoch 88 #########\n",
      "| epoch  89 |   200/  411 batches | lr 0.05 | ms/batch 38.79 | loss  2.09 | ppl     8.06\n",
      "| epoch  89 |   400/  411 batches | lr 0.05 | ms/batch 39.03 | loss  2.09 | ppl     8.08\n",
      "######## Finished Epoch 89 #########\n",
      "| epoch  90 |   200/  411 batches | lr 0.05 | ms/batch 39.05 | loss  2.15 | ppl     8.58\n",
      "| epoch  90 |   400/  411 batches | lr 0.05 | ms/batch 40.75 | loss  2.04 | ppl     7.69\n",
      "######## Finished Epoch 90 #########\n",
      "| epoch  91 |   200/  411 batches | lr 0.05 | ms/batch 41.12 | loss  2.09 | ppl     8.06\n",
      "| epoch  91 |   400/  411 batches | lr 0.05 | ms/batch 38.79 | loss  2.14 | ppl     8.46\n",
      "######## Finished Epoch 91 #########\n",
      "| epoch  92 |   200/  411 batches | lr 0.05 | ms/batch 42.67 | loss  2.06 | ppl     7.87\n",
      "| epoch  92 |   400/  411 batches | lr 0.05 | ms/batch 41.92 | loss  2.06 | ppl     7.81\n",
      "######## Finished Epoch 92 #########\n",
      "| epoch  93 |   200/  411 batches | lr 0.04 | ms/batch 43.21 | loss  2.14 | ppl     8.48\n",
      "| epoch  93 |   400/  411 batches | lr 0.04 | ms/batch 40.43 | loss  2.07 | ppl     7.95\n",
      "######## Finished Epoch 93 #########\n",
      "| epoch  94 |   200/  411 batches | lr 0.04 | ms/batch 39.96 | loss  2.06 | ppl     7.87\n",
      "| epoch  94 |   400/  411 batches | lr 0.04 | ms/batch 41.61 | loss  2.06 | ppl     7.86\n",
      "######## Finished Epoch 94 #########\n",
      "| epoch  95 |   200/  411 batches | lr 0.04 | ms/batch 41.21 | loss  2.07 | ppl     7.96\n",
      "| epoch  95 |   400/  411 batches | lr 0.04 | ms/batch 43.13 | loss  2.11 | ppl     8.25\n",
      "######## Finished Epoch 95 #########\n",
      "| epoch  96 |   200/  411 batches | lr 0.04 | ms/batch 39.21 | loss  2.02 | ppl     7.57\n",
      "| epoch  96 |   400/  411 batches | lr 0.04 | ms/batch 39.34 | loss  2.14 | ppl     8.48\n",
      "######## Finished Epoch 96 #########\n",
      "| epoch  97 |   200/  411 batches | lr 0.04 | ms/batch 39.75 | loss  2.11 | ppl     8.28\n",
      "| epoch  97 |   400/  411 batches | lr 0.04 | ms/batch 39.68 | loss  2.09 | ppl     8.09\n",
      "######## Finished Epoch 97 #########\n",
      "| epoch  98 |   200/  411 batches | lr 0.03 | ms/batch 39.72 | loss  2.14 | ppl     8.49\n",
      "| epoch  98 |   400/  411 batches | lr 0.03 | ms/batch 39.88 | loss  2.03 | ppl     7.63\n",
      "######## Finished Epoch 98 #########\n",
      "| epoch  99 |   200/  411 batches | lr 0.03 | ms/batch 38.50 | loss  2.12 | ppl     8.30\n",
      "| epoch  99 |   400/  411 batches | lr 0.03 | ms/batch 40.95 | loss  2.08 | ppl     8.02\n",
      "######## Finished Epoch 99 #########\n",
      "| epoch 100 |   200/  411 batches | lr 0.03 | ms/batch 39.00 | loss  2.09 | ppl     8.08\n",
      "| epoch 100 |   400/  411 batches | lr 0.03 | ms/batch 38.94 | loss  2.11 | ppl     8.25\n",
      "######## Finished Epoch 100 #########\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    print(\"######## Finished Epoch {} #########\".format(epoch))\n",
    "#     val_loss = evaluate(model, val_data)\n",
    "#     print('-' * 89)\n",
    "#     print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "#           'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "#                                      val_loss, math.exp(val_loss)))\n",
    "#     print('-' * 89)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
