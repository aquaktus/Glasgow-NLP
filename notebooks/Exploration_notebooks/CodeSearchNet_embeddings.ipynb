{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from src.Self_attention_sequence_encoder import SelfAttentionEmbedder\n",
    "from src.vocab_classes import BPE_Code_vocab\n",
    "from src.trainers import Model_Trainer \n",
    "from src.useful_utils import load_CSN_data\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import tqdm.notebook as tqdm \n",
    "import numpy as np\n",
    "from src.dataset_loaders import DummyDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbec010b3c04d1792fc5c1e3c1c52e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8313405d7abe4642b1974c9226a89cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212726b7966b4770868453d46c80a0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CodeSearchNet_data_path = \"/nfs/code_search_net_archive/python/final/jsonl/\"\n",
    "train_data, valid_data, test_data = load_CSN_data(CodeSearchNet_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = []\n",
    "for sample in train_data:\n",
    "    query = \" \".join(sample[\"docstring_tokens\"])\n",
    "    doc = sample[\"code\"].replace(sample[\"docstring\"], \"\")\n",
    "    train_pairs.append((query, doc))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\"datasets/code_search_net/code_bpe_hugging_32k-vocab.json\",\n",
    "                                  \"datasets/code_search_net/code_bpe_hugging_32k-merges.txt\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e1f49a524d4c47ba8429d0242190fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=412178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cutoff = 500\n",
    "trunk_train_pairs = [(q,d) for q,d in tqdm.tqdm(train_pairs) if len(tokenizer.encode(d).ids)<cutoff and len(tokenizer.encode(q).ids)<cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = BPE_Code_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelfAttentionEmbedder(vocab_size=vocab.vocab_size, embed_dim=128, att_heads=8, layers=3, dim_feedforward=512, loss_type=\"cosine\")\n",
    "model.init_train_params(lr=0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9869d4bd5edd43b4a1808184a895eb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=379882.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-23609e6e3faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata2dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunk_train_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/src/Self_attention_sequence_encoder.py\u001b[0m in \u001b[0;36mdata2dataset\u001b[0;34m(self, data, vocab)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mquery_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEOS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mdoc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEOS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             example = torchtext.data.Example.fromdict({\"query\":query_ids, \n",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/src/vocab_classes.py\u001b[0m in \u001b[0;36mencode_input\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOOVs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tokenizers/implementations/base_tokenizer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sequence, pair)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mAn\u001b[0m \u001b[0mEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEncoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = model.data2dataset(trunk_train_pairs, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = BucketIterator(\n",
    "    dataset,\n",
    "    batch_size = 32,\n",
    "    repeat=True,\n",
    "    shuffle=True,\n",
    "    device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-11.4993, -13.5580, -11.1866,  ..., -12.2552, -15.6091, -14.0092],\n",
       "        [ -8.0851, -10.3684, -18.2105,  ..., -20.0531, -16.1658,  -9.4214],\n",
       "        [  1.3219,  -2.1274,  -8.4821,  ...,  -0.9855,  -7.7024,  -3.7673],\n",
       "        ...,\n",
       "        [-11.6607, -15.2321,  -7.0216,  ..., -14.3235, -12.8851,  -7.8668],\n",
       "        [-15.7496, -19.4936, -18.1946,  ..., -22.8966, -20.5250, -16.4565],\n",
       "        [-13.0613, -13.1255, -17.7572,  ..., -16.3334, -12.3127, -12.7974]],\n",
       "       device='cuda:0', grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(it)\n",
    "q_vec, d_vec = model(batch.query, batch.doc)\n",
    "matrix = model.softmax_matrix(q_vec, d_vec)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "        11, 11, 11,  3, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(matrix, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0025, 0.3133])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_input = torch.tensor([[-1.,5],\n",
    "                          [1,0]])\n",
    "tmp_labels = torch.tensor([1,0])\n",
    "model.cross_entropy(tmp_input, tmp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'output_dir' not defined, training and model outputs won't be saved.\n"
     ]
    }
   ],
   "source": [
    "trainer = Model_Trainer(model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b5aae9576140249de55a0e4a1ee269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_logs = trainer.train(model, train_iterator, 500000, save_interval=10000000, log_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1415, 0.0880],\n",
      "        [0.1546, 0.0568]], device='cuda:0', grad_fn=<MmBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "q_vects, d_vects = model(batch.query,batch.doc)\n",
    "model.eval()\n",
    "outputs = model.cosine_matrix(q_vects, d_vects)\n",
    "model.train()\n",
    "plt.imshow(outputs.tolist(), cmap=\"gray\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the same loss function as CodeSearchNet in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_query_representations = tf.constant([[-1,3.],[3,4],[5,6]])\n",
    "tf_code_representations = tf.constant([[1,2.],[3,4],[5,6]])\n",
    "margin = 1.\n",
    "\n",
    "tf_query_norms = tf.norm(tf_query_representations, axis=-1, keepdims=True) + 1e-10\n",
    "tf_code_norms = tf.norm(tf_code_representations, axis=-1, keepdims=True) + 1e-10\n",
    "\n",
    "tf_cosine_similarities = tf.matmul(tf_query_representations / tf_query_norms,\n",
    "                                tf_code_representations / tf_code_norms,\n",
    "                                transpose_a=False,\n",
    "                                transpose_b=True,\n",
    "                                name='code_query_cooccurrence_logits',\n",
    "                                )  # B x B\n",
    "tf_similarity_scores = tf_cosine_similarities\n",
    "\n",
    "# A max-margin-like loss, but do not penalize negative cosine similarities.\n",
    "tf_neg_matrix = tf.linalg.diag(tf.fill(dims=[tf.shape(tf_cosine_similarities)[0]], value=float('-inf')))\n",
    "tf_per_sample_loss = tf.maximum(0., margin\n",
    "                                 - tf.linalg.diag_part(tf_cosine_similarities)\n",
    "                                 + tf.reduce_max(tf.nn.relu(tf_cosine_similarities + tf_neg_matrix),\n",
    "                                                 axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=36, shape=(3,), dtype=float32, numpy=array([0.86210316, 0.9986877 , 0.9986877 ], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_per_sample_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_representations = torch.tensor([[-1,3.],[3,4],[5,6]])\n",
    "code_representations = torch.tensor([[1,2.],[3,4],[5,6]])\n",
    "margin = 1.\n",
    "\n",
    "query_norm = torch.norm(query_representations, dim=-1, keepdim=True) + 1e-10\n",
    "code_norm = torch.norm(code_representations, dim=-1, keepdim=True) + 1e-10\n",
    "\n",
    "# query_vector = query_vector/query_norm\n",
    "# doc_vector = doc_vector/doc_norm\n",
    "# batch_size = query_vector.shape[0]\n",
    "\n",
    "cosine_similarities = torch.mm(torch.div(query_representations,query_norm),\n",
    "                                torch.div(code_representations,code_norm).T\n",
    "                                )\n",
    "\n",
    "neg_matrix = torch.diag(torch.full((cosine_similarities.shape[0],), float('-inf')))\n",
    "\n",
    "good_sample_loss = torch.diagonal(cosine_similarities)\n",
    "bad_sample_loss = torch.max(cosine_similarities + neg_matrix, dim=-1)[0]\n",
    "\n",
    "\n",
    "per_sample_loss = torch.clamp(margin - good_sample_loss + bad_sample_loss, min=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8621, 0.9987, 0.9987])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_sample_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF CodeSearchNet softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_logits = tf.matmul(tf_query_representations,\n",
    "                   tf_code_representations,\n",
    "                   transpose_a=False,\n",
    "                   transpose_b=True,\n",
    "                   name='code_query_cooccurrence_logits',\n",
    "                   )  # B x B\n",
    "\n",
    "similarity_scores = tf_logits\n",
    "\n",
    "tf_per_sample_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=tf.range(tf.shape(tf_code_representations)[0]),  # [0, 1, 2, 3, ..., n]\n",
    "    logits=tf_logits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=575, shape=(3,), dtype=int32, numpy=array([0, 1, 2], dtype=int32)>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(tf.shape(tf_code_representations)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=566, shape=(3,), dtype=float32, numpy=array([ 8.018479, 14.000001,  0.      ], dtype=float32)>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_per_sample_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.mm(query_representations, code_representations.T)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "labels = torch.arange(0,code_representations.shape[0])\n",
    "per_sample_loss = criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.0185, 14.0000, -0.0000])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_sample_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  9., 13.],\n",
       "        [11., 25., 39.],\n",
       "        [17., 39., 61.]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I like carrots and onions', 'he likes carrots and onions'),\n",
       " ('I like peppers and carrots', 'he likes peppers and carrots'),\n",
       " ('I like banana and toast', 'he likes banana and toast')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_sampler = DummyDataset()\n",
    "query_doc_pairs = dummy_sampler.document_pairs(20)\n",
    "query_doc_pairs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = BPE_Code_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = SelfAttentionEmbedder(vocab_size=vocab.vocab_size, embed_dim=128, att_heads=8, layers=3, dim_feedforward=512, loss_type=\"cosine\")\n",
    "model.init_train_params(lr=0.2)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afc0b858eb94862bb0d8f6296cf91c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = model.data2dataset(query_doc_pairs, vocab)\n",
    "train_iterator = BucketIterator(\n",
    "    dataset,\n",
    "    batch_size = 4,\n",
    "    repeat=True,\n",
    "    shuffle=True,\n",
    "    device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'output_dir' not defined, training and model outputs won't be saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9707d9ec5c43b58329fea1065e85d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c4e187d1bf73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_Trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/src/trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, iterator, steps, log_interval, eval_interval, save_interval, learning_interval)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/src/Self_attention_sequence_encoder.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mquery_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/src/Self_attention_sequence_encoder.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, query_representations, code_representations)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mcosine_similarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_representations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mneg_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/src/Self_attention_sequence_encoder.py\u001b[0m in \u001b[0;36mcosine_matrix\u001b[0;34m(self, query_representations, code_representations)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcosine_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_representations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mquery_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mcode_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Model_Trainer(model, vocab)\n",
    "train_logs = trainer.train(model, train_iterator, 10000, save_interval=10000000, log_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, d = ('I like carrots and onions', 'he likes banana and toast')\n",
    "q_ids = torch.tensor(vocab.encode_input(q)[0]).unsqueeze(1).to(\"cuda\")\n",
    "d_ids = torch.tensor(vocab.encode_input(d)[0]).unsqueeze(1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32000, 32000, 32000, 32000],\n",
      "        [   40,    40,    40,    40],\n",
      "        [ 2183,  2183,  2183,  2183],\n",
      "        [ 1350, 18992,  1350,  7614],\n",
      "        [  364,  5939,   364,   299],\n",
      "        [  387,   387,   387,   764],\n",
      "        [  813,  7614,   813,   387],\n",
      "        [  368,   299,   368,   554],\n",
      "        [32001,   764, 32001,  2112],\n",
      "        [32002, 32001, 32002, 32001]], device='cuda:0') tensor([[32000, 32000, 32000, 32000],\n",
      "        [  276,   276,   276,   276],\n",
      "        [ 2183,  2183,  2183,  2183],\n",
      "        [   82,    82,    82,    82],\n",
      "        [ 1350, 18992,  1350,  7614],\n",
      "        [  364,  5939,   364,   299],\n",
      "        [  387,   387,   387,   764],\n",
      "        [  813,  7614,   813,   387],\n",
      "        [  368,   299,   368,   554],\n",
      "        [32001,   764, 32001,  2112],\n",
      "        [32002, 32001, 32002, 32001]], device='cuda:0')\n",
      "tensor([[0.9469, 0.9498, 0.9344, 0.9440],\n",
      "        [0.9475, 0.9425, 0.9382, 0.9466],\n",
      "        [0.9370, 0.9360, 0.9267, 0.9388],\n",
      "        [0.9348, 0.9376, 0.9196, 0.9319]], device='cuda:0',\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "q_vects, d_vects = model(batch.query,batch.doc)\n",
    "model.eval()\n",
    "outputs = model.cosine_matrix(q_vects, d_vects)\n",
    "model.train()\n",
    "plt.imshow(outputs.tolist(), cmap=\"gray\")\n",
    "print(batch.query,batch.doc)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the CodeSearchNet dataset and subtask class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset_loaders import CodeSearchNetDataset\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a1c5447eaf47ce8dbcf9b90f59c25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4d03e099874bccbf888cc2dde719f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a085f448e0406eb0dff75163306441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CSN_datas_object = CodeSearchNetDataset()\n",
    "train_pairs, valid_pairs, test_pairs = CSN_datas_object.get_data(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ast.Module at 0x7fa678db3c18>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.parse(CSN_datas_object.lookup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    \\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSN_datas_object.lookup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
