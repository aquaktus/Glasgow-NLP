{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting too and from ASTs using TranX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import sys\n",
    "sys.path.insert(0,\"src/external_repos/tranX\")\n",
    "from asdl.asdl import ASDLGrammar\n",
    "from asdl.lang.py.py_asdl_helper import *\n",
    "from asdl.lang.py3.py3_transition_system import *\n",
    "from asdl.hypothesis import *\n",
    "import astor\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String representation of the ASDL AST: \n",
      "(Module (stmt*-body (FunctionDef (identifier-name fun) (arguments-args (arguments (arg*-args) (arg?-vararg) (arg*-kwonlyargs) (expr*-kw_defaults) (arg?-kwarg) (expr*-defaults))) (stmt*-body (Expr (expr-value (Call (expr-func (Name (identifier-id print) (expr_context-ctx (Load)))) (expr*-args (Name (identifier-id a) (expr_context-ctx (Load)))) (keyword*-keywords))))) (expr*-decorator_list) (expr?-returns)) (Expr (expr-value (Call (expr-func (Name (identifier-id print) (expr_context-ctx (Load)))) (expr*-args (Str (string-s foo_bar))) (keyword*-keywords))))))\n",
      "Size of the AST: 19\n",
      "t=1, Action=ApplyRule[mod -> Module(stmt* body)]\n",
      "t=2, Action=ApplyRule[stmt -> FunctionDef(identifier name, arguments args, stmt* body, expr* decorator_list, expr? returns)]\n",
      "t=3, Action=GenToken[fun]\n",
      "t=4, Action=ApplyRule[arguments -> arguments(arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults, arg? kwarg, expr* defaults)]\n",
      "t=5, Action=Reduce\n",
      "t=6, Action=Reduce\n",
      "t=7, Action=Reduce\n",
      "t=8, Action=Reduce\n",
      "t=9, Action=Reduce\n",
      "t=10, Action=Reduce\n",
      "t=11, Action=ApplyRule[stmt -> Expr(expr value)]\n",
      "t=12, Action=ApplyRule[expr -> Call(expr func, expr* args, keyword* keywords)]\n",
      "t=13, Action=ApplyRule[expr -> Name(identifier id, expr_context ctx)]\n",
      "t=14, Action=GenToken[print]\n",
      "t=15, Action=ApplyRule[expr_context -> Load()]\n",
      "t=16, Action=ApplyRule[expr -> Name(identifier id, expr_context ctx)]\n",
      "t=17, Action=GenToken[a]\n",
      "t=18, Action=ApplyRule[expr_context -> Load()]\n",
      "t=19, Action=Reduce\n",
      "t=20, Action=Reduce\n",
      "t=21, Action=Reduce\n",
      "t=22, Action=Reduce\n",
      "t=23, Action=Reduce\n",
      "t=24, Action=ApplyRule[stmt -> Expr(expr value)]\n",
      "t=25, Action=ApplyRule[expr -> Call(expr func, expr* args, keyword* keywords)]\n",
      "t=26, Action=ApplyRule[expr -> Name(identifier id, expr_context ctx)]\n",
      "t=27, Action=GenToken[print]\n",
      "t=28, Action=ApplyRule[expr_context -> Load()]\n",
      "t=29, Action=ApplyRule[expr -> Str(string s)]\n",
      "t=30, Action=GenToken[foo_bar]\n",
      "t=31, Action=GenToken[</primitive>]\n",
      "t=32, Action=Reduce\n",
      "t=33, Action=Reduce\n",
      "t=34, Action=Reduce\n",
      "def fun():\n",
      "    print(a)\n",
      "\n",
      "\n",
      "print('foo_bar')\n",
      "def fun():\n",
      "    print(a)\n",
      "\n",
      "\n",
      "print('foo_bar')\n",
      "def fun():\n",
      "    print(a)\n",
      "\n",
      "\n",
      "print('foo_bar')\n"
     ]
    }
   ],
   "source": [
    "asdl_text = open('src/external_repos/tranX/asdl/lang/py3/py3_asdl.txt').read()\n",
    "grammar = ASDLGrammar.from_text(asdl_text)\n",
    "\n",
    "py_code = \"\"\"\n",
    "def fun():\n",
    "    print(a)\n",
    "print(\"foo_bar\")\n",
    "\"\"\"\n",
    "\n",
    "# get the (domain-specific) python AST of the example Python code snippet\n",
    "py_ast = ast.parse(py_code)\n",
    "\n",
    "# convert the python AST into general-purpose ASDL AST used by tranX\n",
    "asdl_ast = python_ast_to_asdl_ast(py_ast, grammar)\n",
    "print('String representation of the ASDL AST: \\n%s' % asdl_ast.to_string())\n",
    "print('Size of the AST: %d' % asdl_ast.size)\n",
    "\n",
    "# we can also convert the ASDL AST back into Python AST\n",
    "py_ast_reconstructed = asdl_ast_to_python_ast(asdl_ast, grammar)\n",
    "\n",
    "# initialize the Python transition parser\n",
    "parser = Python3TransitionSystem(grammar)\n",
    "\n",
    "# get the sequence of gold-standard actions to construct the ASDL AST\n",
    "actions = parser.get_actions(asdl_ast)\n",
    "\n",
    "# a hypothesis is an (partial) ASDL AST generated using a sequence of tree-construction actions\n",
    "hypothesis = Hypothesis()\n",
    "for t, action in enumerate(actions, 1):\n",
    "    # the type of the action should belong to one of the valid continuing types\n",
    "    # of the transition system\n",
    "    assert action.__class__ in parser.get_valid_continuation_types(hypothesis)\n",
    "\n",
    "    # if it's an ApplyRule action, the production rule should belong to the\n",
    "    # set of rules with the same LHS type as the current rule\n",
    "    if isinstance(action, ApplyRuleAction) and hypothesis.frontier_node:\n",
    "        assert action.production in grammar[hypothesis.frontier_field.type]\n",
    "\n",
    "    print('t=%d, Action=%s' % (t, action))\n",
    "    hypothesis.apply_action(action)\n",
    "\n",
    "# get the surface code snippets from the original Python AST,\n",
    "# the reconstructed AST and the AST generated using actions\n",
    "# they should be the same\n",
    "src1 = astor.to_source(py_ast).strip()\n",
    "src2 = astor.to_source(py_ast_reconstructed).strip()\n",
    "src3 = astor.to_source(asdl_ast_to_python_ast(hypothesis.tree, grammar)).strip()\n",
    "\n",
    "print(src1)\n",
    "print(src2)\n",
    "print(src3)\n",
    "# assert src1 == src2 == src3 == \"6 + 4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_ast.body[0].args.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['body']\n",
      "[<_ast.FunctionDef object at 0x7f7ba1237c18>, <_ast.Expr object at 0x7f7ba1237a58>]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0f756b68df15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_ast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-0f756b68df15>\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_ast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-0f756b68df15>\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute '_fields'"
     ]
    }
   ],
   "source": [
    "def visit(node):\n",
    "    fields = node._fields\n",
    "    print(list(fields))\n",
    "    for field in fields:\n",
    "        attr = getattr(node, field)\n",
    "        print(attr)\n",
    "        visit(attr)\n",
    "    \n",
    "visit(py_ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modifying a tree into various sequences\n",
    "#### Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_code = \"\"\"\n",
    "def fun(a=5, b=5):\n",
    "    print(a)\n",
    "print(\"foo_bar\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full reconstructed code:\n",
      "def fun(a=5, b=5):\n",
      "    print(a)\n",
      "\n",
      "\n",
      "print('foo_bar')\n"
     ]
    }
   ],
   "source": [
    "py_ast = ast.parse(py_code)\n",
    "asdl_ast = python_ast_to_asdl_ast(py_ast, grammar)\n",
    "actions = parser.get_actions(asdl_ast)\n",
    "hypothesis = Hypothesis()\n",
    "for t, action in enumerate(actions, 1):\n",
    "    assert action.__class__ in parser.get_valid_continuation_types(hypothesis)\n",
    "    if isinstance(action, ApplyRuleAction) and hypothesis.frontier_node:\n",
    "        assert action.production in grammar[hypothesis.frontier_field.type]\n",
    "    hypothesis.apply_action(action)\n",
    "\n",
    "print(\"Full reconstructed code:\")\n",
    "print(astor.to_source(asdl_ast_to_python_ast(hypothesis.tree, grammar)).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying the Python AST to remove a particular section\n",
    "We will set the amount of arguments to none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_sub_tree = copy.deepcopy(py_ast.body[0].args)\n",
    "py_ast.body[0].args.args = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_ast.arg at 0x7f7ba08e4780>, <_ast.arg at 0x7f7ba08e49b0>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_sub_tree.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full reconstructed code:\n",
      "def fun():\n",
      "    print(a)\n",
      "\n",
      "\n",
      "print('foo_bar')\n"
     ]
    }
   ],
   "source": [
    "asdl_ast = python_ast_to_asdl_ast(py_ast, grammar)\n",
    "actions = parser.get_actions(asdl_ast)\n",
    "hypothesis = Hypothesis()\n",
    "for t, action in enumerate(actions, 1):\n",
    "    assert action.__class__ in parser.get_valid_continuation_types(hypothesis)\n",
    "    if isinstance(action, ApplyRuleAction) and hypothesis.frontier_node:\n",
    "        assert action.production in grammar[hypothesis.frontier_field.type]\n",
    "    hypothesis.apply_action(action)\n",
    "\n",
    "print(\"Full reconstructed code:\")\n",
    "print(astor.to_source(asdl_ast_to_python_ast(hypothesis.tree, grammar)).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments are visualised here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full reconstructed code:\n",
      "a=5, b=5\n"
     ]
    }
   ],
   "source": [
    "asdl_ast = python_ast_to_asdl_ast(missing_sub_tree, grammar)\n",
    "missing_actions = parser.get_actions(asdl_ast)\n",
    "hypothesis = Hypothesis()\n",
    "for t, action in enumerate(missing_actions, 1):\n",
    "    assert action.__class__ in parser.get_valid_continuation_types(hypothesis)\n",
    "    if isinstance(action, ApplyRuleAction) and hypothesis.frontier_node:\n",
    "        assert action.production in grammar[hypothesis.frontier_field.type]\n",
    "    hypothesis.apply_action(action)\n",
    "\n",
    "print(\"Full reconstructed code:\")\n",
    "print(astor.to_source(asdl_ast_to_python_ast(hypothesis.tree, grammar)).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ApplyRule[mod -> Module(stmt* body)],\n",
       " ApplyRule[stmt -> FunctionDef(identifier name, arguments args, stmt* body, expr* decorator_list, expr? returns)],\n",
       " GenToken[fun],\n",
       " ApplyRule[arguments -> arguments(arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults, arg? kwarg, expr* defaults)],\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " ApplyRule[expr -> Num(object n)],\n",
       " GenToken[5],\n",
       " ApplyRule[expr -> Num(object n)],\n",
       " GenToken[5],\n",
       " Reduce,\n",
       " ApplyRule[stmt -> Expr(expr value)],\n",
       " ApplyRule[expr -> Call(expr func, expr* args, keyword* keywords)],\n",
       " ApplyRule[expr -> Name(identifier id, expr_context ctx)],\n",
       " GenToken[print],\n",
       " ApplyRule[expr_context -> Load()],\n",
       " ApplyRule[expr -> Name(identifier id, expr_context ctx)],\n",
       " GenToken[a],\n",
       " ApplyRule[expr_context -> Load()],\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " ApplyRule[stmt -> Expr(expr value)],\n",
       " ApplyRule[expr -> Call(expr func, expr* args, keyword* keywords)],\n",
       " ApplyRule[expr -> Name(identifier id, expr_context ctx)],\n",
       " GenToken[print],\n",
       " ApplyRule[expr_context -> Load()],\n",
       " ApplyRule[expr -> Str(string s)],\n",
       " GenToken[foo_bar],\n",
       " GenToken[</primitive>],\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ApplyRule[arguments -> arguments(arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults, arg? kwarg, expr* defaults)],\n",
       " ApplyRule[arg -> arg(identifier arg, expr? annotation)],\n",
       " GenToken[a],\n",
       " Reduce,\n",
       " ApplyRule[arg -> arg(identifier arg, expr? annotation)],\n",
       " GenToken[b],\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " Reduce,\n",
       " ApplyRule[expr -> Num(object n)],\n",
       " GenToken[5],\n",
       " ApplyRule[expr -> Num(object n)],\n",
       " GenToken[5],\n",
       " Reduce]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'production']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anytree import Node, RenderTree, AsciiStyle, find_by_attr\n",
    "f = Node(\"f\")\n",
    "b = Node(\"b\", parent=f, foo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[2].token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using ANTLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ sudo apt-get install -y antlr4\n",
    "$ pip install antlr4-python3-runtime\n",
    "$ wget https://github.com/antlr/grammars-v4/blob/master/python/python3/Python3.g4\n",
    "$ antlr4 -Dlanguage=Python3 Python3.g4\n",
    "```\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from antlr4 import *\n",
    "from MyGrammarLexer import MyGrammarLexer\n",
    "from MyGrammarParser import MyGrammarParser\n",
    " \n",
    "def main(argv):\n",
    "    input_stream = FileStream(argv[1])\n",
    "    lexer = MyGrammarLexer(input_stream)\n",
    "    stream = CommonTokenStream(lexer)\n",
    "    parser = MyGrammarParser(stream)\n",
    "    tree = parser.startRule()\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.8!=4.7\n",
      "ANTLR runtime and generated code versions disagree: 4.8!=4.7.2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Python3Parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-93af46abc4fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCommonTokenStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPython3Parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/src/ANTLR/Python3Parser.py\u001b[0m in \u001b[0;36mfile_input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mlocalctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPython3Parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile_inputContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menterRule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRULE_file_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_la\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# Token type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/antlr4/Parser.py\u001b[0m in \u001b[0;36menterRule\u001b[0;34m(self, localctx, state, ruleIndex)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocalctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildParseTrees\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddContextToParseTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/antlr4/CommonTokenStream.py\u001b[0m in \u001b[0;36mLT\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazyInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/antlr4/BufferedTokenStream.py\u001b[0m in \u001b[0;36mlazyInit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlazyInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/antlr4/BufferedTokenStream.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustSeekIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/antlr4/BufferedTokenStream.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# how many more elements we need?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/antlr4/BufferedTokenStream.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/phd_by_carlos/notebooks/src/ANTLR/Python3Lexer.py\u001b[0m in \u001b[0;36mnextToken\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m       \u001b[0;31m# Check if the end-of-file is ahead and there are still some DEDENTS expected.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPython3Parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;31m# Remove any trailing EOF tokens from our buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Python3Parser' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"src/ANTLR/\")\n",
    "from antlr4 import *\n",
    "from src.ANTLR.Python3Lexer import Python3Lexer\n",
    "from src.ANTLR.Python3Parser import Python3Parser\n",
    "from src.ANTLR.Python3Listener import Python3Listener\n",
    "\n",
    "input_stream = FileStream(\"test.py\")\n",
    "lexer = Python3Lexer(input_stream)\n",
    "stream = CommonTokenStream(lexer)\n",
    "parser = Python3Parser(stream)\n",
    "tree = parser.file_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tree Sitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser, Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Language.build_library(\n",
    "  # Store the library in the `build` directory\n",
    "  'build/my-languages.so',\n",
    "\n",
    "  # Include one or more languages\n",
    "  [\n",
    "    'src/tree-sitter/tree-sitter-javascript',\n",
    "    'src/tree-sitter/tree-sitter-python'\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "JS_LANGUAGE = Language('build/my-languages.so', 'javascript')\n",
    "PY_LANGUAGE = Language('build/my-languages.so', 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser()\n",
    "parser.set_language(PY_LANGUAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_str_from_coords(string, start, end):\n",
    "    lines = string.split(\"\\n\")\n",
    "    s_line, s_char = start\n",
    "    e_line, e_char = end\n",
    "    assert s_line == e_line\n",
    "    return lines[s_line][s_char:e_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_str = \"\"\"\n",
    "function myFunction(p1, p2) {\n",
    "    console.log.log(\"foo\");\n",
    "    return p1 * p2;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_str = \"\"\"\n",
    "from bar import foo\n",
    "def foo(a=5):\n",
    "    b=4\n",
    "    if bar:\n",
    "        baz()\n",
    "        qux()\n",
    "\"\"\"\n",
    "\n",
    "tree = parser.parse(bytes(code_str, \"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_node(node, indent_carry=\"\"):\n",
    "    if node.type in [\"property_identifier\",\"identifier\", \"string\"]:\n",
    "        print(indent_carry, sub_str_from_coords(code_str, node.start_point, node.end_point), end=' ')\n",
    "    elif not node.is_named:\n",
    "        print(indent_carry, node.type, end=' ')\n",
    "    else:\n",
    "        if node.type in [\"block\", 'statement_block']:\n",
    "            print()\n",
    "            indent_carry = indent_carry + \"\"\n",
    "        elif node.type in [\"expression_statement\",\"return_statement\"]:\n",
    "            print()\n",
    "            indent_carry = \"\"\n",
    "    for child in node.children:\n",
    "        visit_node(child,indent_carry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from  bar  import  foo  def  foo  (  a  =  )  : \n",
      " if  bar  : \n",
      "\n",
      " baz  (  ) \n",
      " qux  (  ) "
     ]
    }
   ],
   "source": [
    "visit_node(tree.root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-0ed718cefbdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tree.root_node.children[0].children[3].children[1].children[0].children[1].children[1].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Node kind=\"\"\", start_point=(2, 20), end_point=(2, 21)>,\n",
       " <Node kind=\"\"\", start_point=(2, 24), end_point=(2, 25)>]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.root_node.children[0].children[3].children[1].children[0].children[1].children[1].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Node kind=identifier, start_point=(1, 4), end_point=(1, 7)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.root_node.children[0].children[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Node kind=expression_list, start_point=(3, 4), end_point=(3, 5)>,\n",
       " <Node kind=\"=\", start_point=(3, 5), end_point=(3, 6)>,\n",
       " <Node kind=expression_list, start_point=(3, 6), end_point=(3, 7)>]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.root_node.children[1].children[4].children[0].children[0].children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the grammar from Tree-Sitter\n",
    "Tree-Sitter has this amazing file that is the `grammar.json` file in `tree-sitter-python/src/` it basically has all the information for the nodes and transitions I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from anytree import NodeMixin, RenderTree\n",
    "\n",
    "with open(\"src/tree-sitter/tree-sitter-python/src/grammar.json\", \"r\") as grammar_file:\n",
    "    python_grammar = json.load(grammar_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['module', '_statement', '_simple_statements', '_simple_statement', 'import_statement', 'import_prefix', 'relative_import', 'future_import_statement', 'import_from_statement', '_import_list', 'aliased_import', 'wildcard_import', 'print_statement', 'chevron', 'assert_statement', 'expression_statement', 'named_expression', 'return_statement', 'delete_statement', 'raise_statement', 'pass_statement', 'break_statement', 'continue_statement', '_compound_statement', 'if_statement', 'elif_clause', 'else_clause', 'for_statement', 'while_statement', 'try_statement', 'except_clause', 'finally_clause', 'with_statement', 'with_item', 'function_definition', 'parameters', 'lambda_parameters', '_parameters', '_parameter', 'default_parameter', 'typed_default_parameter', 'list_splat', 'dictionary_splat', 'global_statement', 'nonlocal_statement', 'exec_statement', 'class_definition', 'parenthesized_list_splat', 'argument_list', 'decorated_definition', 'decorator', '_suite', 'block', 'variables', 'expression_list', 'dotted_name', '_expression_within_for_in_clause', '_expression', '_primary_expression', 'not_operator', 'boolean_operator', 'binary_operator', 'unary_operator', 'comparison_operator', 'lambda', 'lambda_within_for_in_clause', 'assignment', 'augmented_assignment', '_right_hand_side', 'yield', 'attribute', 'subscript', 'slice', 'ellipsis', 'call', 'typed_parameter', 'type', 'keyword_argument', 'list', '_comprehension_clauses', 'list_comprehension', 'dictionary', 'dictionary_comprehension', 'pair', 'set', 'set_comprehension', 'parenthesized_expression', 'tuple', 'generator_expression', 'for_in_clause', 'if_clause', 'conditional_expression', 'concatenated_string', 'string', 'interpolation', 'escape_sequence', '_not_escape_sequence', 'format_specifier', 'format_expression', 'type_conversion', 'integer', 'float', 'identifier', 'keyword_identifier', 'true', 'false', 'none', 'await', 'comment', '_semicolon']\n"
     ]
    }
   ],
   "source": [
    "types = list(python_grammar[\"rules\"].keys())\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'SEQ', 'members': [{'type': 'STRING', 'value': 'elif'}, {'type': 'FIELD', 'name': 'condition', 'content': {'type': 'SYMBOL', 'name': '_expression'}}, {'type': 'STRING', 'value': ':'}, {'type': 'FIELD', 'name': 'consequence', 'content': {'type': 'SYMBOL', 'name': '_suite'}}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_grammar[\"rules\"][\"elif_clause\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'CHOICE', 'members': [{'type': 'ALIAS', 'content': {'type': 'SYMBOL', 'name': '_simple_statements'}, 'named': True, 'value': 'block'}, {'type': 'SEQ', 'members': [{'type': 'SYMBOL', 'name': '_indent'}, {'type': 'SYMBOL', 'name': 'block'}]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_grammar[\"rules\"][\"_suite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'SEQ', 'members': [{'type': 'REPEAT', 'content': {'type': 'SYMBOL', 'name': '_statement'}}, {'type': 'SYMBOL', 'name': '_dedent'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_grammar[\"rules\"][\"block\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode(NodeMixin):\n",
    "    def __init__(self, node_name, grammar):\n",
    "        if node_name not in grammar[\"rules\"]:\n",
    "            raise Exception(\"Node not recognised\")\n",
    "        self.name = node_name\n",
    "        self.member = grammar[\"rules\"][node_name]\n",
    "        self.children = []\n",
    "        \n",
    "    @property\n",
    "    def node_complete(self):\n",
    "        return len(children) == len(members)\n",
    "    \n",
    "    def get_first_symbols(self, member, member_in_case_BLANK=None):\n",
    "        if member == None:\n",
    "            return set()\n",
    "        elif member[\"type\"] == \"SYMBOL\":\n",
    "            return set([member[\"name\"]])\n",
    "        elif member[\"type\"] == \"STRING\":\n",
    "            return set([member[\"value\"]])\n",
    "        elif member[\"type\"] == \"SEQ\":\n",
    "            return self.get_first_symbols(member[\"members\"][0])\n",
    "        elif member[\"type\"] in [\"REPEAT\",\"REPEAT1\",\"FIELD\",\"PREC\",\"PREC_LEFT\",\"PREC_RIGHT\"]:\n",
    "            return self.get_first_symbols(member[\"content\"]) \n",
    "        elif member[\"type\"] == \"CHOICE\":\n",
    "            choices = set()\n",
    "            for choice in member[\"members\"]:\n",
    "                choices.update(self.get_first_symbols(choice))\n",
    "            return choices\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def BLANK_possible(self, member):\n",
    "        if member[\"type\"] == \"BLANK\":\n",
    "            return True\n",
    "        elif member[\"type\"] == \"CHOICE\":\n",
    "            for CHOICE_member in member[\"members\"]:\n",
    "                if self.BLANK_possible(CHOICE_member):\n",
    "                    return True\n",
    "        elif member[\"type\"] == \"REPEAT\":\n",
    "            return True\n",
    "        elif member[\"type\"] == \"REPEAT1\":\n",
    "            if self.BLANK_possible(member[\"content\"]):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \"\"\"##### Doodle area\n",
    "    \n",
    "    node_sequence -> ['(']\n",
    "    -> False, [\"_parameters\", \")\"]\n",
    "    cursor = 0\n",
    "    \n",
    "    node_sequence -> ['(', \"foo\"]\n",
    "    -> False, []\n",
    "    \n",
    "    #######\n",
    "    \"\"\"\n",
    "    def walk_member(self, node_sequence, member):\n",
    "        sequence_cursor = 0\n",
    "        if member[\"type\"] == \"SEQ\":\n",
    "            last_member_expansion_nodes = set()\n",
    "            for SEQ_member in member[\"members\"]:\n",
    "                is_match, expansion_nodes, new_cursor = self.walk_member(node_sequence[sequence_cursor:], SEQ_member)\n",
    "                sequence_cursor += new_cursor\n",
    "                last_member_expansion_nodes.update(expansion_nodes)\n",
    "                if is_match == False:\n",
    "                    # the node doesn't match, could be because it's wrong or None\n",
    "                    if sequence_cursor != len(node_sequence):\n",
    "                        return False, set(), sequence_cursor\n",
    "                    return False, last_member_expansion_nodes, sequence_cursor\n",
    "                else:\n",
    "                    if new_cursor != 0:\n",
    "                        last_member_expansion_nodes = expansion_nodes\n",
    "            if sequence_cursor != len(node_sequence):\n",
    "                return False, set(), sequence_cursor            \n",
    "            return True, last_member_expansion_nodes, sequence_cursor\n",
    "        \n",
    "        if member[\"type\"] == \"CHOICE\":\n",
    "            if sequence_cursor == len(node_sequence):\n",
    "                # if BLANK is possible, the node_sequence is still valid\n",
    "                return self.BLANK_possible(member), self.get_first_symbols(member), 0\n",
    "            else:\n",
    "                best_cursor = 0\n",
    "                for CHOICE_member in member[\"members\"]:\n",
    "                    is_match, expansion_nodes, new_cursor = self.walk_member(node_sequence[sequence_cursor:], CHOICE_member)\n",
    "                    best_cursor = max(best_cursor, new_cursor)\n",
    "                    if is_match:\n",
    "                        break\n",
    "                if is_match:\n",
    "                    if new_cursor == 0:\n",
    "                        # case if BLANK\n",
    "                        return True, self.get_first_symbols(member), best_cursor\n",
    "                    else:\n",
    "                        return True, expansion_nodes, new_cursor\n",
    "                else:\n",
    "                    return False, set(), 0\n",
    "                \n",
    "        if member[\"type\"] == \"REPEAT\":\n",
    "            if sequence_cursor == len(node_sequence):\n",
    "                # if BLANK is possible, the node_sequence is still valid\n",
    "                return True, self.get_first_symbols(member), 0\n",
    "            else:\n",
    "                is_match = True\n",
    "                while is_match:\n",
    "                    is_match, expansion_nodes, new_cursor = self.walk_member(node_sequence[sequence_cursor:], member[\"content\"])\n",
    "                    if new_cursor != 0:\n",
    "                        #movement\n",
    "                        sequence_cursor += new_cursor\n",
    "                if sequence_cursor != len(node_sequence):\n",
    "                    return False, set(), sequence_cursor  # this might be wrong\n",
    "                else:\n",
    "                    return True, expansion_nodes, sequence_cursor\n",
    "                \n",
    "        if member[\"type\"] == \"REPEAT1\":\n",
    "            if sequence_cursor == len(node_sequence):\n",
    "                # if BLANK is possible, the node_sequence is still valid\n",
    "                return False, self.get_first_symbols(member), 0\n",
    "            else:\n",
    "                is_match, expansion_nodes, new_cursor = self.walk_member(node_sequence[sequence_cursor:], member[\"content\"])\n",
    "                if not is_match:\n",
    "                    return False, expansion_nodes, 0\n",
    "                else:\n",
    "                    sequence_cursor += new_cursor\n",
    "                    is_match = True\n",
    "                    while is_match:\n",
    "                        is_match, expansion_nodes, new_cursor = self.walk_member(node_sequence[sequence_cursor:], member[\"content\"])\n",
    "                        if new_cursor != 0:\n",
    "                            #movement\n",
    "                            sequence_cursor += new_cursor\n",
    "                    if sequence_cursor != len(node_sequence):\n",
    "                        return False, set(), sequence_cursor  \n",
    "                    else:\n",
    "                        return True, expansion_nodes, sequence_cursor\n",
    "        if member[\"type\"] == \"FIELD\":\n",
    "            is_match, expansion_nodes, new_cursor = self.walk_member(node_sequence[sequence_cursor:], member[\"content\"])\n",
    "            return is_match, expansion_nodes, new_cursor\n",
    "        \n",
    "        if member[\"type\"] == \"STRING\":\n",
    "            if sequence_cursor == len(node_sequence):\n",
    "                # equivalent to if the node sequence is empty\n",
    "                # no node available\n",
    "                return False, set([member[\"value\"]]), 0\n",
    "            else:\n",
    "                if node_sequence[sequence_cursor] == member[\"value\"]:\n",
    "                    return True, set(), 1\n",
    "                else:\n",
    "                    return False, set(), 0\n",
    "        \n",
    "        if member[\"type\"] == \"SYMBOL\":\n",
    "            if sequence_cursor == len(node_sequence):\n",
    "                # no node available\n",
    "                return False, set([member[\"name\"]]), 0\n",
    "            else:\n",
    "                if node_sequence[sequence_cursor] == member[\"name\"]:\n",
    "                    return True, set(), 1\n",
    "                else:\n",
    "                    return False, set(), 0\n",
    "                \n",
    "        if member[\"type\"] == \"BLANK\":\n",
    "            return True, set(), 0        \n",
    "        \n",
    "                \n",
    "    def validate_sequence(self, node_sequence, member):\n",
    "        if member[\"type\"] == \"SEQ\":\n",
    "            for i in range(max(len(node_sequence)), len(member[\"members\"])):\n",
    "                pass\n",
    "        if member[\"type\"] == \"REPEAT\":\n",
    "            ignore\n",
    "        if len(node_sequence) == 0:\n",
    "            self.get_next_symbols(member)\n",
    "        else:\n",
    "            last_node = node_sequence[0]\n",
    "            if member[\"type\"] == \"SEQ\":\n",
    "                pass\n",
    "    \n",
    "    def get_sub_member_rules(self, node_sequence, member):\n",
    "        if len(node_sequence) == 0:\n",
    "            self.get_next_symbols(member)\n",
    "        else:\n",
    "            last_node = node_sequence[0]\n",
    "            if member[\"type\"] == \"SEQ\":\n",
    "                pass\n",
    "            \n",
    "    \n",
    "    def next_child_type(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = TreeNode(\"_statement\", python_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_simple_statements', '_compound_statement'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.get_first_symbols(node.member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, set(), 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.walk_member([\"_simple_statements\"],node.member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 8, 9]])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;34m/usr/lib/python3.6/ast.py\u001b[0m, in \u001b[0;32mparse\u001b[0m:\nLine \u001b[0;34m35\u001b[0m:    \u001b[34mreturn\u001b[39;49;00m \u001b[36mcompile\u001b[39;49;00m(source, filename, mode, PyCF_ONLY_AST)\n",
      "\u001b[0;31mSyntaxError\u001b[0m: unexpected EOF while parsing (<string>, line 1)\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpython",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
